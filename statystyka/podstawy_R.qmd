---
title: Podstawy programowania w R
---

W kilku miejscach w dalszej części wrzucam informacje, jak daną rzecz omawianą teoretycznie można zrobić w R. **Absolutnie nie jest to konieczne do zrozumienia statystyki!** Jest to tylko jedna z możliwości, jak można opisaną dalej teorię przekuć w praktykę. O R można myśleć jako o programie do robienia statystyki. Podobnie jak SPSS, Statistica, Stata czy (oparte na R, darmowe i otwartoźródłowe) jamovi. Jeśli jednak Czytelnik widział kiedyś program statystyczny, spodziewać się będzie ekranu podobnego do Excela, gdzie na górnej belce wybiera się testy statystyczne, jakie chce się przeprowadzić. Praca w R tak nie wygląda. Największa wada i zaleta R polega na tym, że jest on jednocześnie językiem programowania. A to daje bardzo ciekawe możliwości, o których niżej. Także praca w R wygląda tak, że w specjalnym języku piszemy komputerowi, co ma zrobić, potem uruchamiamy te instrukcje i gotowe.

W tym miejscu spróbuję krótko opisać, jak zacząć pracę z R. Nie mam ambicji zrobić pełnego wprowadzenia, bo wyszedłby z tego osobny podręcznik. Mam ambicje dać jakikolwiek zarys fundamentu, który pozwoli Czytelnikowi wyczyścić mało brudne dane i zrobić proste testy. Jeśli ktoś po przeczytaniu tego stwierdzi „Może warto się zagłębić", to znajdzie mnóstwo materiałów, które mu na to pozwolą. Ze swojej strony mogę polecić podręcznik „Język R. Kompletny zestaw narzędzi dla analityków danych" Wickhama i Grolemunda, interaktywne kursy na DataCamp, a pomocy w rozwiązaniu konkretnych problemów zawsze można szukać na StackOverflow.

# Dlaczego warto uczyć się R?

Dla niektórych ludzi perspektywa uczenia się języka programowania tylko po to, żeby móc robić testy statystyczne, wydaje się być stratą czasu. Albo budzi lęk. Po co poświęcać tyle czasu i energii dla czegoś, co możemy zrobić poprzez klikanie w obrazki w jamovi? Powodów jest wiele i kilka pozwolę sobie wymienić.

1.  Można zrobić wszystko. Absolutnie wszystko. Nie ma sensu pytać „czy da się to zrobić?", w R pytamy „jak to zrobić?". Wyobraźmy sobie, że chcemy zastosować jakiś test i nagle odkrywamy, że nie ma go w naszym programie statystycznym. Jeżeli jest nim coś w rodzaju Statistici, to możemy co najwyżej napisać do deweloperów z nadzieją, że dołożą. W R prawdopodobnie istnieje już pakiet, który interesujący nas test zawiera. A jeśli nie, to jeśli mamy wzór matematyczny, to wklepanie go w R to nic trudnego.

2.  R to darmowe, otwartoźródłowe oprogramowanie (tzw. FOSS). Wszystkie pakiety, których używam dalej, nie kosztują nic. Dla porównania najpopularniejszy program statystyczny na świecie, SPSS [^podstawy_r-1] w podstawowej wersji w chwili pisania tego tekstu kosztuje od 99 dolarów miesięcznie za osobę. I to nawet nie jest wersja dająca wszystkie możliwości. W R da się policzyć właściwie wszystko to, co w SPSS, korzysta się ze wszystkich innych zalet języka programowania (patrz niżej), a to wszystko za okrągłe 0 dolarów miesięcznie za osobę.

[^podstawy_r-1]: Drugim najpopularniejszym jest R.

```{=html}
<!-- -->
```
3.  R to język programowania. Pozwala to na szybką automatyzację dużej liczby zadań i radzenia sobie z nieprzewidzianymi okolicznościami. Dla przykładu program do prowadzenia eksperymentów PsychoPy generuje dane w dość pogmatwanych arkuszach kalkulacyjnych, które trzeba dość mocno wyczyścić przed analizą. Problem jest jednak taki, że wyniki każdej osoby badanej zapisują się w osobnych plikach. Jeśli przebadaliśmy 150 osób, mamy 150 plików do wyczyszczenia. Jeśli piszemy analizę w R, to wystarczy wyczyścić jeden plik, a potem dopisać linijkę czy dwie, żeby R tak samo wyczyścił pozostałe 149 plików. Bardzo cenię sobie też możliwość łatwego wygenerowania nazw dla kolumn typu MMPI_001, MMPI_002, ..., MMPI_567. Wpisywanie tego ręcznie to nuda i marnowanie mnóstwa czasu, a w R to jedna linijka kodu.

4.  Analizy są modyfikowalne i wielorazowego użytku. Co mam na myśli? Załóżmy, że zrobiliśmy całą analizę i nagle się orientujemy, że nie usunęliśmy z bazy kilku wierszy, które przez przypadek wytworzyliśmy, jak bawiliśmy się kwestionariuszem, zanim go upubliczniliśmy. Albo prowadzący przy sprawdzaniu prosi, żeby zmodyfikować nieco analizę, bo chce wyniki kwestionariuszy wyrażone jako średnia, a nie suma. Jeśli korzystamy ze zwykłego oprogramowania, w obu przypadkach musimy wyklikiwać analizy od początku. Jeśli nasza analiza jest w R, to mamy ją całą zapisaną przed oczami. Możemy zmienić coś na samym jej początku, kliknąć jeden guzik i gotowe. Jeszcze lepiej -- ta analiza siedzi na naszym komputerze, póki jej nie usuniemy, więc jeśli kiedyś trzeba by było wykonać podobne zadanie, to wystarczy prosta operacja kopiuj-wklej.

5.  R to kompletny zestaw narzędzi dla analityków danych, jak głosi tytuł polecanej na wstępie książki. Nie są to słowa rzucone na wiatr. Korzystając np. z jamovi, zazwyczaj wcześniej czyścimy dane w Excelu. Jeśli potrzebujemy dane zmodyfikować, np. połączyć dwie bazy albo zmienić format szeroki na długi[^podstawy_r-2], to również w Excelu. A to się ciężko robi w Excelu, bo albo robimy to ręcznie, albo musimy nauczyć się Power Query. Potem robimy analizę, ładne wykresy (często wracamy się do Excela w tym celu) i piszemy raport w Wordzie. W R da się zrobić to wszystko. Niezwykle potężne i giętkie czyszczenie danych (za które uwielbiam R) z `tidyverse`, testy statystyczne, ładne wykresy w `ggplot2`, a na końcu nawet raport w Quarto (dawniej RMarkdown). Możemy nie wychodzić z R i często tak będziemy robić.

[^podstawy_r-2]: To trudno sobie wyobrazić, łatwiej zobaczyć na obrazkach. Chodzi o to, czy mamy klasyczną wyczyszczoną bazę, w której jedna osoba badana to jeden wiersz, a każda kolumna to jeden pomiar (np. odpowiedź na pytanie). Czasem chcemy zamiast 11 kolumn (ID osoby badanej + 10 odpowiedzi na pytania) mieć tylko 3 kolumny (ID osoby badanej, numer pytania, odpowiedź). Wtedy każda osoba badana zajmuje 10 wierszy, ale każdy wiersz ma odpowiedź na tylko jedno pytanie. To potrafi być potrzebne do niektórych testów, grupowania danych itp. To jest technikalium, to nie jest statystyka *per se*, ale czasem trzeba to ogarniać, żeby rzeczywiście zrobić analizę.

```{=html}
<!-- -->
```
6.  Wykresy. Wspominałem o tym wyżej, ale wykresy są warte osobnego punktu. Możliwości Excela są ograniczone. Dla przykładu wykres poniżej prezentuje wyniki badania nt. tego, jak wygląda homofobia u mężczyzn, którym się powie, że są niemęscy. W badaniu mierzyliśmy dodatkowo zmienną psychologiczną o nazwie normatywna męskość, czyli niejako stopień przywiązania do reguł, co *prawdziwy mężczyzna* powinien, a czego nie powinien. Wykres jest dość skomplikowany. Zawiera trzy zmienne dychotomiczne, co daje łącznie 8 kolumn w grupach po 4 i 2. Nie powiem, że zrobić taki wykres jest łatwo, bo dobre opanowanie `ggplot2` trochę zajmuje, ale gdy już opanujemy, co jest do opanowania, to hulaj dusza. Nawet najbardziej skomplikowane wykresy stają się możliwe. Gdy połączyć to z możliwościami dawanymi przez programistyczną warstwę R, nagle łatwe staje się wytworzenie serii pasujących do siebie stylem wykresów o tych samych wymiarach. Pozwala to uzyskać pełną spójność bez ręcznego poprawiania szczególików.

![Homofobia u mężczyzn o zagrożonej i niezagrożonej męskości, w zależności od tego, jak wysoką mieli tzw. normatywną męskość. Gdy dodać to tego jeszcze podział na dwa rodzaje homofobii, robią się z tego trzy podziały i osiem różnych kolumn ze wspólnymi osiami, legendą i kolorami. Zrobienie tego w Excelu, o ile w ogóle możliwe, byłoby bardzo trudne.](./ilustracje/homofobia){#fig-homofobia}

6.  Analizy można napisać niezależnie od procesu zbierania danych. Na III roku studiów miałem projekt studencki, w którym musieliśmy zrobić prosty eksperyment w PsychoPy, przeanalizować dane i zaprezentować wyniki. Termin był krótki, także ostatnie dane większość grup zdążyła zebrać dopiero na dzień przed terminem, w tym moja. Oznaczało to, że osoby robiące analizę statystyczną musiały siedzieć do późna, żeby wyrobić się z prezentacją. Ale że już wtedy umiałem co nieco R, to napisałem całą analizę kilka dni wcześniej, kiedy akurat miałem wolny wieczór. Kiedy zebraliśmy ostatnie dane, wystarczyło kliknąć „Run" i gotowe -- wykresy, testy, w mgnieniu oka policzone dla całego zestawu danych (wyczyszczonych! jak wspominałem wyżej, w PsychoPy czyszczenie danych bywa żmudne).

7.  R jest zgodny z filozofią Open Science. O ruchu Open Science można mówić bardzo długo, ale ogólnie jest to ruch dążący do otwartego dostępu do raportów naukowych (za darmo), surowych danych i dokładnych opisów analiz statystycznych. Jeśli naukowiec załącza do swojego artykułu surową bazę danych i skrypt w R, inni naukowcy mogą sprawdzić, czy tamten nie manipulował danymi, nie stosował podejrzanych praktyk oraz czy po ludzku się gdzieś nie pomylił. Poza tym każdy może powtórzyć jego analizę i sprawdzić, czy rzeczywiście wychodzi, jak naukowiec zaraportował.

8.  R jest świetnym środowiskiem do współpracy. Wiele osób może pracować nad jedną analizą statystyczną podobnie jak teraz pracujemy nad prezentacjami czy plikami w Wordzie -- online. Współpracownicy mogą czytać swój kod, uzupełniać się nawzajem, wprowadzać poprawki i dzielić się pracą nawet w obrębie pojedynczych czynności do wykonania. W programach, w których analizy wyklikujemy, jest to znacznie trudniejsze. Gdy zaś mamy kod przed oczami, widzimy wyraźnie, co współpracownik robi i jak mu pomóc.

9.  R wymaga, by wiedzieć, co się robi. Niektórzy mogą to potraktować jako wadę, ale ja sądzę, że to wielka zaleta. Jeśli chcę nauczyć się statystyki, żeby sprawnie zrobić analizy do swojej pracy magisterskiej i rozumieć analizy z artykułów naukowych, to muszę wiedzieć, co robię. Z R ciężko jest nauczyć się, w co klikać, a potem po prostu robić to za każdym razem tak samo. Jest to swego rodzaju wyzwanie, ale cenne.

# Przygotowanie

Żeby zacząć pracować z R, trzeba R pobrać i zainstalować[^podstawy_r-3]. Pobrać R możemy ze strony [r-project.org](https://www.r-project.org). W menu po lewej znajduje się nagłówek *Download*, a pod nim odnośnik *CRAN* (*Comprehensive R Archive Network*, można to nazwać głównym serwerem R). Podchwytliwe jest to, że po wejściu w odnośnik do CRAN pojawi nam się dziwnie wyglądająca lista linków. To strona internetowa pyta nas, z jakiego serwera chcemy pobrać R. Najsensowniej jest wybrać *0-Cloud*, czyli coś, co przekieruje nas do optymalnego serwera. Reszta jest już dość intuicyjna -- pobieramy R dla naszego systemu i instalujemy jak każdy inny program.

[^podstawy_r-3]: Jeśli korzystasz z jakiejś dystrybucji linuxa, R możesz zainstalować z oficjalnych repozytoriów. Pakiet zazwyczaj nazywa się `r-base`. W Ubuntu wystarczy użyć komendy `sudo apt install r-base`.

Druga sprawa to IDE, czyli coś, w czym będziemy pisać nasze skrypty. Ale jak to? R nie wystarczy? Technicznie wystarczy, ale R to program działający z wiersza poleceń. To jest to czarne okienko, gdzie wpisujemy polecenia ręcznie. Większość z nas będzie chciała skorzystać z czegoś bardziej przystępnego niż goły wiesz poleceń. Standardem jest tu program o nazwie RStudio[^podstawy_r-4]. Możemy go pobrać ze strony [posit.co](https://posit.co) w wersji Desktop. Z tym raczej nie będzie już problemu.

[^podstawy_r-4]: W R da się programować też w innych IDE, np. w Visual Studio Code. Niemniej na sam początek lepiej przylgnąć do standardu RStudio, który pomoże nam nabrać najlepszych praktyk i nauczyć się, z czym się R je.

Gdy zainstalujemy i uruchomimy RStudio, naszym oczom ukaże się biały ekran, który na razie nie ma sensu, ale go nabierze. Przed rozpoczęciem pracy warto jest zajrzeć w ustawienia (Tools → Global options...), gdzie możemy zmienić kilka rzeczy. W zakładce Appearance możemy ustawić sobie ciemny tryb[^podstawy_r-5] i poczuć się jak programista. W zakładce Spelling możemy wybrać polski słownik do sprawdzania pisowni. Na ten moment tyle zmian powinno wystarczyć.

[^podstawy_r-5]: Polecam Monokai.

# Konsola, zmienne i matematyka

Ekran RStudio składa się z trzech okienek. Duże okienko po lewej i dwa mniejsze po prawej. Skierujmy naszą uwagę na okienko po lewej, czyli konsolę. Wita nas ona ciepłą informacją, że R jest zainstalowany i znakiem zachęty `>` zachęca nas do wydawania jej poleceń. Konsola R to miejsce, w którym możemy mówić R, żeby coś dla nas liczył. Można to potraktować jako super kalkulator. Spróbuj -- wpisz w konsolę `2+3*5`, zatwierdź enterem i zwróć uwagę, że R stosuje poprawną kolejność wykonywania działań. Spacje nie mają znaczenia, także możemy wpisać również bardziej estetyczną wersję `2 + 3 * 5`. Nie wiem po co, ale można.

Wynik takiego działania nigdzie się nie zapisuje, tylko wyświetla się w konsoli. Jeśli chcemy zapisać nasz wynik, możemy to zrobić stosując znaczek `<-`[^podstawy_r-6]. Przydatnym skrótem jest tu w RStudio jest alt+-, który od razu wstawia nam tę strzałeczkę. Wyjaśnijmy to na przykładzie.

[^podstawy_r-6]: Jeśli znasz jakiś inny język programowania, możesz zapytać, dlaczego nie używamy `=`. Operator `=` też zadziała, ale za dobrą praktykę w R uznawane jest stosowanie `<-` do przypisywania obiektów do zmiennych, a `=` do ustalania wartości argumentów.

```{r}
#| eval: false
wynik <- 2 + 3 * 5
a <- wynik * 3^3
```

Powyżej zapisałem dwa polecenia, które do konsoli powinniśmy wpisać jedno po drugim i każde z nich zatwierdzić enterem. Pierwsze polecenie mówi coś takiego -- policz `2 + 3 * 5` i zapisz to w zmiennej `wynik`. Po zatwierdzeniu tego polecenia możemy zauważyć, że wynik działania nam się nie wyświetlił, za to w prawym górnym okienku pojawiło się słowo `wynik` i obok wartość `17`. Od tego momentu możemy używać słowa `wynik` zamiast 17. Spróbuj wpisać w konsolę samo słowo `wynik` i zatwierdzić enterem. Konsola informuje nas, że w zmiennej `wynik` kryje się liczba 17. Jeśli teraz wpiszesz np. `wynik * 2`, to konsola zwróci to samo, co zwróciłaby po wpisaniu `17 * 2`. Co więc robi drugie polecenie? Możemy je odczytać jako „W zmiennej o nazwie `a` zapisz wynik mnożenia `wynik` i 3 do potęgi 3". Operator `^` to właśnie potęgowanie. Jeśli wpiszemy w konsolę `a`, naszym oczom ukaże się wynik 459.

Jak się potem okaże, w zmiennych możemy zapisywać dużo więcej, niż tylko wyniki prostych działań matematycznych. W identyczny sposób do odpowiednich zmiennych trafią wyniki testów statystycznych albo całe bazy danych. Ale o tym dalej.

# Funkcje

Tym, co robi robotę w R (jak i w każdym innym języku programowania) są funkcje. Funkcja to taka maszynka, do której coś wrzucamy, ona nam to przekształca i wywala coś innego. Tak jak w matematyce. Podobnie jak w matematyce, funkcje zapisujemy konwencją `f(x)`, czyli `nazwa(co_wrzucam_do_funkcji)`. Dla przykładu funkcja `seq` pozwala nam wytwarzać sekwencje liczb. Musimy do tej funkcji wrzucić od jakiej liczby chcemy zacząć, na jakiej chcemy skończyć i jaki chcemy mieć krok. Dla przykładu:

```{r}
seq(5, 62, 3)
```

To, co wrzucamy do funkcji, nazywamy argumentami. Funkcja `seq` wie, że ma zacząć od 5 i skończyć na 62, a nie zacząć od 62 i skończyć na 5, bo ma pod maską zapisane, w jakiej kolejności będzie dostawać te liczby. Takie argumenty nazywamy pozycyjnymi -- funkcja wie, co to jest, na podstawie pozycji. W R każdy argument możemy też nazwać. Dla przykładu wiemy, że funkcja `seq` oczekuje argumentów `from`, `to` i `by`. Możemy więc wprost powiedzieć funkcji, że oto dajemy jej `from`, `to` i `by`.

```{r}
seq(from = 5, to = 62, by = 3)
seq(to = 62, by = 3, from = 5) # jeśli nazywamy argumenty, kolejność nie ma znaczenia
```

Tego typu argumenty nazywamy kluczowymi (*keyword*). W praktyce wykorzystuje się mieszankę jednego i drugiego typu argumentów. Nazywanie argumentów zwiększa czytelność kodu, ale czasem pozycja jest wystarczająco jasna. Dla przykładu mogę napisać `sqrt(x = 9)`, żeby wyciągnąć pierwiastek kwadratowy (*square root*) z 9, ale czy zapis `sqrt(9)` jest jakkolwiek mniej jasny?

Czasami też używamy argumentów kluczowych, żeby przestawić jakieś ustawienia domyślne albo odblokować nowe możliwości. Dla przykładu funkcja `seq` dysponuje dodatkowym argumentem `length.out`. Jeśli ustawimy `length.out`, możemy ustalić długość naszego wyniku zamiast punktu końcowego albo kroku.

```{r}
seq(5, by = 3, length.out = 10) # daj mi 10 kolejnych liczb zaczynając od 5 i co 3
seq(1, 100, length.out = 10) # podaj 10 liczb między 1 a 100
```

## Dokumentacja

Różne funkcje przyjmują różne argumenty. Podobnie jak nie powiemy piekarzowi, żeby stosował białą fugę do chleba, tak samo nie wrzucimy do funkcji `seq` słów zamiast liczb. Tak jak musimy wiedzieć, że piekarz zajmuje się pieczywem, tak samo musimy znać funkcje, których używamy. W poprzednim podrozdziale wiedzieliśmy, co można wrzucić do funkcji `seq` i jak nazywają się jej argumenty, bo to napisałem. Tak samo podałem ot tak, że argument funkcji `sqrt` nazywa się `x`. Skąd mam to jednak wiedzieć?

Nie bez powodu mówimy o *językach programowania* -- wiele funkcji nauczymy się na pamięć i będziemy po prostu wiedzieć, jak z nich korzystać. Jednak znacznie częściej, w wielu przypadkach też dla funkcji, które znamy, będziemy korzystać z dokumentacji. R dysponuje świetną dokumentacją dla każdej funkcji[^podstawy_r-7]. Zawiera ona opis, co dana funkcja robi, jakie argumenty przyjmuje, a często nawet tło teoretyczne jej działania. Żeby dostać się do dokumentacji danej funkcji, wywołujemy ją **w konsoli** ze znakiem zapytania, np. `?seq`. Powoduje to, że w okienku *Help* po prawej wyświetla nam się pełna dokumentacja tej funkcji. Nie trzeba więc sięgać do Google, żeby uzyskać odpowiedź na podstawowe problemy. O ile wiemy, jakiej funkcji chcemy użyć. Zachęcam do częstego sięgania do dokumentacji. To absolutnie podstawowe narzędzie w programowaniu czegokolwiek.

[^podstawy_r-7]: Z założenia. Struktura dokumentacji jest bardzo dobra, a każda funkcja w pakietach CRAN musi być udokumentowana. Zdarzało mi się jednak mieć problem ze zrozumieniem, o co chodzi autorom funkcji, co ma być czym. Jak to w życiu bywa, niektórzy tłumaczą lepiej, a inni piszą treść tego przypisu.

Świetnym źródłem informacji o funkcjach, pozwalającym również znaleźć odpowiednią funkcję do naszego celu, są ściągi (*cheat sheets*). Pakiety `tidyverse` mają nawet swoje oficjalne ściągi, które na początku swojej nauki R wydrukowałem i zalaminowałem. Polecam je gorąco, zwłaszcza do pakietów `dplyr`, `ggplot2` i `stringr`. Można je znaleźć bezpośrednio w RStudio wybierając Help → Cheat sheets → Browse all cheat sheets albo na stronie Posit, czyli firmy, która wypuszcza RStudio.

# Skrypty

Wpisaliśmy w konsolę już sporo rzeczy i historię naszych komend możemy zobaczyć przechodząc do odpowiedniej zakładki w prawym górnym okienku. Jednak wyjście z programu może nam skutecznie skasować tę historię. Jeśli mamy całą sporą analizę statystyczną, która składa się z 200 linijek kodu, to chcielibyśmy mieć jakiś sposób na zapisanie tego na przyszłość, żeby nie musieć za każdym razem wklepywać tego kodu z pamięci. Zaopatrujemy się więc w gruby zeszyt w linie i wszystkie komendy piszemy również tam. Żarcik. Do przechowywania kodu służą specjalne pliki zwane skryptami. Tak jak mamy pliki .pdf, .txt, .docx, tak w plikach .R zapisujemy kod R.

Najprościej wytworzyć nowy skrypt klikając w biały kwadracik z plusem w lewym górnym rogu RStudio. Spowoduje to otworzenie listy rzeczy, które możemy wytworzyć. Nas w tej chwili interesuje `R Script`. Gdy utworzymy nasz skrypt, otworzy się on nad konsolą. Warto od razu zapisać go na dysku skrótem Ctrl+S (lub File → Save). Warto się upewnić, że zapisywany plik rzeczywiście kończy się rozszerzeniem .R.

Na razie nasz skrypt jest pusty, ale możemy w nim pisać dowolne polecenia tak samo, jak napisalibyśmy w konsoli. Różnica jest taka, że nie są one od razu wykonywane. Skrypt to tekst. Jeśli chcemy wykonać jakieś polecenie ze skryptu, to albo kopiujemy je do konsoli, albo umieszczamy na nim kursor i klikamy ctrl+enter. Możemy też myszką zaznaczyć większy fragment kodu i kliknąć Ctrl+Enter, żeby go wykonać. Jeśli chcielibyśmy wykonać cały nasz skrypt, to zaznaczamy cały kod (Ctrl+A) i ponownie używamy Ctrl+Enter. Ewentualnie możemy skorzystać ze skrótu Ctrl+Shift+S[^podstawy_r-8].

[^podstawy_r-8]: Piszę o wielu skrótach klawiszowych, bo umiejętność pisania kodu bez odrywania rąk od klawiatury znacząco przyspiesza pracę. Jednocześnie wiele z tych rzeczy da się wyklikać myszką. Zachęcam jednak, żeby próbować uczyć się skrótów. Listę wszystkich skrótów w RStudio znajdziemy w menu Tools → Keyboard Shortcuts Help (albo pod skrótem Alt+Shift+K).

To jest najważniejsza różnica między skryptem a konsolą -- cokolwiek wpisane w konsolę jest wykonywane natychmiast i znika. Z konsoli korzystamy, kiedy chcemy zrobić jakieś jednorazowe operacje albo coś sobie przetestować. W skrypt wpisujemy to, co chcemy zachować. Ewentualnie szkic. Gdy ludzie przechodzą nagle z konsoli do skryptu, bardzo często zaczynają wpisywać w swój skrypt różne śmieci, które wcześniej wpisaliby w konsolę. Konsola nie zniknęła, ciągle jest do naszej dyspozycji. Skrypt w swojej ostatecznej postaci powinien jednak działać tak, że jak go uruchomimy, to cały przeleci bez błędów. No, przynajmniej do tego dążymy. Czyli konsola do testów, skrypt do prawdziwego kodu.

## Komentarze

Jeśli chcielibyśmy zrobić w skrypcie jakąś notatkę dla siebie, która nie jest kodem używamy znaczka `#`. Jest to tzw. komentarz. Możemy na przykład napisać:

```{r}
#| eval: false
print("Hello world!") # czuję się programistą
```

Jeśli wykonamy taką linijkę, konsola zignoruje wszystko po znaku `#`. Pozwala to nam zostawiać sobie notatki w rodzaju `# hipoteza 1` albo `# nie wiem, czemu to działa, ale działa`. Komentowanie kodu może nam (i naszym współpracownikom) ułatwić zrozumienie, o co nam chodziło, gdy to pisaliśmy. Jeśli chcemy zaopatrzyć nasz kod w nagłówki, konwencja mówi, żeby formatować je tak:

```{r}
# Przygotowanie ----

## Ładowanie danych ----

# kod ładujący dane

## Ładowanie bibliotek ----

# kod ładujący biblioteki
```

Każdy znaczek `#` to niższy poziom nagłówka, czyli wytworzyłem sekcję `Przygotowanie`, a w niej dwie podsekcje `Ładowanie danych` i `Ładowanie bibliotek`. Takich poziomów nagłówków możemy mieć ile chcemy. Nagłówek tym się różni od zwykłego komentarza, że po nim występują cztery myślniki `----` lub inne znaki. Tak sformatowane nagłówki wyświetlają się w bocznym panelu RStudio i pozwalają się lepiej ogarnąć w długim kodzie. Panel *outline* możemy rozwinąć skrótem Ctrl+Shift+O albo klikając skrajną prawą ikonkę nad edytorem skryptu (poziome kreski na prawo od guzika `Source`).

# Projekty

Zazwyczaj projekt badawczy składa się z wielu plików. Nie jest to tylko kod R, ale też chociażby pliki z danymi i inne. Zazwyczaj trzymamy to wszystko w jednym folderze, o ile utrzymujemy jakikolwiek porządek w plikach. Możemy też mieć całe studia luzem na pulpicie, nie oceniam. RStudio pomaga nam w zarządzaniu takimi grupami plików poprzez projekty. Projekty w RStudio robią kilka rzeczy, m.in. pozwalają ustawić niestandardowe opcje (np. zmienić język słownika na angielski tylko dla tego jednego projektu), zapamiętać otwarte okna i ich układ, ale przede wszystkim pomagają nam lokalizować pliki znajdujące się w tym samym folderze[^podstawy_r-9]. Zawsze, kiedy planujemy zachować jakiś zbiór powiązanych plików na dłużej, warto jest wytworzyć projekt.

[^podstawy_r-9]: Ustawiają katalog roboczy (*working directory*), dzięki czemu, kiedy musimy powiedzieć, gdzie jest, powiedzmy, plik z danymi, możemy napisać `"./dane/dane.xlsx"`. `./` oznacza "w tym folderze", `/dane/` oznacza "w podfolderze dane", a `/dane.xlsx` oznacza "w pliku `dane.xlsx`". Możemy to też zrobić ręcznie funkcją `setwd`.

Projekty tworzymy i otwieramy przez guzik w prawym górnym rogu. Rozwijane menu pozwana nam wytworzyć nowy projekt, a wyskakujące okienko pyta, czy wytworzyć go w już istniejącym folderze, stworzyć nowy folder, czy może pobrać repozytorium Git. Jeśli wybraliśmy nowy folder, mamy kilka typów projektów do wyboru, ale w większości przypadków wybieramy po prostu `New Project`. Okienko pozwala nam nadać projektowi nazwę, wybrać jego lokalizację, a także wytworzyć puste repozytorium Git[^podstawy_r-10]. RStudio wytworzy nam w ten sposób plik .Rproj organizujący nasz projekt.

[^podstawy_r-10]: Jeśli nie wiesz o co chodzi, nie przejmuj się. To duży temat i nie jest to coś, bez czego nie da się żyć.

# Pakiety

Pakiety to pewnie dodatki do R, które rozszerzają jego możliwości. Dla przykładu -- R w swojej podstawowej wersji nie ma funkcji liczącej skośność. Nie jest to jednak żaden problem, bo możemy R rozszerzyć np. o pakiet o nazwie `e1071` albo `moments`. Oba te pakiety dodają nam do R możliwość szybkiego i prostego policzenia skośności. Pakiety -- w olbrzymiej większości -- są darmowe.

Absolutnie podstawowym pakietem, czy właściwie zbiorem pakietów, jest `tidyverse`. `Tidyverse` usprawnia R właściwie we wszystkim, co w podstawowej wersji jest niewygodne -- `readr` (*czyt.* rider) pozwala łatwo ładować dane, `dplyr` (*czyt.* diplajer) niesamowicie usprawnia czyszczenie danych, `lubridate` i `stringr` (*czyt.* stringer) to podstawowe narzędzie do pracy z datami i z tekstem[^podstawy_r-11], nie mówiąc już o `ggplot2`, czyli najpotężniejszym narzędziu do tworzenia wykresów. Na szczęście nie musimy wszystkich tych pakietów przywoływać z osobna, bo możemy załadować je wszystkie naraz, ładując jeden zbiorczy pakiet `tidyverse`. Współcześnie `tidyverse` to podstawowy sposób programowania w R. Pakiety ładujemy za pomocą funkcji `library`, do której wrzucamy nazwę pakietu w cudzysłowie. Nasz skrypt zaczniemy więc od takiej instrukcji:

[^podstawy_r-11]: Ciągi znaków tekstowych w informatyce określamy mianem *string*, stąd `stringr`.

```{r}
#| eval: false
library("tidyverse")
```

Jeśli robimy to po raz pierwszy, to po wykonaniu skryptu konsola wyrzuci nam błąd `Błąd w poleceniu 'library("tidyverse")':nie ma pakietu o nazwie ‘tidyverse’`. Wynika to z faktu, że instrukcja `library` tylko ładuje pakiet, ale przed załadowaniem trzeba go pobrać i zainstalować. Na szczęście robimy to tylko raz, zawsze później wystarczy samo `library`. Dlatego też nie będziemy wpisywać komendy instalującej pakiet do skryptu, tylko bezpośrednio do konsoli. Nie chcemy w końcu, żeby pakiet `tidyverse` instalował się za każdym razem, kiedy będziemy uruchamiać skrypt. Będzie to niemiłosiernie spowalniało skrypt i wymuszało dostęp do Internetu. Dlatego też do konsoli wpisujemy:

```{r}
#| eval: false
install.packages("tidyverse")
```

Innym sposobem instalowania pakietów jest skierowanie się w prawe dolne okienko w RStudio, przejście do zakładki Packages, kliknięcie guzika Install, wpisanie nazwy pakietu w wyskakującym okienku (już bez cudzysłowu) i zatwierdzenie guzikiem Install.

Gdy zainstalujemy już pakiet `tidyverse` -- dowolną z metod -- ponownie próbujemy go załadować, tym razem, mam nadzieję, już bez błędu. Konsola poinformuje nas wtedy co dokładnie załadowała.

```{r}
library("tidyverse")
```

# Ładowanie danych

Jeśli chcemy cokolwiek liczyć, musimy mieć na czym liczyć[podstawy_r-23]. R ma wiele sposobów ładowania danych, które w każdym podręczniku zajmują cały rozdział. R poradzi sobie z prawie każdym formatem danych, a także potrafi ładować je bezpośrednio z serwera (dla przykładu pakiet `googlesheets4` pozwala ładować pliki z Google Sheets bez pobierania ich na dysk albo `QualtRics` ułatwiający importowanie plików z danymi ankietowymi z Qualtrics).

[podstawy_r-23]: Wszystkie bazy, których używam w tym tekście, są dostępne w repozytorium na GitHubie. Można wejść w [ten link](https://github.com/jakub-jedrusiak/jakub-jedrusiak.github.io/tree/main/statystyka/dane/podstawy-R), wybrać pożądany plik i albo pobrać go na własny dysk, albo zamiast ścieżki do pliku użyć linku. By uzyskać działający link, nazleży na koniec właściwego linku dopisać `?raw=true`. Takie linki można wklejać w komendy czytające dane zamiast ścieżek. Przykład linku, jak i tego, jak używać ich z funkcjami typu `read_csv` podaję w podrozdziale o grupowaniu i agregowaniu.

Podstawowe pakiety do ładowania danych do `readr` i `readxl` dla plików Excela. Na początek nie trzeba znać struktury funkcji ładujących dane, bo RStudio dysponuje przyjemnym graficznym narzędziem do tego celu. Jeśli w prawym dolnym oknie, w zakładce Files, klikniemy na plik zawierający dane, dostaniemy do dyspozycji opcję Import Dataset... Po jej wybraniu otworzy nam się okno z podglądem danych i kilkoma opcjami do dostosowania, m.in. czy jakichś wierszy nie pominąć albo z którego arkusza pobrać dane. W plikach .csv czasem musimy też wybrać rodzaj separatora i znaku dziesiętnego (opcja Locale). W polskich plikach z danymi separatorem jest zazwyczaj średnik, a znakiem dziesiętnym przecinek, podczas gdy w angielskich danych będą to odpowiednio przecinek i kropka. Jeśli nie ustawimy znaku dziesiętnego, R może potraktować liczby z przecinkiem jako tekst. Poza tym możemy kliknąć w każdy nagłówek kolumny i wybrać typ danych. Dane powinny być czyste, tj. kolumny liczbowe powinny zawierać same liczby. Jeśli dane nie są czyste (a zazwyczaj nie są), możemy je wyczyścić potem.

Możemy kliknąć Import, by dopełnić dzieła, ale jeśli tworzymy skrypt, będzie nas interesowała komenda, jaką RStudio dla nas przygotowało w czarnym okienku w prawym dolnym rogu. W pierwszej linijce RStudio proponuje załadowanie odpowiedniego pakietu. Warto to wpisać na początku skryptu, w miejscu, gdzie załadowaliśmy `tidyverse`. Jeśli proponowanym pakietem jest `readr`, nie musimy ładować go oddzielnie, bo ładując pakiet `tidyverse` załadowaliśmy od razu `readr`. Druga linijka do właściwe ładowanie danych. Widzimy znaną nam już składnię `nazwa <-`. Nasze dane potrzebują jakiejś nazwy, za pomocą której będziemy się do niej odnosić. Może być to `df`, `dane`, `warunek_kontrolny` czy cokolwiek innego. Za strzałką mamy funkcję odczytującą dane. Najpewniej będzie to `read_csv()` albo `read_excel()`. Do tych funkcji wrzucamy jako pierwszy argument lokalizację naszego pliku z danymi w cudzysłowie ('pojedynczym' lub "podwójnym"). Ponieważ nam komendę przygotowało RStudio, możemy ją po prostu skopiować i wkleić do naszego skryptu, zmieniając tylko nazwę zmiennej. Co robią poszczególne argumenty, możemy sprawdzić w dokumentacji. Jeśli dane ładuje `readr`, wykonanie takiej komendy spowoduje ukazanie się informacji zwrotnej -- jakie kolumny załadowano i jaki mają typ. Dla przykładu:

```{r}
df <- read_csv("dane/podstawy-R/complex_database.csv")
```

Już widzę, że nic mi się nie zgadza, z 18 kolumn 17 zostało rozpoznane jako kolumny tekstowe (`chr`), a tylko kolumna ID jako liczby (`dbl`). Za chwilę będziemy to czyścić. Żeby wyświetlić nasze dane, mogę wpisać w konsolę nazwę, pod którą je zapisałem. Nasza baza pojawiła się też w okienku w prawym górnym rogu. Kliknięcie na nią tam spowoduje wyświetlenie jej w oddzielnej karcie. Warto zauważyć, że nie możemy jej tam edytować.

```{r}
df
```

# Data wrangling

Tutaj zaczyna się zabawa. *Data wrangling* to zbiorcze określenie na wszystkie te zdarzenia, kiedy musimy zmienić formę naszych danych, na przykład wziąć tylko niektóre kolumny, odfiltrować jakieś przypadki (np. wybrać tylko osoby z grupy kontrolnej), całkiem zmienić formę danych (np. z szerokiej na długą), dodać nowe zmienne czy przypadki (np. zsumować wyniki kwestionariusza), zmodyfikować kolumny (np. odwrócić punktację w jakiejś pozycji kwestionariusza) itp. itd. Tak naprawdę to, a nie samo wykonywanie testów statystycznych, zajmuje najwięcej czasu i powoduje najwięcej problemów. Jak więc się za to zabrać?

Zarówno tutaj, jak i później, nie będę wchodził w to, *dlaczego* to działa jak działa. Większość poradników R opisuje, jak wykonać pewne operacje w podstawowym R, a następnie jak to robi się współcześnie, czyli wykorzystując pakiet `tidyverse`. O ile znajomość podstawowego R jest niezbędna do wykonywania bardziej skomplikowanych operacji i przydaje się, gdy coś nie działa i trzeba to jakoś naprawić, to tutaj jednak skupię się na podstawach, a współcześnie podstawy to `tidyverse`.

## Wybieranie kolumn i wierszy

Bardzo często będziemy potrzebowali tylko określonych kolumn albo tylko określonych przypadków. Przeglądając nasze dane zauważamy, że składają się w dużej części z niepotrzebnych kolumn, jakie wygenerował dla nas program do ankiet. Kolumny takie jak godziny wypełniania są nam niepotrzebne do analizy. Nasze dane to fragment bazy danych z badania, w którym mówiliśmy mężczyznom, że są mało męscy i patrzyliśmy, jak to wpłynie na nich homofobię. Badaliśmy więc wyłącznie mężczyzn, a mimo to ankietę próbowało też wypełnić kilka kobiet i osób o innej płci. Ponieważ ankieta nie dopuściła ich nawet do metryczki, widzimy w ich przypadkach wartości `NA`, co w R oznacza „brak danych".

### Filtrowanie wierszy

Zacznijmy od tego, że w naszej bazie zostawimy tylko mężczyzn. Wszystkie komendy poniżej wpisuję w konsoli, dla testów. Jeśli wpisujemy komendy modyfikujące dane w konsolę, to nie zapisujemy zmian, tylko sprawdzamy, co się stanie, jak tak zrobimy. Dopiero na koniec podam, jak nasze zmiany rzeczywiście zapisać. Jak więc odfiltrować nie-mężczyzn? Robimy to za pomocą komendy `filter()`[^podstawy_r-12]. `Tidyverse` opiera się o intuicyjnie brzmiące czasowniki takie jak `filter`, `select`, `group_by`, `summarise` itd. Komenda `filter` przyjmuje naszą bazę i jakieś warunki, np. płeć męska. W naszym wypadku będzie to wyglądać tak:

[^podstawy_r-12]: `filter` wybiera wiersze spełniające jakiś warunek. Jeśli chcemy wybrać wiersze na podstawie ich pozycji (np. pierwsze 10 wierszy), użyjemy funkcji `slice`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna")
```

Dobra, co to jest `%>%`? Nie było o tym mowy. Owszem, nie było, ale to bardzo wygodna rzecz. Nazywa się *pipe* (czasem tłumaczone jako *potok*). Mówi mniej więcej „wrzuć to do tego". W naszym przykładzie `df %>% filter()` oznacza „wrzuć bazę danych `df` do funkcji `filter`", czyli dokładnie to samo, co `filter(df)`. Po co więc w ogóle bawić się w potoki? Bo pozwalają nam wygodnie łączyć komendy w ciągi, jak zobaczymy za chwilę. Do wstawiania potoków służy nam wygodny skrót klawiszowy Ctrl+Shift+M, który jest chyba najczęściej stosowanym skrótem przy pisaniu dowolnego programu. Druga kwestia to podział na linijki. Rozbiłem tę komendę na dwie linijki dla czytelności, ale spokojnie mógłbym zapisać to w jednej linijce. Warto jednak pisać kod tak, żeby dało się go potem łatwo czytać. RStudio podpowiada nam też wcięcia, żebyśmy widzieli, że te linijki tworzą jedną całość. Potem opiszę, jak wygodnie formatować kod.

Ta komenda oznacza „weź zmienną `df`, wrzuć ją do komendy `filter` i zostaw tylko te przypadki, w których w kolumnie `Płeć` jest wartość `"Mężczyzna"`." Nazwy kolumn piszemy bez cudzysłowu, ale jeśli wartość komórki to tekst, to zawsze piszemy go w cudzysłowie. Inaczej R pomyśli, że podajemy mu jakąś zmienną, z której ma dopiero odczytać, co ma być w kolumnie `Płeć`. Nam chodzi o dosłowny tekst `"Mężczyzna"`.

Ostatecznie zostaje operator logiczny. Dlaczego piszę `==` zamiast `=`? W programowaniu znak `=` służy do przypisywania wartości do zmiennych. Zapis `a = 5` oznacza „niech `a` ma wartość 5". Sprawdzenie *czy* `a` ma wartość 5 odbywa się poprzez komendę `a == 5`. Konsola wyrzuci nam wtedy `TRUE`, `FALSE` albo `BŁĄD: nie znaleziono obiektu 'a'`. Kilka innych operatorów logicznych prezentuje tabela.

| **Operator** |            **Znaczenie**             |                       **Przykład** |
|------------------|:--------------------------:|-------------------------:|
| ==           |              równa się               |              `Płeć == "Mężczyzna"` |
| !=           |            nie równa się             |                `Płeć != "Kobieta"` |
| \> (\>=)     |  większe niż<br>(większe lub równe)  |                        `Wiek > 40` |
| \< (\<=)     | mniejsze niż<br>(mniejsze lub równe) |                        `Wiek < 40` |
| \|           |                 lub                  |           `Wiek < 18 \| Wiek > 60` |
| &            |                  i                   |  `Płeć == "Mężczyzna" & Wiek > 40` |
| %in%         |        zawiera się w zbiorze         |   `Płeć %in% c("Kobieta", "Inna")` |
| !            |             zaprzeczenie             | `! Płeć %in% c("Kobieta", "Inna")` |

: Część operatorów logicznych dostępnych w R.

### Wybieranie kolumn

Odfiltrowaliśmy więc nie-mężczyzn. Kolejny problem to cała seria niepotrzebnych kolumn. Godziny, adres, zgoda etyczna (która była obowiązkowa i wszyscy się zgodzili) i płeć (już jednakowa dla wszystkich) są nam do niczego niepotrzebne. Do wybierania, jakie kolumny zostawić, służy funkcja `select()`. Wrzucamy do niej nazwy albo numery kolumn, które chcemy zostawić w bazie. Rozszerzmy więc naszą poprzednią instrukcję o dodatkową komendę za pomocą potoku.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18)
```

Po pierwsze zauważmy, że wystarczyło dodać potok i kolejną komendę. Teraz cała nasza instrukcja oznacza „Weź `df`, odfiltruj mężczyzn i potem wybierz kolumny `Id` `Wiek (ukończony w latach)`, `Wykształcenie` oraz kolumny od 9. do 18.". Do tego więc służą potoki -- pozwalają naraz wykonać całą serię modyfikacji tego samego obiektu. Wypada tu wyjaśnić dwie sprawy. Po pierwsze kolumna z wiekiem zawiera w nazwie spacje. Jeśli nazwa kolumny zawiera niestandardowe znaki, trzeba ją otoczyć znakami \` (*pol.* grawis, *ang.* *backtick*), który znajduje się na klawiaturze tuż pod Esc. Druga rzecz to `9:18`, co znaczy „liczby od 9 do 18" i jest wygodnym, skrótowym zapisem `seq(9, 18)`.

Ewentualnie możemy chcieć powiedzieć, żeby zostawić wszystkie kolumny *poza* jakąś kolumną. Jeśli chcemy wykluczyć 2 kolumny z 200, to lepiej wskazać te 2 do wywalenia niż pozostałe 198 do zachowania. Możemy to zrobić dodając przed kolumną `-`. Możemy ustawić minus zarówno przed nazwą kolumny lub zakresem, ale warto zauważyć, że zakres pozycji trzeba wziąć w nawias. Inaczej R pomyśli, że chodzi nam np. o kolumny od -2 do 5. Gdzie nie jest to głupie, kolumna -2 oznacza „druga od końca".

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(-(2:5), -`Wyrażam świadomą i dobrowolną zgodę na udział w badaniu.`)
```

## Zmiana nazw kolumn

Co zrobić, jeśli chcemy w naszej bazie coś pozmieniać? Zacznijmy może od zmiany nazw kolumn, żeby łatwiej nam się pisało dalsze komendy. Do tego służą komendy `rename` z pakietu `dplyr` i `set_names` z pakietu `purrr`[^podstawy_r-13]. `rename` służy do zmiany nazw pojedynczych kolumn i przyjmuje argumenty w postaci `rename("nowa_nazwa" = "stara nazwa")`. Za jednym zamachem możemy zmienić ile nazw chcemy. Jeśli chcemy zmienić wszystkie nazwy, wygodniejsza jest funkcja `set_names` do której **po kolei** wrzucamy nowe nazwy, których potrzebujemy. Znowu -- nazwy to dosłowne ciągi znaków, więc zawsze piszemy je w cudzysłowie.

[^podstawy_r-13]: Zdążyłem zauważyć, że różne funkcje `set_names` występują w różnych pakietach i wiele razy zdarzyło mi się szukać, dlaczego mój kod nie działa, jak wszystko jest dobrze. Okazywało się potem, że R bierze `set_names` nie z `purrr`, tylko np. z `magrittr`, a to są zupełnie inne funkcje. Jeśli R napotyka w dwóch różnych pakietach funkcje o takich samych nazwach, domyślnie wybiera tę załadowaną **później**. Jeśli więc najpierw załadowałem `tidyverse` a potem `magrittr`, to gdy piszę `set_names` R wybiera wersję z `magrittr`. Na szczęście możemy wprost powiedzieć R, z którego pakietu ma brać daną funkcję, używając jej pełnej nazwy `purrr::set_names`. Warto też wiedzieć, że używając takiego zapisu nie musimy ładować całego pakietu, żeby użyć funkcji z danego pakietu, której np. używamy tylko raz w całym kodzie. Osobiście, dla spokoju ducha, akurat `set_names` zawsze zapisuję `purrr::set_names`. Zbyt dużo czasu straciłem na szukanie tego błędu.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_"))
```

Co znowu namieszałem? Czemu znowu coś utrudniam? Cóż, żeby ułatwić. o ile kolejne nazwy `"id"`, `"wiek"` i `"wyksztalcenie"` rozumieją się same przez się, to do czego służy tutaj funkcja `paste`? Jeśli zerkniemy w bazę danych, zauważymy, że kolejne 10 kolumn to to samo pytanie „Takie widoki w przestrzeni publicznej są normalne". Odpowiedź na to pytanie (znajdujące się pod obrazkiem neutralnym lub przedstawiającym parę jednopłciową) traktowaliśmy jako wskaźnik homofobii. Jest bardzo częste, że czyszcząc dane z badania mamy serię odpowiedzi z jednego kwestionariusza. Zazwyczaj wszystkie te pytania nazywamy według jednej konwencji np. wszystkie odpowiedzi z kwestionariusza TIPI nazywamy `TIPI_1`, `TIPI_2`, `TIPI_3` itd. Ale po co pisać te etykiety ręcznie, skoro możemy je wygenerować? Do tego służy funkcja `paste`.

```{r}
paste("H", 1:10, sep = "_")
```

Jak widzimy, `paste` wygenerowało nam 10 kolejnych etykiet łącząc `"H"` i liczby od 1 do 10. Argument `sep = "_"` mówi, żeby między kolejnymi kawałkami wstawiać podkreślnik. Do `paste` możemy wrzucić dowolną liczbę znaków do połączenia. Jeśli nie chcemy żadnego separatora, możemy ustawić `sep = ""`, czyli pusty ciąg znaków w separatorze albo możemy użyć bliźniaczej funkcji `paste0`, która nie ma separatora.

## Zmiana wartości komórek

Jak widzimy, odpowiedzi na pytania z homofobią zawierają nie tylko liczby, ale też tekst z legendą. My jednak chcemy zostawić same liczby. Jeśli spojrzymy na kolumnę z wiekiem, możemy zauważyć, że jest to również jest kolumna tekstowa. Dziwne, przecież wiek to (tylko) liczba. Przejrzenie danych pozwala stwierdzić, że respondent o id 50 w pytaniu o wiek wpisał „18 (2021)". Nieważne, jak się będziemy przed tym bronić, co dopiszemy do pytania o wiek, zawsze znajdzie się ktoś, kto zrobi w nim elaborat. Ten jeden respondent sprawił, że cała kolumna nie jest traktowana jako kolumna liczbowa, ale jako tekst. Odpowiedź na oba te problemy jest taka sama -- `mutate` i `parse_number`.

Funkcja `mutate` to ogólna funkcja, za pomocą której modyfikujemy kolumny albo dodajemy nowe. Jej składnia wygląda następująco:

```{r}
#| eval: false
zmienna_z_danymi %>%
    mutate(
        kolumna_do_modyfikacji = jakas_funkcja(kolumna_do_modyfikacji),
        nowa_kolumna = inna_funkcja(jak_stworzyc_nowa_kolumne)
    )
```

`mutate` w pewnym sensie zawsze tworzy nową kolumnę. Jeśli nowa kolumna ma taką samą nazwę, jak stara, to zastępuje starą. W naszym przykładzie chcemy do całej kolumny `wiek` zastosować funkcję `parse_number`, która pozbywa się wszystkiego poza pierwszą napotkaną liczbą[^podstawy_r-14]. Taka instrukcja będzie wyglądała następująco:

[^podstawy_r-14]: Jej zachowanie możemy modyfikować, np. mówiąc jej, że w liczbach typu `128 242,78` spację ma traktować jako rozdzielacz dużych liczb, a przecinek jako operator dziesiętny (w krajach anglosaskich tę funkcję pełni zazwyczaj kropka). Inaczej `parse_number("128 242,78")` zwróci nam pierwszą napotkaną liczbę, czyli 128. Szczegóły można znaleźć w dokumentacji, ale w takim wypadku powinniśmy użyć `parse_number("128 242,78", locale = locale(grouping_mark = " ", decimal_mark = ","))`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek)
    )
```

### Przekształcanie wielu kolumn jednocześnie

Po wykonaniu tej funkcji widzimy, że cała kolumna jest już numeryczna. To samo możemy zrobić dla pytań z homofobią. Moglibyśmy, oczywiście, zapisać `H_1 = parse_number(H_1), H_2 = parse_number(H_2)` itd., ale po co się męczyć? Na początku roku 2020 dostaliśmy cudowną funkcję pomocniczą `across`, która przydaje nam się w takich dokładnie wypadkach, czyli gdy chcemy zmodyfikować serię kolumn w taki sam sposób. Jak jej używać?

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(starts_with("H_"), parse_number)
    )
```

Pierwszą rzeczą, którą ta funkcja przyjmuje, jest zestaw kolumn. Można je wskazać na różne sposoby, np. wypisać ich nazwy albo numery, ale możemy też użyć jednej z cudownych funkcji pomocniczych z zestawu `tidy-select`. Tutaj akurat użyłem `starts_with("H_")`, żeby powiedzieć, że ma to zrobić ze wszystkimi kolumnami, których nazwy zaczynają się od `H_`. Te same funkcje możemy wykorzystywać w funkcji `select`. Kilka innych tego typu funkcji umieściłem w tabeli.

| Funkcja         |                                           Wybierz wszystkie kolumny... |
|-----------------|------------------------------------------------------:|
| `starts_with()` |                                         których nazwy zaczynają się od |
| `ends_with()`   |                                            których nazwy kończą się na |
| `contains()`    |                                        których nazwy zawierają w sobie |
| `matches()`     | których nazwy zawierają w sobie<br>wyrażenie regularne[^podstawy_r-15] |
| `:`             |                           zawierają się w zakresie<br>(np. `H_1:H_10`) |
| `all_of()`      |                w których wszystkie wartości<br>spełniają jakiś warunek |
| `any_of()`      |                       w których jakakolwiek<br>wartość spełnia warunek |
| `everything()`  |                                              w ogóle wszystkie kolumny |

: Funkcje pomocnicze do `select` i `across`.

[^podstawy_r-15]: Wyrażenia regularne (*Regular expressions* w skrócie *RegEx*) to wielki temat, a liźnięcie go polecam każdemu, czy programuje, czy nie. Pozwalają one na wyszukiwanie regularnych wzorców w tekście, np. `"H_\d+"` oznacza „"H\_" a potem jedna lub więcej liczba". Można je wykorzystywać np. w LibreOffice czy Google Docs (Word zrobił własny system, bo przecież czemu wdrażać światowe standardy, jak można zrobić coś tylko dla tego jednego programu?). Wiele razy oszczędzały mi godzin mechanicznej pracy. Obfita instrukcja znajduje się w ściądze do `stringr`.

Drugim argumentem, jaki przyjmuje `across` jest nazwa funkcji, którą chcemy zastosować. Co ważne, musi to być jej nazwa **bez nawiasów**. Jest to częsty błąd i subtelna różnica, polegająca na tym, że jeśli nie używamy nawiasów, podajemy `across` samą funkcję, a jeśli damy nawiasy, to wrzucamy w ten sposób do `across` *wynik działania* tej funkcji. Spowodowałoby to, że w tym wypadku dostalibyśmy błąd, że funkcja `parse_number()` nie dostała wymaganych argumentów. Jeśli chcielibyśmy dorzucić do `parse_number` jakieś argumenty (jak `locale`[^podstawy_r-16]), możemy to zrobić po przecinku. Szczegóły, jak zwykle, znajdziemy w dokumentacji funkcji `across`.

[^podstawy_r-16]: Jej zachowanie możemy modyfikować, np. mówiąc jej, że w liczbach typu `128 242,78` spację ma traktować jako rozdzielacz dużych liczb, a przecinek jako operator dziesiętny (w krajach anglosaskich tę funkcję pełni zazwyczaj kropka). Inaczej `parse_number("128 242,78")` zwróci nam pierwszą napotkaną liczbę, czyli 128. Szczegóły można znaleźć w dokumentacji, ale w takim wypadku powinniśmy użyć `parse_number("128 242,78", locale = locale(grouping_mark = " ", decimal_mark = ","))`.

### Odwracanie punktacji

Bardzo często zdarza nam się, że w kwestionariuszach niektóre pozycje mają odwróconą punktację. Na przykład w kwestionariuszu samooceny Rosenberga SES pojawia się pozycja „Czasem czuję się bezużyteczny(-a)". Odpowiada się na skali 1 do 4. Wiadomo, że osoba, która zaznacza przy takiej pozycji 4, nie pokazuje swojej wysokiej samooceny. Jest to pozycja z odwróconą punktacją, czyli 4 należy liczyć jako 1, 3 jako 2 itd. Przekształcenie to można zrobić bardzo łatwo. Najpierw dodajemy skrajne wartości skali, np. dla SES $1 + 4 = 5$. Teraz od 5 odejmujemy odpowiedź osoby badanej i dzięki temu rzeczywiście 4 zamienia się w 1, 3 w 2 itd. Jak odwrócić punktację w R?

Ponieważ jest to modyfikacja kolumny, użyjemy funkcji `mutate`. Załóżmy, że `H_5` ma odwróconą punktację. Oceny były na skali od 1 do 6, więc wyniki osób badanych musimy odjąć od 7. W takiej sytuacji kod wyglądałby następująco:

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        H_5 = 7 - H_5
    )
```

Niektórzy lubią tworzyć nowe kolumny na odwróconą punktację, my jednak po prostu zastąpiliśmy oryginalną kolumnę `H_5`. Jeśli chcemy odwrócić wiele kolumn, możemy użyć `across`. Załóżmy, że `H_7` też ma odwróconą punktację. W takim wypadku nasz kod mógłby wyglądać tak:

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        H_5 = 7 - H_5
        H_7 = 7 - H_7
    )
```

Albo z użyciem `across`:

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x)
    )
```

Pierwszym argumentem jest zestaw kolumn, dlatego nazwy kolumn opakowałem w `c()`. Jest to podstawowa funkcja, która zbiera kilka rzeczy w jeden zestaw. `select` czy `filter` nie potrzebowały, żeby robić takie zestawy, ale wiele funkcji (zwłaszcza spoza `tidyverse`) tego wymaga. Co z drugim argumentem, czyli funkcją? Tutaj wchodzimy głębiej w programistyczne meandry i można, oczywiście, zostać przy wersji bez `across`. Odważnych zapraszam do świata funkcji anonimowych.

### Własne funkcje

Drugi argument w `across` to funkcja, jakiej `across` ma użyć do przekształcenia kolumn. Niestety nie ma funkcji, która odejmowałaby od 7. Żeby sobie z tym poradzić, musimy albo taką funkcję wcześniej zadeklarować, albo użyć tzw. funkcji anonimowej (zwanej też *lambda*). Pierwsza opcja jest łatwa do zrozumienia, ale wymaga sporo pisania jak na coś, czego użyjemy tylko raz. Tworzenie własnych funkcji w R jest dość łatwe. Nasza funkcja mogłaby wyglądać tak:

```{r}
odejmij_od_7 <- function(wynik) {
    7 - wynik
}
```

Pierwsza rzecz to nazwa. Obrazowo nazwałem naszą funkcję `odejmij_od_7`. Dalej następuje słowo kluczowe `function` i w nawiasie argumenty naszej funkcji. Do naszej funkcji wrzucamy wynik osoby badanej, więc nasz argument nazwałem obrazowo `wynik`. Jeśli chcemy, dla czytelności, rozbić funkcję na kilka linijek otwieramy teraz nawiasy klamrowe i w nich opisujemy, co funkcja ma robić. Nic nie stoi na przeszkodzie, żeby opisać to wszystko w jednej linijce `function(wynik) 7 - wynik`. Po wykonaniu nasza funkcja rzeczywiście działa, co możemy sprawdzić używając jej w konsoli.

```{r}
odejmij_od_7(3)

odejmij_od_7(12)
```

Jeśli mamy kilka kwestionariuszy z odwróconą punktacją, każdy z inną skalą, możemy od razu zrobić bardziej ogólną funkcję do odwracania.

```{r}
odejmij_od <- function(wynik, od_czego) {
    od_czego - wynik
}

odejmij_od(3, 7)

odejmij_od(2, od_czego = 4)
```

Bardziej ogólna funkcja wymaga podania drugiego argumentu, tzn. od czego trzeba odjąć wynik. Jak widać, ta funkcja też działa i przyjmuje argumenty pozycyjne lub nazwane. Jednej albo drugiej funkcji po zadeklarowaniu możemy użyć w `across`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), odejmij_od, od_czego = 7)
    )
```

#### Funkcje anonimowe

Jeśli funkcja jest prosta i używamy jej tylko raz, często nie chcemy zaśmiecać sobie kodu jej definicją. Wtedy z pomocą przychodzą nam funkcje anonimowe (zwane też funkcjami *lambda*). Anonimowe, bo nie mają swojej nazwy. Podstawowy sposób ich używania to zadeklarowanie ich od razu w miejscu użycia.

```{r}
#| eval: false
across(c(H_5, H_7), function(wynik) 7 - wynik)
```

Zamiast nazwy funkcji użyliśmy tutaj od razu jej definicji. Funkcja jest w pełni sprawna i różni się od `odejmij_od_7` tylko tym, że nie ma nazwy.

Inny, jeszcze bardziej zwięzły, sposób używania funkcji anonimowych dodaje pakiet `purrr` wchodzący w skład `tidyverse`. Polega on na użyciu znaczka `~` (*czyt.* tylda). Nasza funkcja `odejmij_od_7` ma w tej konwencji postać `~ 7 - .x`. Poprzez `.x` oznacza się to, co do funkcji wrzucamy, czyli to, co w `odejmij_od_7` oznaczyliśmy jako `wynik`. Takie coś możemy zapisać w miejscu funkcji w `across`, co zrobiłem na początku.

## Nowe kolumny

Jak wspomniałem, funkcja `mutate` nie tylko pozwala na modyfikowanie istniejących kolumn, ale też na tworzenie nowych. Zazwyczaj robimy to w dwóch przypadkach -- gdy chcemy zagregować dane z wierszy, np. zsumować wyniki kwestionariusza albo gdy chcemy podzielić naszą bazę na kategorie, np. „młodzi", „w średnim wieku", „seniorzy". Omówmy to po kolei.

### Agregowanie danych z wierszy

Załóżmy, że homofobię będziemy liczyć poprzez dodanie `H_1` + `H_2` + `H_3` itd. Czasami będziemy chcieli robić sumy, czasami policzyć średnią. W kwestionariuszach zazwyczaj liczymy sumy, ale dla czasów reakcji często będziemy chcieli policzyć średnią. Jak więc dodać taką sumującą kolumnę w R? Mamy dwa sposoby. Pierwszy to wprost opisanie, co dodajemy, wewnątrz `mutate`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = H_1 + H_2 + H_3 + H_4 + H_5 + H_6 + H_7 + H_8 + H_9 + H_10
    )
```

Jak można się domyślić, istnieje sposób niewymagający tyle pisania, które w skomplikowanych bazach i długich kwestionariuszach naprawdę może być długotrwałe i uciążliwe. `tidyverse` ratuje nas tutaj funkcją `pick`[^podstawy_r-17], a standardowy R dokłada funkcję `rowSums` (i `rowMeans`). Wystarczy, że do funkcji `rowSums` wrzucimy, które kolumny chcemy zsumować, wskazując je właśnie za pomocą `pick` i funkcji pomocniczych.

[^podstawy_r-17]: W chwili kiedy to piszę, czyli luty 2023 r., `pick` jest świeżynką, wprowadzoną do `dplyr` w wersji 1.1.0 na koniec stycznia 2023 r. Jeśli Twój R jej nie znajduje, warto zaktualizować pakiety, albo guzikami w RStudio, albo funkcją `devtools::update_packages()`. Jeśli nie ma takiej możliwości, w miejsce `pick` można użyć `across`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10))
    )
```

### Kategoryzowanie przypadków i przekodowywanie

Czasem zdarza się, że chcemy podzielić dane ilościowe (np. wiek, wzrost, szczęście mierzone kwestionariuszowo) na kategorie (młodzi vs nie-aż-tak-młodzi, wysocy vs niscy, szczęśliwi vs nieszczęśliwi). Zdarza się też, że osoba tworząca ankietę nie buła na tyle przewidująca, żeby w odpowiedziach na pozycję kwestionariuszową do „zdecydowanie się zgadzam" dodać 6, więc nie możemy po prostu użyć `parse_number`. I w jednym, i w drugim wypadku musimy stworzyć wartości na podstawie innych wartości, np. wzrost poniżej 160 cm zamienić na „niski" albo tekst „zdecydowanie się nie zgadzam" zamienić na 1. Do takich celów służy niezwykle przydatna funkcja `case_when` z pakietu `dplyr`. Załóżmy, że chcemy podzielić mężczyzn w naszej bazie na trzy kategorie wykształcenia -- podstawowe, ponadpodstawowe i wyższe. Oznacza to, że osoby z wykształceniem średnim i zawodowym musimy wrzucić do jednego worka. W tym celu rozszerzymy naszą instrukcję o kolejną komendę. Funkcja `case_when` ma dość prostą składnię.

```{r}
#| eval: false
case_when(
    warunek_1 ~ wartosc_jesli_prawda,
    warunek_2 ~ wartosc_jesli_prawda,
    warunek_3 ~ wartosc_jesli_prawda,
    TRUE ~ wartosc_dla_calej_reszty
)
```

Funkcja po kolei sprawdza warunki. Jeśli natrafi na jakiś spełniony warunek zatrzyma się i da taką wartość, jaką temu warunkowi przypisaliśmy. Warunek jest logiczny, czyli może to być cokolwiek od `wyksztalcenie == "Średnie"` po `wzrost <= 160`. Należy pamiętać, że jeśli wartość wynikowa ma być tekstem, musimy napisać ją w cudzysłowie, jak każdy dosłowny tekst. Na końcu funkcja dochodzi do `TRUE`, które jest zawsze prawdziwe, więc `TRUE ~ wartosc` może nam służyć do ustalania, co ma być, jeśli żaden z powyższych warunków się nie sprawdzi. Jeśli chodzi o przykład z wykształceniem, moglibyśmy rozwiązać go tak:

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10)),
        wyksztalcenie_grupa = case_when(
            wyksztalcenie == "Średnie" ~ "Ponadpodstawowe",
            wyksztalcenie == "Zawodowe" ~ "Ponadpodstawowe",
            TRUE ~ wyksztalcenie
        )
    )
```

W tym wypadku `case_when`, idąc wiersz po wierszu, sprawdza, czy w kolumnie `wyksztalcenie` nie znajduje się wartość `"Średnie"`. Jeśli tak, to w tworzonej właśnie kolumnie `wyksztalcenie_grupa` wstawia wartość `"Ponadpodstawowe"` i przechodzi do kolejnego. Jeśli wykształcenie nie jest średnie, to sprawdza, czy jest zawodowe i w razie czego również wstawia `"Ponadpodstawowe"`. Jeśli nie jest ani średnie, ani zawodowe, to wstawia to, co akurat jest w kolumnie `wyksztalcenie`, czyli dla osób z wykształceniem podstawowym wstawia `"Podstawowe"`, a dla osób z wykształceniem wyższym `"Wyższe"`[^podstawy_r-22]. W ten sposób z 4 kategorii wykształcenia zrobiły nam się 3. W podobny sposób przekodowywalibyśmy klucz w ankiecie na liczby, np. pisząc `H_1 == "Zdecydowanie się zgadzam" ~ 6`.

[^podstawy_r-22]: Warto zauważyć, że zapisane tu warunki moglibyśmy uprościć do postaci `wyksztalcenie == "Średnie" | wyksztalcenie == "Podstawowe"` albo jeszcze bardziej `wyksztalcenie %in% c("Średnie", "Podstawowe")`. Warto też zerknąć w nową siostrę `case_when`, tj. `case_match`, która również mogłaby nam tutaj pomóc. Szczegóły, oczywiście, w dokumentacji.

## Sortowanie i kolejność kolumn

Wychodzimy już z potężnej funkcji `mutate` i możemy czyścić dalej. Ostatnia rzecz, którą czasem chcemy zrobić (zazwyczaj ze względów estetycznych), to posortowanie wartości i ustawienie kolumn w określonej kolejności.

### Sortowanie

Za sortowanie w `tidyverse` odpowiada funkcja `arrange`. Domyślnie sortuje ona rosnąco, więc jeśli chcemy sortowanie malejące, użyjemy pomocniczej funkcji `desc` (od *descending*). Załóżmy, że chcemy posortować nasze dane najpierw według wykształcenia (od najwyższego, do najniższego), a w obrębie wykształcenia według wieku (od najmłodszych do najstarszych). W pierwszym odruchu chcielibyśmy wpisać `arrange(desc(wyksztalcenie), wiek)`. Jest to dobry odruch, jednak jeśli to zrobimy zorientujemy się, że najwyższym z wykształceń jest wykształcenie zawodowe. Dzieje się tak dlatego, że w tej chwili wykształcenie to zwykły tekst, a więc jest sortowany alfabetycznie, nie według naszego klucza. Żeby to zmienić, musimy poznać nowy rodzaj danych.

#### Factors

*Factor* to rodzaj danych, za pomocą których przechowujemy tekst, który ma tylko kilka możliwych wartości albo te wartości mają jakąś kolejność, którą chcemy wziąć pod uwagę. Jeśli mamy etykiety takie jak wykształcenie, czy grupa kontrolna/eksperymentalna, to powinniśmy je przechowywać właśnie w tej postaci. Danymi tego typu w `tidyverse` zarządza pakiet `forcats`. Żeby zmienić wykształcenie z tekstu na factor, dopiszemy jedną linijkę do naszego `mutate` i od razu posortujemy.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10)),
        wyksztalcenie_grupa = case_when(
            wyksztalcenie == "Średnie" ~ "Ponadpodstawowe",
            wyksztalcenie == "Zawodowe" ~ "Ponadpodstawowe",
            TRUE ~ wyksztalcenie
        ),
        wyksztalcenie = factor(
            wyksztalcenie,
            levels = c("Podstawowe", "Zawodowe", "Średnie", "Wyższe"),
            ordered = TRUE
            )
    ) %>%
    arrange(desc(wyksztalcenie), wiek)
```

Komendę `factor` dla czytelności rozbiłem tutaj na trzy linijki, ale -- oczywiście -- można ją całą zapisać w jednej. Po pierwsze wskazałem, że na factor przerobiona ma być kolumna `wyksztalcenie`. Po drugie wskazałem, jakie wykształcenie może mieć wartości, zbierając je w jeden zestaw funkcją `c` i wrzucając do argumentu `levels`. Na koniec poinformowałem R, że w kolejność jest tutaj ważna, dopisując `ordered = TRUE`. Jeśli tak przerobione dane posortujemy, zobaczymy, że baza rzeczywiście zaczyna się od wykształcenia wyższego.

### Kolejność kolumn

Możemy chcieć mieć nasze kolumny w określonej kolejności. Są zasadniczo dwa sposoby zmieniania kolejności kolumn. Jest funkcja `relocate`, która służy raczej przestawianiu pojedynczych kolumn lub ich niewielkiej liczby. Jeśli chcemy od nowa określić kolejność kolumn, możemy wykorzystać w tym celu znaną nam już funkcję `select`. Załóżmy, że chcielibyśmy przestawić kolumnę `H_suma` przed kolumny z cząstkowymi wynikami.

```{r}
#| eval: false
# z użyciem relocate
df %>%
    relocate(H_suma, .before = H_1)

# z użyciem select
df %>%
    select(id, wiek, wyksztalcenie, H_suma, everything())
```

Jeśli przestawiamy kolumny z użyciem `relocate`, powinniśmy ustawić argument `.before` albo `.after`. Oba wymagają nazwy kolumny przed którą lub po której chcemy przenieść naszą kolumnę. Jeśli nie ustawimy żadnego, nasze kolumny zostaną przeniesione na początek tabeli. Jeśli używamy `select` musimy wpisać kolejność naszych kolumn ręcznie. Ciekawe może być użycie przeze mnie `everything()`. W tym kontekście znaczy ono „i cała reszta".

## Zapisywanie zmian

W ten sposób uzyskaliśmy cały kod czyszczący. Mamy ów kod zapisany w naszym skrypcie, jeśli go uruchomimy, to widzimy, że działa. Jednak jeśli w konsolę wpiszemy `df`, naszym oczom ciągle ukazuje się stara, brzydka baza. Jak więc zmienić nasze `df` na wyczyszczoną wersję? Tak jak zawsze przypisujemy wartości w R -- operatorem `<-`. Nasz kod na zmianę brudnej bazy w czystą ostatecznie przyjmie postać:

```{r}
df <- df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10)),
        wyksztalcenie_grupa = case_when(
            wyksztalcenie == "Średnie" ~ "Ponadpodstawowe",
            wyksztalcenie == "Zawodowe" ~ "Ponadpodstawowe",
            TRUE ~ wyksztalcenie
        ),
        wyksztalcenie = factor(
            wyksztalcenie,
            levels = c("Podstawowe", "Zawodowe", "Średnie", "Wyższe"),
            ordered = TRUE
            )
    ) %>%
    arrange(desc(wyksztalcenie), wiek) %>%
    relocate(H_suma, .before = H_1)
```

Kod ten możemy uruchomić dla dowolnej ilości danych, w dowolnym momencie. Jest wielokrotnego użytku i spokojnie możemy go wykorzystać, kiedy baza się rozrośnie. Nie musimy go wtedy pisać od nowa, wystarczy, że go uruchomimy. Co więcej, mogę wpaść jeszcze na jakiś pomysł i dopisać linijkę na samym początku, nie musząc całej reszty robić od nowa. Sprawmy sobie tę przyjemność i zerknijmy na naszą wyczyszczoną bazę.

```{r}
df
```

Po zapisaniu zmiennej `df`, tracimy naszą starą zmienną. Tym samym, jeśli uruchomilibyśmy nasz kod jeszcze raz, ale już na nowej zmiennej `df`, wyskoczy nam błąd. W końcu nowa zmienna nie ma tych samych kolumn, co stara zmienna. Co więcej, takiej operacji nie da się cofnąć. Jeśli chcemy dostać swoją starą, brudną bazę, musimy ponownie załadować ją z pliku. To prowadzi nas do ważnego wniosku co do pisania skryptów -- powinniśmy pisać je tak, żeby dało się z nich odtworzyć wszystko, co robiliśmy od samego początku[^podstawy_r-18]. Dzięki temu, jeśli chcemy się wycofać, zaznaczamy i wykonujemy cały kod przed interesującym nas momentem. Brak skrótu Ctrl+Z jest jedną z ważniejszych różnic w analizie w programach typu SPSS czy Statistica a w językach programowania typu R czy Python. Wbrew pozorom, idzie się przyzwyczaić. Ta sama właściwość pozwala na zachowanie przejrzystości w nauce -- pokaż mi swój kod, a będę wiedział bardzo dokładnie, jak prowadziłeś(-aś) swoją analizę.

[^podstawy_r-18]: Dlatego ja zazwyczaj wstępną bazę ładuję do zmiennej typu `df_raw`, i ją przekształcam, zaś do zmiennej `df` zapisuję już wyczyszczoną bazę. Swój kod czyszczący zaczynam więc od `df <- df_raw %>%`. Dzięki temu zawsze mogę na szybko zerknąć do niewyczyszczonej bazy, jakbym czegoś z niej potrzebował.

### Grupowanie i agregowanie

Gdy mamy już bazę, zazwyczaj chcemy policzyć pewne statystyki dla grup badanych, np. dla osób różniących się wykształceniem, płcią czy jakąś manipulacją. Chcemy na przykład poznać średnią homofobię osób o różnym wykształceniu, sprawdzić liczność naszych podgrup czy policzyć inne zbiorcze statystyki. Możemy, oczywiście, odfiltrować najpierw osoby o wykształceniu podstawowym, policzyć dla nich, potem osoby o wykształceniu średnim itd. Są jednak prostsze sposoby, a obejmują one użycie `group_by` i `summarise`[^podstawy_r-19]. Te dwie funkcje zazwyczaj idą ze sobą w parze i zgrupowane dane od razu trafiają do `summarise`. Poniżej przykład.

[^podstawy_r-19]: `tidyverse` dopuszcza zarówno pisownię brytyjską `summarise` jak i amerykańską `summarize`. Podobnie w przypadku innych rozbieżności. Dokumentacja wskazuje jednak, że pisownia brytyjska jest preferowana. Też ją preferuję.

```{r}
df %>%
    group_by(wyksztalcenie) %>%
    summarise(
        n = n(),
        H_M = mean(H_suma),
        H_SD = sd(H_suma),
        H_Me = median(H_suma),
        V = H_SD / H_M
    )
```

Jak widzimy, dostaliśmy tabelkę z wykształceniem i wskazanymi statystykami. Funkcja `n` zliczyła nam przypadki osób z poszczególnym wykształceniem, `mean` policzyła średnią, `sd` odchylenie standardowe, a `median` medianę. `V` to tzw. współczynnik zmienności. Co to jest, nie jest teraz szczególnie ważne. Policzyłem to tutaj, żeby pokazać, że w obliczeniach możemy też wpisywać niestandardowe operacje (jak dzielenie) bez żadnych strasznych funkcji anonimowych, a także że możemy wziąć wartości z innych kolumn jako argumenty do naszych przekształceń. Tutaj `V` to odchylenie standardowe średniej homofobii (`H_SD`) podzielone przez samą średnią (`H_M`). Każdą kolumnę mogliśmy nazwać wedle życzenia. Grupować możemy na podstawie wielu zmiennych. Jak dowiemy się w podrozdziale o eksploracji danych, istnieją funkcje, które najpopularniejsze zastawy danych eksploracyjnych liczą za nas.

Tak robiliśmy to zawsze, jednak `dplyr` 1.1.0. wprowadził inny sposób grupowania. Jeśli nie chcemy zapisywać grup w naszej bazie danych na później (czyli w większości przypadków), nie musimy w ogóle używać funkcji `group_by`. Zamiast tego `summarise` dostał argument `.by`, za pomocą którego możemy wskazać grupy jednorazowo, tylko na potrzeby tego jednego podsumowania. Więcej na temat argumentu `.by` można znaleźć w dokumentacji. Poniżej przykład z innego zbioru danych, w którym pojawia się liczba krzyków w piosence w zależności od typu piosenki i jej autora [@field2012].

```{r}
# załadowanie danych z sieci
df_gaze <- read_csv("https://github.com/profandyfield/discovr/blob/master/data-raw/csv_files/escape.csv?raw=true")

# podejrzenie, jak dane wyglądają
glimpse(df_gaze)

# pogrupowanie i zliczenie średniej liczby krzyków
df_gaze %>%
    summarise(
        M = mean(screams),
        .by = c(song_type, songwriter)
    )
```

Kolumny do grupowania podałem jako zestaw, czyli wewnątrz `c()`. Zgrupowane w ten sposób dane pokazują nam, że Andy pisze bardziej krzykliwe piosenki od Malcolma, ale różnica powiększa się, gdy chodzi o piosenki symfoniczne.

## Format długi i szeroki

Format długi i szeroki to coś, co rzadko pojawia się w tekstach wprowadzających i nie mam pojęcia czemu. To jest naprawdę ważne. Przełożenie danych z jednego formatu na drugi to często podstawowa operacja, jaką musimy wykonać, kiedy chcemy coś policzyć. Nie mam chyba ani jednego projektu, w którym bym tego nie robił. Do tego współczesne komendy, które to robią, są naprawdę proste. Tym bardziej zaskakujące jest, że np. w Excelu wykonać taką operację jest trudno, jeśli nie umie się korzystać z Power Query. Zacznijmy jednak od tego, co to jest format długi i szeroki.

Terminy te odnoszą się do sposobu, w jaki składujemy dane. Format szeroki jest tym, co odruchowo tworzymy, kiedy robimy czyste tabelki. Jeden wiersz to jedna obserwacja. Wszystkie dane o konkretnej osobie badanej znajdują się w tym jednym wierszu. Każda kolumna to jedna zebrana dana, np. odpowiedź na konkretne pytanie. W takim formacie znajduje się teraz nasza baza. Weźmy z niej kilka kolumn, po czym użyjmy `head`, żeby zobaczyć pierwszych pięć wierszy.

```{r}
df_wide <- df %>% # zapiszę to jako df_wide, na później
    select(id, H_1:H_5) %>%
    mutate(id = 1:nrow(df)) %>% # poprawiam id, żeby były kolejne liczby, zmiana kosmetyczna
    arrange(id) # sortuję wg id

df_wide %>% # zapisane dane trzeba jeszcze wyświetlić
    head(n = 5) # tylko 5 pierwszych wierszy
```

Są to typowe dane w formacie szerokim. Żeby jednak zrozumieć różnicę, między formatem długim, a szerokim, trzeba jeszcze zobaczyć dane długie. Stwórzmy więc takie.

```{r}
df_long <- df_wide %>%
    pivot_longer(H_1:H_5, names_to = "pytanie", values_to = "ocena")

df_long %>%
    head(n = 10)
```

Zacznę od skomentowania samych danych, a potem wyjaśnię funkcję. Dane w formacie długim mają oddzielną kolumnę na numer pytania i odpowiedź. Pięć kolumn z odpowiedziami na pytania zmieniliśmy w dwie. Powoduje to jednak, że każda osoba badana ma pięć wierszy – w każdym odpowiedź na tylko jedno pytanie. Najpierw następuje 5 wierszy osoby z id 1, potem 5 wierszy osoby z id 2 itd. Widać więc dlaczego formaty te nazywają się szeroki i długi. Szeroki ma wiele kolumn, mało wierszy (1 na osobę), długi mało kolumn, wiele wierszy (1 na każde pytanie).

Po co nam taki format? Zawiera te same informacje, co format długi, a trudniej się to czyta. Po pierwsze umożliwia nam to policzenie niektórych rzeczy, których nie policzylibyśmy z formatu szerokiego. Dla przykładu, teraz mogę grupować dane według pytań, żeby sprawdzić, czy na każde pytanie badani odpowiadają podobnie. Jeśli moja skala jest dobra i każde pytanie rzeczywiście mierzy to samo, to odpowiedzi na wszystkie pytania powinny być podobne. Być może zrobiłem jakieś kontrowersyjne pytanie, na które wszyscy odpowiadają nisko, mimo że nie różnią się, w tym przykładzie, poziomem homofobii. Mogę więc, na oko, sprawdzić rzetelność pozycji testowych[^podstawy_r-20]. Formatu długiego wymagają też niektóre testy statystyczne.

[^podstawy_r-20]: Oczywiście psychometria dysponuje lepszymi wskaźnikami rzetelności testów niż proste średnie odpowiedzi. Fakt pozostaje faktem jednak, że grupować dane po pytaniach można tylko wtedy, gdy są w jednej kolumnie.

```{r}
df_long %>%
    summarise(
        M = mean(ocena),
        SD = sd(ocena),
        .by = c(pytanie)
    )
```

Nawet częściej niż do grupowania po pytaniach, wykorzystujemy format długi do wykresów. Jak się przekonamy dalej, w gramatyce graficznej (*the grammar of graphics*) do jednego obiektu na wykresie możemy przypisać tylko jedną kolumnę. Jeśli więc chcemy zrobić wykres słupkowy np. wyników *przed* i *po*, to do osi X przypiszemy kolumnę z etykietami, a do osi Y kolumnę z wynikami. Nie da się więc sensownie zrobić wykresu, jeśli wyniki *przed* i *po* mamy w osobnych kolumnach.

Omówmy więc funkcję, której użyłem do zmiany formatu. Kiedyś robiło się to skomplikowanymi funkcjami `melt` i `cast`, które często można znaleźć w innych językach programowania. Dziś w R mamy, na szczęście, intuicyjne funkcje `pivot_wider` i `pivot_longer`. Tej pierwszej używamy zmieniając format na szeroki, tą drugą zmieniamy format na długi. Na przykładzie powyżej można stwierdzić, że `pivot_longer` przyjmuje trzy argumenty. Pierwszy to zbiór kolumn, do jakich chcemy tę funkcję zastosować. Można tu skorzystać z funkcji pomocniczych typu `starts_with()` albo `everything()`. Kolejne dwa argumenty funkcji `pivot_longer` to `names_to` i `values_to`. Są to nazwy kolumn, do których mają trafić, jak nazwa wskazuje, nazwy i wartości kolumn. W naszym przykładzie etykiety `H_1`, `H_2` itd. trafiły do kolumny `pytanie`, zaś same odpowiedzi na te pytania do kolumny `ocena`.

```{r}
#| eval: false
df_long %>%
    pivot_wider(names_from = "pytanie", values_from = "ocena")
```

`pivot_wider` ma prostszą składnię, ponieważ nie trzeba w niej wskazywać zakresu kolumn do rozwinięcia, a jedynie w jakiej kolumnie znajdują się nazwy kolumn, a w jakiej jej wartości. Robimy to odpowiednio argumentami `names_from` i `values_from`. Jeśli jakiejś wartości nie ma w formacie długim (np. gdy osoba z numerem 4 nie odpowiedziała na pytanie 2, to w formacie długim może nie być wiersza `4 H_2`), to `pivot_wider` automatycznie wstawi w tę komórkę `NA`[^podstawy_r-21]. Zdarza się, że funkcji tej musimy użyć dlatego, że niektóre programy generują dane w formacie długim.

[^podstawy_r-21]: To zachowanie można zmodyfikować argumentem `values_fill`. Szczegóły można znaleźć w dokumentacji. Swoją drogą to jest piękne w R, jak bardzo jest on giętki. Funkcje mają domyślne zachowania, ale jeśli użytkownik w swojej szczególnej sytuacji potrzebuje czegoś innego, to zawsze może to ustawić. Ta uniwersalność jest sama w sobie dobrym powodem, żeby uczyć się programowania.

## Retesty czyli złączenia (*joins*)

Złączenia (*joins*) to, jak nazwa wskazuje, metoda łączenia dwóch baz danych. Jest to jedna z podstawowych operacji na bazach danych, znana co najmniej od lat 70. i instrukcji `JOIN` w SQL. Jest to także jedna z operacji niedostępnych w Excelu bez Power Query. W praktyce badawczej może być ona konieczna, gdy mamy badanie wieloczęściowe, w którym musimy stosować wiele baz danych (np. jedną tworzą pomiary z eyetrackera, drugą wyniki w ankiecie, a trzecią test szybkości reakcji). Zdarza się to również często w prostych badaniach ankietowych, kiedy po jakimś czasie musimy wykonać retest. W obu tych przypadkach lądujemy z dwiema (lub więcej) bazami, które -- miejmy nadzieję -- mają jaką wspólną kolumnę, identyfikator osoby badanej, taki sam w każdej z trzech baz[^podstawy_r-21].

[^podstawy_r-21]: Czasami możemy też wykorzystać inną informację o osobie badanej jako identyfikator. Brałem udział w badaniach, w których osoba badana wypełniała ankietę i w tym samym czasie była nagrywana. Potem łączyłem informacje z nagrania z informacjami z ankiety na podstawie czasu wypełniania ankiety i czasu nagrywania. Jak widać, wszystko jest możliwe, choć domyślne złączenia z `dplyr`, w chwili pisania tego tekstu, nie pozwalają na pełną giętkość. Wtedy użyłem złączeń z pakietu `fuzzyjoin`.

Jak więc takie bazy połączyć? Wykorzystajmy tutaj dwie bazy zawierające test i retest, zrobione podczas walidacji kwestionariusza o nazwie KTR. Składał się on z dwóch skal oznaczonych tutaj literkami O i W. Standardową procedurą przy projektowaniu kwestionariusza jest powtórzenie pomiaru po jakimś czasie, żeby sprawdzić, na ile wyniki są stabilne. My taką procedurę wykonaliśmy, przez co dysponujemy dwoma oddzielnymi bazami. Zerknijmy na nie.

```{r}
db_test <- read_csv("./dane/podstawy-R/join-test.csv", show_col_types = FALSE)
db_retest <- read_csv("./dane/podstawy-R/join-retest.csv", show_col_types = FALSE)

db_test

db_retest
```

Pierwsza rzecz, która może zwrócić naszą uwagę, to znacznie mniejsza liczba osób badanych przy reteście. Jest to naturalne, jako że wiele osób, mimo wcześniejszych deklaracji nie wypełnia naszego testu po raz drugi. Widzimy też, że każdy wiersz posiada jakiegoś rodzaju kolumnę z unikatowym identyfikatorem osoby badanej. W języku relacyjnych baz danych takie unikatowe kolumny określa się jako `PRIMARY KEY`. W bazie danych z pierwszego testu kolumna ta nosi nazwę `ID`, a w bazie danych z retestu nazywa się ona `Subject`. Od razu wychodzi na jaw, że identyfikatory są spreparowane, bo nikt się nie pomylił, nie robił dopisków ani nie zdecydował się z jakiegoś powodu NaGlE pIsAć TaK.

Żeby połączyć te bazy, musimy najpierw zdecydować, jak chcemy to zrobić. Możemy albo przyłączyć wyniki z retestu do bazy z testem, albo przyłączyć wyniki z testu do bazy z retestem. Jest to o tyle istotne, że jeśli przyłączymy retest do testu, to będziemy mieli puste wartości u tych osób, które nie wypełniły retestu. Jeśli zrobimy odwrotnie, to z założenia każda osoba, która wypełniła retest, wcześniej wypełniła test, a więc figuruje w pierwotnej bazie. W praktyce bywa różnie. Na przykład ludzie kłamią, że wypełnili test, a jak dostaną link do retestu, to myślą, że w takim razie chociaż to wypełnią. Tak czy inaczej, ta decyzja determinuje typ złączenia, jaki wybierzemy. Najbardziej powszechnym typem jest `LEFT JOIN`, który do każdego wiersza jednej bazy (pisanej jako pierwszej, czyli po lewej) przypisuje pasujący wiersz drugiej bazy (pisanej jako drugiej, czyli po prawej). Jeśli jakiś wiersz w lewej bazie nie ma odpowiednika w prawej bazie, otrzymujemy puste wartości. Jeśli jakiś wiersz w bazie po prawej nie został przypisany do żadnego wiersza po lewej, nie jesteśmy o tym informowani. Więcej o różnych typach złączeń (np. pozwalających uzyskać wszystkie możliwe kombinacje wierszy) można przeczytać i zobaczyć na obrazkach [na przykład tutaj](https://dataschool.com/how-to-teach-people-sql/sql-join-types-explained-visually/).

Ja przyłączę retest do bazy z wynikami pierwszego testu. Widzę jednak dwa problemy, które będę musiał rozwiązać. Po pierwsze, kolumna z identyfikatorem osoby badanej nazywa się inaczej w obu bazach. Po drugie, kolumny `KTR_O` i `KTR_W` nazywają się tak samo w obu bazach. Będę więc musiał wskazać R, na podstawie jakich kolumn ma dokonać złączenia, a także jak ma nazwać kolumny w gotowej bazie, żebym wiedział, które wyniki dotyczą pierwszego testu, a które retestu.

```{r}
db_test %>%
    left_join(
        db_retest,
        by = join_by(ID == Subject),
        suffix = c("", "_retest")
    )
```

Pierwszy problem rozwiązałem za pomocą argumentu `by`. Od wersji `dplyr` 1.1.0 przyjmuje on inną funkcję o nazwie `join_by`. W jej nawiasach precyzujemy, na podstawie jakich kolumn należy dokonać złączenia. Identyczne kolumny łączymy znakiem `==`. Drugi problem rozwiązałem dodając w argumencie `suffix` przyrostki do nazw kolumn. Zawsze zapisuje się je jako zestaw, czyli wewnątrz `c()` i zawsze najpierw jest w cudzysłowie przyrostek lewej bazy (u nas `db_test`), a potem przyrostek prawej bazy (u nas `db_retest`). Ja chciałem, by kolumny pierwotnej bazy nie miały przyrostka, więc za przyrostek dałem pusty ciąg znaków (czyli po prostu nic w cudzysłowie), zaś do kolumn bazy z retestem dodałem przyrostek `"_retest"`. Efekt widać na obrazku -- 5 kolumn i puste wartości u osób, które nie wypełniły retestu.

Jak widać złączenia to zaskakująco szeroki temat, który daje duże możliwości. Omówiona tu funkcja `left_join` jest najczęściej stosowana, ale warto zerknąć do dokumentacji i w tutoriale, żeby chociaż dowiedzieć się, co możemy za pomocą złączeń zrobić.

> Pozostała część już wkrótce.

# Eksploracja danych

# Wykresy

# Testy statystyczne

## Czyste wyniki, czyli pakiet `broom`

# Formatowanie kodu

# Bibliografia