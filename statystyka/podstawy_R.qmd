---
number-sections: true
---

# Podstawy programowania w R {#podstawy-programowania-w-r}

W kilku miejscach w dalszej części wrzucam informacje, jak daną rzecz omawianą teoretycznie można zrobić w R. **Absolutnie nie jest to konieczne do zrozumienia statystyki!** Jest to tylko jedna z możliwości, jak można opisaną dalej teorię przekuć w praktykę. O R można myśleć jako o programie do robienia statystyki. Podobnie jak SPSS, Statistica, Stata czy (oparte na R, darmowe i otwartoźródłowe) jamovi. Jeśli jednak Czytelnik widział kiedyś program statystyczny, spodziewać się będzie ekranu podobnego do Excela, gdzie na górnej belce wybiera się testy statystyczne, jakie chce się przeprowadzić. Praca w R tak nie wygląda. Największa wada i zaleta R polega na tym, że jest on jednocześnie językiem programowania. A to daje bardzo ciekawe możliwości, o których niżej. Także praca w R wygląda tak, że w specjalnym języku piszemy komputerowi, co ma zrobić, potem uruchamiamy te instrukcje i gotowe.

W tym miejscu spróbuję krótko opisać, jak zacząć pracę z R. Nie mam ambicji zrobić pełnego wprowadzenia, bo wyszedłby z tego osobny podręcznik. Mam ambicje dać jakikolwiek zarys fundamentu, który pozwoli Czytelnikowi wyczyścić mało brudne dane i zrobić proste testy. Jeśli ktoś po przeczytaniu tego stwierdzi „Może warto się zagłębić", to znajdzie mnóstwo materiałów, które mu na to pozwolą. Ze swojej strony mogę polecić podręcznik „Język R. Kompletny zestaw narzędzi dla analityków danych" Wickhama i Grolemunda, interaktywne kursy na DataCamp, a pomocy w rozwiązaniu konkretnych problemów zawsze można szukać na StackOverflow.

# 10 powodów dla których warto uczyć się R {#sec-reasons}

Dla niektórych ludzi perspektywa uczenia się języka programowania tylko po to, żeby móc robić testy statystyczne, wydaje się być stratą czasu. Albo budzi lęk. Po co poświęcać tyle czasu i energii dla czegoś, co możemy zrobić poprzez klikanie w obrazki w jamovi? Powodów jest wiele i kilka pozwolę sobie wymienić.

1.  Można zrobić wszystko. Absolutnie wszystko. Nie ma sensu pytać „czy da się to zrobić?", w R pytamy „jak to zrobić?". Wyobraźmy sobie, że chcemy zastosować jakiś test i nagle odkrywamy, że nie ma go w naszym programie statystycznym. Jeżeli jest nim coś w rodzaju Statistici, to możemy co najwyżej napisać do deweloperów z nadzieją, że dołożą. W R prawdopodobnie istnieje już pakiet, który interesujący nas test zawiera. A jeśli nie, to jeśli mamy wzór matematyczny, to wklepanie go w R to nic trudnego.

2.  R to darmowe, otwartoźródłowe oprogramowanie (tzw. FOSS). Wszystkie pakiety, których używam dalej, nie kosztują nic. Dla porównania najpopularniejszy program statystyczny na świecie, SPSS [^podstawy_r-1] w podstawowej wersji w chwili pisania tego tekstu kosztuje od 99 dolarów miesięcznie za osobę. I to nawet nie jest wersja dająca wszystkie możliwości. W R da się policzyć właściwie wszystko to, co w SPSS, korzysta się ze wszystkich innych zalet języka programowania (patrz niżej), a to wszystko za okrągłe 0 dolarów miesięcznie za osobę.

[^podstawy_r-1]: Drugim najpopularniejszym jest R.

<!-- -->

3.  R to język programowania. Pozwala to na szybką automatyzację dużej liczby zadań i radzenia sobie z nieprzewidzianymi okolicznościami. Dla przykładu program do prowadzenia eksperymentów PsychoPy generuje dane w dość pogmatwanych arkuszach kalkulacyjnych, które trzeba dość mocno wyczyścić przed analizą. Problem jest jednak taki, że wyniki każdej osoby badanej zapisują się w osobnych plikach. Jeśli przebadaliśmy 150 osób, mamy 150 plików do wyczyszczenia. Jeśli piszemy analizę w R, to wystarczy wyczyścić jeden plik, a potem dopisać linijkę czy dwie, żeby R tak samo wyczyścił pozostałe 149 plików. Bardzo cenię sobie też możliwość łatwego wygenerowania nazw dla kolumn typu MMPI_001, MMPI_002, ..., MMPI_567. Wpisywanie tego ręcznie to nuda i marnowanie mnóstwa czasu, a w R to jedna linijka kodu.

4.  Analizy są modyfikowalne i wielorazowego użytku. Co mam na myśli? Załóżmy, że zrobiliśmy całą analizę i nagle się orientujemy, że nie usunęliśmy z bazy kilku wierszy, które przez przypadek wytworzyliśmy, jak bawiliśmy się kwestionariuszem, zanim go upubliczniliśmy. Albo prowadzący przy sprawdzaniu prosi, żeby zmodyfikować nieco analizę, bo chce wyniki kwestionariuszy wyrażone jako średnia, a nie suma. Jeśli korzystamy ze zwykłego oprogramowania, w obu przypadkach musimy wyklikiwać analizy od początku. Jeśli nasza analiza jest w R, to mamy ją całą zapisaną przed oczami. Możemy zmienić coś na samym jej początku, kliknąć jeden guzik i gotowe. Jeszcze lepiej -- ta analiza siedzi na naszym komputerze, póki jej nie usuniemy, więc jeśli kiedyś trzeba by było wykonać podobne zadanie, to wystarczy prosta operacja kopiuj-wklej.

5.  R to kompletny zestaw narzędzi dla analityków danych, jak głosi tytuł polecanej na wstępie książki. Nie są to słowa rzucone na wiatr. Korzystając np. z jamovi, zazwyczaj wcześniej czyścimy dane w Excelu. Jeśli potrzebujemy dane zmodyfikować, np. połączyć dwie bazy albo zmienić format szeroki na długi[^podstawy_r-2], to również w Excelu. A to się ciężko robi w Excelu, bo albo robimy to ręcznie, albo musimy nauczyć się Power Query. Potem robimy analizę, ładne wykresy (często wracamy się do Excela w tym celu) i piszemy raport w Wordzie. W R da się zrobić to wszystko. Niezwykle potężne i giętkie czyszczenie danych (za które uwielbiam R) z `tidyverse`, testy statystyczne, ładne wykresy w `ggplot2`, a na końcu nawet raport w Quarto (dawniej RMarkdown). Możemy nie wychodzić z R i często tak będziemy robić.

[^podstawy_r-2]: To trudno sobie wyobrazić, łatwiej zobaczyć na obrazkach. Chodzi o to, czy mamy klasyczną wyczyszczoną bazę, w której jedna osoba badana to jeden wiersz, a każda kolumna to jeden pomiar (np. odpowiedź na pytanie). Czasem chcemy zamiast 11 kolumn (ID osoby badanej + 10 odpowiedzi na pytania) mieć tylko 3 kolumny (ID osoby badanej, numer pytania, odpowiedź). Wtedy każda osoba badana zajmuje 10 wierszy, ale każdy wiersz ma odpowiedź na tylko jedno pytanie. To potrafi być potrzebne do niektórych testów, grupowania danych itp. To jest technikalium, to nie jest statystyka *per se*, ale czasem trzeba to ogarniać, żeby rzeczywiście zrobić analizę. Poświęciłem temu cały podrozdział [-@sec-pivot].

<!-- -->

6.  Wykresy. Wspominałem o tym wyżej, ale wykresy są warte osobnego punktu. Możliwości Excela są ograniczone. Dla przykładu wykres poniżej prezentuje wyniki badania nt. tego, jak wygląda homofobia u mężczyzn, którym się powie, że są niemęscy. W badaniu mierzyliśmy dodatkowo zmienną psychologiczną o nazwie normatywna męskość, czyli niejako stopień przywiązania do reguł, co *prawdziwy mężczyzna* powinien, a czego nie powinien. Wykres jest dość skomplikowany. Zawiera trzy zmienne dychotomiczne, co daje łącznie 8 kolumn w grupach po 4 i 2. Nie powiem, że zrobić taki wykres jest łatwo, bo dobre opanowanie `ggplot2` trochę zajmuje, ale gdy już opanujemy, co jest do opanowania, to hulaj dusza. Nawet najbardziej skomplikowane wykresy stają się możliwe. Gdy połączyć to z możliwościami dawanymi przez programistyczną warstwę R, nagle łatwe staje się wytworzenie serii pasujących do siebie stylem wykresów o tych samych wymiarach. Pozwala to uzyskać pełną spójność bez ręcznego poprawiania szczególików.

<!-- -->

7.  Analizy można napisać niezależnie od procesu zbierania danych. Na III roku studiów miałem projekt studencki, w którym musieliśmy zrobić prosty eksperyment w PsychoPy, przeanalizować dane i zaprezentować wyniki. Termin był krótki, także ostatnie dane większość grup zdążyła zebrać dopiero na dzień przed terminem, w tym moja. Oznaczało to, że osoby robiące analizę statystyczną musiały siedzieć do późna, żeby wyrobić się z prezentacją. Ale że już wtedy umiałem co nieco R, to napisałem całą analizę kilka dni wcześniej, kiedy akurat miałem wolny wieczór. Kiedy zebraliśmy ostatnie dane, wystarczyło kliknąć „Run" i gotowe -- wykresy, testy, w mgnieniu oka policzone dla całego zestawu danych (wyczyszczonych! jak wspominałem wyżej, w PsychoPy czyszczenie danych bywa żmudne).

8.  R jest zgodny z filozofią Open Science. O ruchu Open Science można mówić bardzo długo, ale ogólnie jest to ruch dążący do otwartego dostępu do raportów naukowych (za darmo), surowych danych i dokładnych opisów analiz statystycznych. Jeśli naukowiec załącza do swojego artykułu surową bazę danych i skrypt w R, inni naukowcy mogą sprawdzić, czy tamten nie manipulował danymi, nie stosował podejrzanych praktyk oraz czy po ludzku się gdzieś nie pomylił. Poza tym każdy może powtórzyć jego analizę i sprawdzić, czy rzeczywiście wychodzi, jak naukowiec zaraportował.

9.  R jest świetnym środowiskiem do współpracy. Wiele osób może pracować nad jedną analizą statystyczną podobnie jak teraz pracujemy nad prezentacjami czy plikami w Wordzie -- online. Współpracownicy mogą czytać swój kod, uzupełniać się nawzajem, wprowadzać poprawki i dzielić się pracą nawet w obrębie pojedynczych czynności do wykonania. W programach, w których analizy wyklikujemy, jest to znacznie trudniejsze. Gdy zaś mamy kod przed oczami, widzimy wyraźnie, co współpracownik robi i jak mu pomóc.

10. R wymaga, by wiedzieć, co się robi. Niektórzy mogą to potraktować jako wadę, ale ja sądzę, że to wielka zaleta. Jeśli chcę nauczyć się statystyki, żeby sprawnie zrobić analizy do swojej pracy magisterskiej i rozumieć analizy z artykułów naukowych, to muszę wiedzieć, co robię. Z R ciężko jest nauczyć się, w co klikać, a potem po prostu robić to za każdym razem tak samo. Jest to swego rodzaju wyzwanie, ale cenne.

# Przygotowanie {#sec-preparation}

Żeby zacząć pracować z R, trzeba R pobrać i zainstalować[^podstawy_r-3]. Pobrać R możemy ze strony [r-project.org](https://www.r-project.org). W menu po lewej znajduje się nagłówek *Download*, a pod nim odnośnik *CRAN* (*Comprehensive R Archive Network*, można to nazwać głównym serwerem R). Podchwytliwe jest to, że po wejściu w odnośnik do CRAN pojawi nam się dziwnie wyglądająca lista linków. To strona internetowa pyta nas, z jakiego serwera chcemy pobrać R. Najsensowniej jest wybrać *0-Cloud*, czyli coś, co przekieruje nas do optymalnego serwera. Reszta jest już dość intuicyjna -- pobieramy R dla naszego systemu i instalujemy jak każdy inny program.

[^podstawy_r-3]: Jeśli korzystasz z jakiejś dystrybucji linuxa, R możesz zainstalować z oficjalnych repozytoriów. Pakiet zazwyczaj nazywa się `r-base`. W Ubuntu wystarczy użyć komendy `sudo apt install r-base`.

Druga sprawa to IDE, czyli coś, w czym będziemy pisać nasze skrypty. Ale jak to? R nie wystarczy? Technicznie wystarczy, ale R to program działający z wiersza poleceń. To jest to czarne okienko, gdzie wpisujemy polecenia ręcznie. Większość z nas będzie chciała skorzystać z czegoś bardziej przystępnego niż goły wiesz poleceń. Standardem jest tu program o nazwie RStudio[^podstawy_r-4]. Możemy go pobrać ze strony [posit.co](https://posit.co) w wersji Desktop. Z tym raczej nie będzie już problemu.

[^podstawy_r-4]: W R da się programować też w innych IDE, np. w Visual Studio Code. Niemniej na sam początek lepiej przylgnąć do standardu RStudio, który pomoże nam nabrać najlepszych praktyk i nauczyć się, z czym się R je.

Gdy zainstalujemy i uruchomimy RStudio, naszym oczom ukaże się biały ekran, który na razie nie ma sensu, ale go nabierze. Przed rozpoczęciem pracy warto jest zajrzeć w ustawienia (Tools → Global options...), gdzie możemy zmienić kilka rzeczy. W zakładce Appearance możemy ustawić sobie ciemny tryb[^podstawy_r-5] i poczuć się jak programista. W zakładce Spelling możemy wybrać polski słownik do sprawdzania pisowni. Na ten moment tyle zmian powinno wystarczyć.

[^podstawy_r-5]: Sam używam [rscodeio](https://github.com/anthonynorth/rscodeio).

# Środowisko R i RStudio {#sec-env}

## Konsola, zmienne i matematyka {#sec-console}

Ekran RStudio składa się z trzech okienek. Duże okienko po lewej i dwa mniejsze po prawej. Skierujmy naszą uwagę na okienko po lewej, czyli konsolę. Wita nas ona ciepłą informacją, że R jest zainstalowany i znakiem zachęty `>` zachęca nas do wydawania jej poleceń. Konsola R to miejsce, w którym możemy mówić R, żeby coś dla nas liczył. Można to potraktować jako super kalkulator. Spróbuj -- wpisz w konsolę `2+3*5`, zatwierdź enterem i zwróć uwagę, że R stosuje poprawną kolejność wykonywania działań. Spacje nie mają znaczenia, także możemy wpisać również bardziej estetyczną wersję `2 + 3 * 5`. Nie wiem po co, ale można.

Wynik takiego działania nigdzie się nie zapisuje, tylko wyświetla się w konsoli. Jeśli chcemy zapisać nasz wynik, możemy to zrobić stosując znaczek `<-`[^podstawy_r-6]. Przydatnym skrótem jest tu w RStudio jest alt+-, który od razu wstawia nam tę strzałeczkę. Wyjaśnijmy to na przykładzie.

[^podstawy_r-6]: Jeśli znasz jakiś inny język programowania, możesz zapytać, dlaczego nie używamy `=`. Operator `=` też zadziała, ale za dobrą praktykę w R uznawane jest stosowanie `<-` do przypisywania obiektów do zmiennych, a `=` do ustalania wartości argumentów.

```{r}
#| eval: false
wynik <- 2 + 3 * 5
a <- wynik * 3^3
```

Powyżej zapisałem dwa polecenia, które do konsoli powinniśmy wpisać jedno po drugim i każde z nich zatwierdzić enterem. Pierwsze polecenie mówi coś takiego -- policz `2 + 3 * 5` i zapisz to w zmiennej `wynik`. Po zatwierdzeniu tego polecenia możemy zauważyć, że wynik działania nam się nie wyświetlił, za to w prawym górnym okienku pojawiło się słowo `wynik` i obok wartość `17`. Od tego momentu możemy używać słowa `wynik` zamiast 17. Spróbuj wpisać w konsolę samo słowo `wynik` i zatwierdzić enterem. Konsola informuje nas, że w zmiennej `wynik` kryje się liczba 17. Jeśli teraz wpiszesz np. `wynik * 2`, to konsola zwróci to samo, co zwróciłaby po wpisaniu `17 * 2`. Co więc robi drugie polecenie? Możemy je odczytać jako „W zmiennej o nazwie `a` zapisz wynik mnożenia `wynik` i 3 do potęgi 3". Operator `^` to właśnie potęgowanie. Jeśli wpiszemy w konsolę `a`, naszym oczom ukaże się wynik 459.

Jak się potem okaże, w zmiennych możemy zapisywać dużo więcej, niż tylko wyniki prostych działań matematycznych. W identyczny sposób do odpowiednich zmiennych trafią wyniki testów statystycznych albo całe bazy danych. Ale o tym dalej.

## Funkcje {#sec-functions}

Tym, co robi robotę w R (jak i w każdym innym języku programowania) są funkcje. Funkcja to taka maszynka, do której coś wrzucamy, ona nam to przekształca i wywala coś innego. Tak jak w matematyce. Podobnie jak w matematyce, funkcje zapisujemy konwencją `f(x)`, czyli `nazwa(co_wrzucam_do_funkcji)`. Dla przykładu funkcja `seq` pozwala nam wytwarzać sekwencje liczb. Musimy do tej funkcji wrzucić od jakiej liczby chcemy zacząć, na jakiej chcemy skończyć i jaki chcemy mieć krok. Dla przykładu:

```{r}
seq(5, 62, 3)
```

To, co wrzucamy do funkcji, nazywamy argumentami. Funkcja `seq` wie, że ma zacząć od 5 i skończyć na 62, a nie zacząć od 62 i skończyć na 5, bo ma pod maską zapisane, w jakiej kolejności będzie dostawać te liczby. Takie argumenty nazywamy pozycyjnymi -- funkcja wie, co to jest, na podstawie pozycji. W R każdy argument możemy też nazwać. Dla przykładu wiemy, że funkcja `seq` oczekuje argumentów `from`, `to` i `by`. Możemy więc wprost powiedzieć funkcji, że oto dajemy jej `from`, `to` i `by`.

```{r}
seq(from = 5, to = 62, by = 3)
seq(to = 62, by = 3, from = 5) # jeśli nazywamy argumenty, kolejność nie ma znaczenia
```

Tego typu argumenty nazywamy kluczowymi (*keyword*). W praktyce wykorzystuje się mieszankę jednego i drugiego typu argumentów. Nazywanie argumentów zwiększa czytelność kodu, ale czasem pozycja jest wystarczająco jasna. Dla przykładu mogę napisać `sqrt(x = 9)`, żeby wyciągnąć pierwiastek kwadratowy (*square root*) z 9, ale czy zapis `sqrt(9)` jest jakkolwiek mniej jasny?

Czasami też używamy argumentów kluczowych, żeby przestawić jakieś ustawienia domyślne albo odblokować nowe możliwości. Dla przykładu funkcja `seq` dysponuje dodatkowym argumentem `length.out`. Jeśli ustawimy `length.out`, możemy ustalić długość naszego wyniku zamiast punktu końcowego albo kroku.

```{r}
seq(5, by = 3, length.out = 10) # daj mi 10 kolejnych liczb zaczynając od 5 i co 3
seq(1, 100, length.out = 10) # podaj 10 liczb między 1 a 100
```

## Dokumentacja {#sec-docs}

Różne funkcje przyjmują różne argumenty. Podobnie jak nie powiemy piekarzowi, żeby stosował białą fugę do chleba, tak samo nie wrzucimy do funkcji `seq` słów zamiast liczb. Tak jak musimy wiedzieć, że piekarz zajmuje się pieczywem, tak samo musimy znać funkcje, których używamy. W poprzednim podrozdziale wiedzieliśmy, co można wrzucić do funkcji `seq` i jak nazywają się jej argumenty, bo to napisałem. Tak samo podałem ot tak, że argument funkcji `sqrt` nazywa się `x`. Skąd mam to jednak wiedzieć?

Nie bez powodu mówimy o *językach programowania* -- wiele funkcji nauczymy się na pamięć i będziemy po prostu wiedzieć, jak z nich korzystać. Jednak znacznie częściej, w wielu przypadkach też dla funkcji, które znamy, będziemy korzystać z dokumentacji. R dysponuje świetną dokumentacją dla każdej funkcji[^podstawy_r-7]. Zawiera ona opis, co dana funkcja robi, jakie argumenty przyjmuje, a często nawet tło teoretyczne jej działania. Żeby dostać się do dokumentacji danej funkcji, wywołujemy ją **w konsoli** ze znakiem zapytania, np. `?seq`. Powoduje to, że w okienku *Help* po prawej wyświetla nam się pełna dokumentacja tej funkcji. Nie trzeba więc sięgać do Google, żeby uzyskać odpowiedź na podstawowe problemy. O ile wiemy, jakiej funkcji chcemy użyć. Zachęcam do częstego sięgania do dokumentacji. To absolutnie podstawowe narzędzie w programowaniu czegokolwiek.

[^podstawy_r-7]: Z założenia. Struktura dokumentacji jest bardzo dobra, a każda funkcja w pakietach CRAN musi być udokumentowana. Zdarzało mi się jednak mieć problem ze zrozumieniem, o co chodzi autorom funkcji, co ma być czym. Jak to w życiu bywa, niektórzy tłumaczą lepiej, a inni piszą treść tego przypisu.

Świetnym źródłem informacji o funkcjach, pozwalającym również znaleźć odpowiednią funkcję do naszego celu, są ściągi (*cheat sheets*). Pakiety `tidyverse` mają nawet swoje oficjalne ściągi, które na początku swojej nauki R wydrukowałem i zalaminowałem. Polecam je gorąco, zwłaszcza do pakietów `dplyr`, `ggplot2` i `stringr`. Można je znaleźć bezpośrednio w RStudio wybierając Help → Cheat sheets → Browse all cheat sheets albo na stronie Posit, czyli firmy, która wypuszcza RStudio. Jak przejdziemy dalej, do części praktycznej, polecam, żeby mieć te ściągi już przygotowane, tuż obok.

## Skrypty {#sec-scripts}

Wpisaliśmy w konsolę już sporo rzeczy i historię naszych komend możemy zobaczyć przechodząc do odpowiedniej zakładki w prawym górnym okienku. Jednak wyjście z programu może nam skutecznie skasować tę historię. Jeśli mamy całą sporą analizę statystyczną, która składa się z 200 linijek kodu, to chcielibyśmy mieć jakiś sposób na zapisanie tego na przyszłość, żeby nie musieć za każdym razem wklepywać tego kodu z pamięci. Zaopatrujemy się więc w gruby zeszyt w linie i wszystkie komendy piszemy również tam. Żarcik. Do przechowywania kodu służą specjalne pliki zwane skryptami. Tak jak mamy pliki .pdf, .txt, .docx, tak w plikach .R zapisujemy kod R.

Najprościej wytworzyć nowy skrypt klikając w biały kwadracik z plusem w lewym górnym rogu RStudio. Spowoduje to otworzenie listy rzeczy, które możemy wytworzyć. Nas w tej chwili interesuje `R Script`. Gdy utworzymy nasz skrypt, otworzy się on nad konsolą. Warto od razu zapisać go na dysku skrótem Ctrl+S (lub File → Save). Warto się upewnić, że zapisywany plik rzeczywiście kończy się rozszerzeniem .R.

Na razie nasz skrypt jest pusty, ale możemy w nim pisać dowolne polecenia tak samo, jak napisalibyśmy w konsoli. Różnica jest taka, że nie są one od razu wykonywane. Skrypt to tekst. Jeśli chcemy wykonać jakieś polecenie ze skryptu, to albo kopiujemy je do konsoli, albo umieszczamy na nim kursor i klikamy ctrl+enter. Możemy też myszką zaznaczyć większy fragment kodu i kliknąć Ctrl+Enter, żeby go wykonać. Jeśli chcielibyśmy wykonać cały nasz skrypt, to zaznaczamy cały kod (Ctrl+A) i ponownie używamy Ctrl+Enter. Ewentualnie możemy skorzystać ze skrótu Ctrl+Shift+S[^podstawy_r-8].

[^podstawy_r-8]: Piszę o wielu skrótach klawiszowych, bo umiejętność pisania kodu bez odrywania rąk od klawiatury znacząco przyspiesza pracę. Jednocześnie wiele z tych rzeczy da się wyklikać myszką. Zachęcam jednak, żeby próbować uczyć się skrótów. Listę wszystkich skrótów w RStudio znajdziemy w menu Tools → Keyboard Shortcuts Help (albo pod skrótem Alt+Shift+K).

To jest najważniejsza różnica między skryptem a konsolą -- cokolwiek wpisane w konsolę jest wykonywane natychmiast i znika. Z konsoli korzystamy, kiedy chcemy zrobić jakieś jednorazowe operacje albo coś sobie przetestować. W skrypt wpisujemy to, co chcemy zachować. Ewentualnie szkic. Gdy ludzie przechodzą nagle z konsoli do skryptu, bardzo często zaczynają wpisywać w swój skrypt różne śmieci, które wcześniej wpisaliby w konsolę. Konsola nie zniknęła, ciągle jest do naszej dyspozycji. Skrypt w swojej ostatecznej postaci powinien jednak działać tak, że jak go uruchomimy, to cały przeleci bez błędów. No, przynajmniej do tego dążymy. Czyli konsola do testów, skrypt do prawdziwego kodu.

## Komentarze {#sec-comments}

Jeśli chcielibyśmy zrobić w skrypcie jakąś notatkę dla siebie, która nie jest kodem używamy znaczka `#`. Jest to tzw. komentarz. Możemy na przykład napisać:

```{r}
#| eval: false
print("Hello world!") # czuję się programistą
```

Jeśli wykonamy taką linijkę, konsola zignoruje wszystko po znaku `#`. Pozwala to nam zostawiać sobie notatki w rodzaju `# hipoteza 1` albo `# nie wiem, czemu to działa, ale działa`. Komentowanie kodu może nam (i naszym współpracownikom) ułatwić zrozumienie, o co nam chodziło, gdy to pisaliśmy. Jeśli chcemy zaopatrzyć nasz kod w nagłówki, konwencja mówi, żeby formatować je tak:

```{r}
# Przygotowanie ----

## Ładowanie danych ----

# kod ładujący dane

## Ładowanie bibliotek ----

# kod ładujący biblioteki
```

Każdy znaczek `#` to niższy poziom nagłówka, czyli wytworzyłem sekcję `Przygotowanie`, a w niej dwie podsekcje `Ładowanie danych` i `Ładowanie bibliotek`. Takich poziomów nagłówków możemy mieć ile chcemy. Nagłówek tym się różni od zwykłego komentarza, że po nim występują cztery myślniki `----` lub inne znaki. Tak sformatowane nagłówki wyświetlają się w bocznym panelu RStudio i pozwalają się lepiej ogarnąć w długim kodzie. Panel *outline* możemy rozwinąć skrótem Ctrl+Shift+O albo klikając skrajną prawą ikonkę nad edytorem skryptu (poziome kreski na prawo od guzika `Source`).

## Projekty {#sec-projects}

Zazwyczaj projekt badawczy składa się z wielu plików. Nie jest to tylko kod R, ale też chociażby pliki z danymi i inne. Zazwyczaj trzymamy to wszystko w jednym folderze, o ile utrzymujemy jakikolwiek porządek w plikach. Możemy też mieć całe studia luzem na pulpicie, nie oceniam. RStudio pomaga nam w zarządzaniu takimi grupami plików poprzez projekty. Projekty w RStudio robią kilka rzeczy, m.in. pozwalają ustawić niestandardowe opcje (np. zmienić język słownika na angielski tylko dla tego jednego projektu), zapamiętać otwarte okna i ich układ, ale przede wszystkim pomagają nam lokalizować pliki znajdujące się w tym samym folderze[^podstawy_r-9]. Zawsze, kiedy planujemy zachować jakiś zbiór powiązanych plików na dłużej, warto jest wytworzyć projekt.

[^podstawy_r-9]: Ustawiają katalog roboczy (*working directory*), dzięki czemu, kiedy musimy powiedzieć, gdzie jest, powiedzmy, plik z danymi, możemy napisać `"./dane/dane.xlsx"`. `./` oznacza "w tym folderze", `/dane/` oznacza "w podfolderze dane", a `/dane.xlsx` oznacza "w pliku `dane.xlsx`". Możemy to też zrobić ręcznie funkcją `setwd`.

Projekty tworzymy i otwieramy przez guzik w prawym górnym rogu. Rozwijane menu pozwana nam wytworzyć nowy projekt, a wyskakujące okienko pyta, czy wytworzyć go w już istniejącym folderze, stworzyć nowy folder, czy może pobrać repozytorium Git. Jeśli wybraliśmy nowy folder, mamy kilka typów projektów do wyboru, ale w większości przypadków wybieramy po prostu `New Project`. Okienko pozwala nam nadać projektowi nazwę, wybrać jego lokalizację, a także wytworzyć puste repozytorium Git[^podstawy_r-10]. RStudio wytworzy nam w ten sposób plik .Rproj organizujący nasz projekt.

[^podstawy_r-10]: Jeśli nie wiesz o co chodzi, nie przejmuj się. To duży temat i nie jest to coś, bez czego nie da się żyć.

## Pakiety {#sec-libs}

Pakiety (*packages* lub *libraries*) to pewnie dodatki do R, które rozszerzają jego możliwości. Dla przykładu -- R w swojej podstawowej wersji nie ma funkcji liczącej skośność. Nie jest to jednak żaden problem, bo możemy R rozszerzyć np. o pakiet o nazwie `e1071` albo `moments`. Oba te pakiety dodają nam do R możliwość szybkiego i prostego policzenia skośności. Pakiety -- w olbrzymiej większości -- są darmowe.

Absolutnie podstawowym pakietem, czy właściwie zbiorem pakietów, jest `tidyverse`. `Tidyverse` usprawnia R właściwie we wszystkim, co w podstawowej wersji jest niewygodne -- `readr` (*czyt.* rider) pozwala łatwo ładować dane, `dplyr` (*czyt.* diplajer) niesamowicie usprawnia czyszczenie danych, `lubridate` i `stringr` (*czyt.* stringer) to podstawowe narzędzie do pracy z datami i z tekstem[^podstawy_r-11], nie mówiąc już o `ggplot2`, czyli najpotężniejszym narzędziu do tworzenia wykresów. Na szczęście nie musimy wszystkich tych pakietów przywoływać z osobna, bo możemy załadować je wszystkie naraz, ładując jeden zbiorczy pakiet `tidyverse`. Współcześnie `tidyverse` to podstawowy sposób programowania w R. Pakiety ładujemy za pomocą funkcji `library`, do której wrzucamy nazwę pakietu w cudzysłowie. Nasz skrypt zaczniemy więc od takiej instrukcji:

[^podstawy_r-11]: Ciągi znaków tekstowych w informatyce określamy mianem *string*, stąd `stringr`.

```{r}
#| eval: false
library("tidyverse")
```

Jeśli robimy to po raz pierwszy, to po wykonaniu skryptu konsola wyrzuci nam błąd `Błąd w poleceniu 'library("tidyverse")':nie ma pakietu o nazwie ‘tidyverse’`. Wynika to z faktu, że instrukcja `library` tylko ładuje pakiet, ale przed załadowaniem trzeba go pobrać i zainstalować. Na szczęście robimy to tylko raz, zawsze później wystarczy samo `library`. Dlatego też nie będziemy wpisywać komendy instalującej pakiet do skryptu, tylko bezpośrednio do konsoli. Nie chcemy w końcu, żeby pakiet `tidyverse` instalował się za każdym razem, kiedy będziemy uruchamiać skrypt. Będzie to niemiłosiernie spowalniało skrypt i wymuszało dostęp do Internetu. Dlatego też do konsoli wpisujemy:

```{r}
#| eval: false
install.packages("tidyverse")
```

Innym sposobem instalowania pakietów jest skierowanie się w prawe dolne okienko w RStudio, przejście do zakładki Packages, kliknięcie guzika Install, wpisanie nazwy pakietu w wyskakującym okienku (już bez cudzysłowu) i zatwierdzenie guzikiem Install.

Gdy zainstalujemy już pakiet `tidyverse` -- dowolną z metod -- ponownie próbujemy go załadować, tym razem, mam nadzieję, już bez błędu. Konsola poinformuje nas wtedy co dokładnie załadowała.

```{r}
library("tidyverse")
```

# Ładowanie danych z `readr` {#sec-loading}

Jeśli chcemy cokolwiek liczyć, musimy mieć na czym liczyć\[podstawy_r-23\]. R ma wiele sposobów ładowania danych, które w każdym podręczniku zajmują cały rozdział. R poradzi sobie z prawie każdym formatem danych, a także potrafi ładować je bezpośrednio z serwera (dla przykładu pakiet `googlesheets4` pozwala ładować pliki z Google Sheets bez pobierania ich na dysk albo `QualtRics` ułatwiający importowanie plików z danymi ankietowymi z Qualtrics).

\[podstawy_r-23\]: Wszystkie bazy, których używam w tym tekście, są dostępne w repozytorium na GitHubie. Można wejść w [ten link](https://github.com/jakub-jedrusiak/jakub-jedrusiak.github.io/tree/main/statystyka/dane/podstawy-R), wybrać pożądany plik i albo pobrać go na własny dysk, albo zamiast ścieżki do pliku użyć linku. By uzyskać działający link, należy na koniec właściwego linku dopisać `?raw=true`. Takie linki można wklejać w komendy czytające dane zamiast ścieżek. Przykład linku, jak i tego, jak używać ich z funkcjami typu `read_csv` podaję w podrozdziale [-@sec-summarise] o grupowaniu.

Podstawowe pakiety do ładowania danych do `readr` i `readxl` dla plików Excela. Na początek nie trzeba znać struktury funkcji ładujących dane, bo RStudio dysponuje przyjemnym graficznym narzędziem do tego celu. Jeśli w prawym dolnym oknie, w zakładce Files, klikniemy na plik zawierający dane, dostaniemy do dyspozycji opcję Import Dataset... Po jej wybraniu otworzy nam się okno z podglądem danych i kilkoma opcjami do dostosowania, m.in. czy jakichś wierszy nie pominąć albo z którego arkusza pobrać dane. W plikach .csv czasem musimy też wybrać rodzaj separatora i znaku dziesiętnego (opcja Locale). W polskich plikach z danymi separatorem jest zazwyczaj średnik, a znakiem dziesiętnym przecinek, podczas gdy w angielskich danych będą to odpowiednio przecinek i kropka. Jeśli nie ustawimy znaku dziesiętnego, R może potraktować liczby z przecinkiem jako tekst. Poza tym możemy kliknąć w każdy nagłówek kolumny i wybrać typ danych. Dane powinny być czyste, tj. kolumny liczbowe powinny zawierać same liczby. Jeśli dane nie są czyste (a zazwyczaj nie są), możemy je wyczyścić potem.

Możemy kliknąć Import, by dopełnić dzieła, ale jeśli tworzymy skrypt, będzie nas interesowała komenda, jaką RStudio dla nas przygotowało w czarnym okienku w prawym dolnym rogu. W pierwszej linijce RStudio proponuje załadowanie odpowiedniego pakietu. Warto to wpisać na początku skryptu, w miejscu, gdzie załadowaliśmy `tidyverse`. Jeśli proponowanym pakietem jest `readr`, nie musimy ładować go oddzielnie, bo ładując pakiet `tidyverse` załadowaliśmy od razu `readr`. Druga linijka do właściwe ładowanie danych. Widzimy znaną nam już składnię `nazwa <-`. Nasze dane potrzebują jakiejś nazwy, za pomocą której będziemy się do niej odnosić. Może być to `df`, `dane`, `warunek_kontrolny` czy cokolwiek innego. Za strzałką mamy funkcję odczytującą dane. Najpewniej będzie to `read_csv()` albo `read_excel()`. Do tych funkcji wrzucamy jako pierwszy argument lokalizację naszego pliku z danymi w cudzysłowie ('pojedynczym' lub "podwójnym"). Ponieważ nam komendę przygotowało RStudio, możemy ją po prostu skopiować i wkleić do naszego skryptu, zmieniając tylko nazwę zmiennej. Co robią poszczególne argumenty, możemy sprawdzić w dokumentacji. Jeśli dane ładuje `readr`, wykonanie takiej komendy spowoduje ukazanie się informacji zwrotnej -- jakie kolumny załadowano i jaki mają typ. Dla przykładu:

```{r}
df <- read_csv("dane/podstawy-R/complex_database.csv")
```

Już widzę, że nic mi się nie zgadza, z 18 kolumn 17 zostało rozpoznane jako kolumny tekstowe (`chr`), a tylko kolumna ID jako liczby (`dbl`). Za chwilę będziemy to czyścić. Żeby wyświetlić nasze dane, mogę wpisać w konsolę nazwę, pod którą je zapisałem. Nasza baza pojawiła się też w okienku w prawym górnym rogu. Kliknięcie na nią tam spowoduje wyświetlenie jej w oddzielnej karcie. Warto zauważyć, że nie możemy jej tam edytować.

```{r}
df
```

# Data wrangling {#sec-wrangling}

Tutaj zaczyna się zabawa. *Data wrangling* to zbiorcze określenie na wszystkie te zdarzenia, kiedy musimy zmienić formę naszych danych, na przykład wziąć tylko niektóre kolumny, odfiltrować jakieś przypadki (np. wybrać tylko osoby z grupy kontrolnej), całkiem zmienić formę danych (np. z szerokiej na długą), dodać nowe zmienne czy przypadki (np. zsumować wyniki kwestionariusza), zmodyfikować kolumny (np. odwrócić punktację w jakiejś pozycji kwestionariusza) itp. itd. Tak naprawdę to, a nie samo wykonywanie testów statystycznych, zajmuje najwięcej czasu i powoduje najwięcej problemów. Jak więc się za to zabrać?

Zarówno tutaj, jak i później, nie będę wchodził w to, *dlaczego* to działa jak działa. Większość poradników R opisuje, jak wykonać pewne operacje w podstawowym R, a następnie jak to robi się współcześnie, czyli wykorzystując pakiet `tidyverse`. O ile znajomość podstawowego R jest niezbędna do wykonywania bardziej skomplikowanych operacji i przydaje się, gdy coś nie działa i trzeba to jakoś naprawić, to tutaj jednak skupię się na podstawach, a współcześnie podstawy to `tidyverse`.

## Wybieranie kolumn i wierszy {#sec-choosing}

Bardzo często będziemy potrzebowali tylko określonych kolumn albo tylko określonych przypadków. Przeglądając nasze dane zauważamy, że składają się w dużej części z niepotrzebnych kolumn, jakie wygenerował dla nas program do ankiet. Kolumny takie jak godziny wypełniania są nam niepotrzebne do analizy. Nasze dane to fragment bazy danych z badania, w którym mówiliśmy mężczyznom, że są mało męscy i patrzyliśmy, jak to wpłynie na nich homofobię. Badaliśmy więc wyłącznie mężczyzn, a mimo to ankietę próbowało też wypełnić kilka kobiet i osób o innej płci. Ponieważ ankieta nie dopuściła ich nawet do metryczki, widzimy w ich przypadkach wartości `NA`, co w R oznacza „brak danych".

### Filtrowanie wierszy z `dplyr::filter` {#sec-filter}

Zacznijmy od tego, że w naszej bazie zostawimy tylko mężczyzn. Wszystkie komendy poniżej wpisuję w konsoli, dla testów. Jeśli wpisujemy komendy modyfikujące dane w konsolę, to nie zapisujemy zmian, tylko sprawdzamy, co się stanie, jak tak zrobimy. Dopiero na koniec podam, jak nasze zmiany rzeczywiście zapisać. Jak więc odfiltrować nie-mężczyzn? Robimy to za pomocą komendy `filter()`[^podstawy_r-12]. `Tidyverse` opiera się o intuicyjnie brzmiące czasowniki takie jak `filter`, `select`, `group_by`, `summarise` itd. Komenda `filter` przyjmuje naszą bazę i jakieś warunki, np. płeć męska. W naszym wypadku będzie to wyglądać tak:

[^podstawy_r-12]: `filter` wybiera wiersze spełniające jakiś warunek. Jeśli chcemy wybrać wiersze na podstawie ich pozycji (np. pierwsze 10 wierszy), użyjemy funkcji `slice`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna")
```

Dobra, co to jest `%>%`? Nie było o tym mowy. Owszem, nie było, ale to bardzo wygodna rzecz. Nazywa się *pipe* (czasem tłumaczone jako *potok*). Mówi mniej więcej „wrzuć to do tego". W naszym przykładzie `df %>% filter()` oznacza „wrzuć bazę danych `df` do funkcji `filter`", czyli dokładnie to samo, co `filter(df)`[^podstawy_r-13]. Po co więc w ogóle bawić się w potoki? Bo pozwalają nam wygodnie łączyć komendy w ciągi, jak zobaczymy za chwilę. Do wstawiania potoków służy nam wygodny skrót klawiszowy Ctrl+Shift+M, który jest chyba najczęściej stosowanym skrótem przy pisaniu dowolnego programu. Druga kwestia to podział na linijki. Rozbiłem tę komendę na dwie linijki dla czytelności, ale spokojnie mógłbym zapisać to w jednej linijce. Warto jednak pisać kod tak, żeby dało się go potem łatwo czytać. RStudio podpowiada nam też wcięcia, żebyśmy widzieli, że te linijki tworzą jedną całość. Potem opiszę, jak wygodnie formatować kod.

[^podstawy_r-13]: $* < 0,05$; $** < 0,01$; $*** < 0,001$.

Ta komenda oznacza „weź zmienną `df`, wrzuć ją do komendy `filter` i zostaw tylko te przypadki, w których w kolumnie `Płeć` jest wartość `"Mężczyzna"`." Nazwy kolumn piszemy bez cudzysłowu, ale jeśli wartość komórki to tekst, to zawsze piszemy go w cudzysłowie. Inaczej R pomyśli, że podajemy mu jakąś zmienną, z której ma dopiero odczytać, co ma być w kolumnie `Płeć`. Nam chodzi o dosłowny tekst `"Mężczyzna"`.

Ostatecznie zostaje operator logiczny. Dlaczego piszę `==` zamiast `=`? W programowaniu znak `=` służy do przypisywania wartości do zmiennych. Zapis `a = 5` oznacza „niech `a` ma wartość 5". Sprawdzenie *czy* `a` ma wartość 5 odbywa się poprzez komendę `a == 5`. Konsola wyrzuci nam wtedy `TRUE`, `FALSE` albo `BŁĄD: nie znaleziono obiektu 'a'`. Kilka innych operatorów logicznych prezentuje tabela.

| **Operator** |            **Znaczenie**             |                       **Przykład** |
|------------------|:--------------------------:|-------------------------:|
| ==           |              równa się               |              `Płeć == "Mężczyzna"` |
| !=           |            nie równa się             |                `Płeć != "Kobieta"` |
| \> (\>=)     |  większe niż<br>(większe lub równe)  |                        `Wiek > 40` |
| \< (\<=)     | mniejsze niż<br>(mniejsze lub równe) |                        `Wiek < 40` |
| \|           |                 lub                  |           `Wiek < 18 \| Wiek > 60` |
| &            |                  i                   |  `Płeć == "Mężczyzna" & Wiek > 40` |
| %in%         |        zawiera się w zbiorze         |   `Płeć %in% c("Kobieta", "Inna")` |
| !            |             zaprzeczenie             | `! Płeć %in% c("Kobieta", "Inna")` |

: Część operatorów logicznych dostępnych w R.

### Wybieranie kolumn z `dplyr::select` {#sec-select}

Odfiltrowaliśmy więc nie-mężczyzn. Kolejny problem to cała seria niepotrzebnych kolumn. Godziny, adres, zgoda etyczna (która była obowiązkowa i wszyscy się zgodzili) i płeć (już jednakowa dla wszystkich) są nam do niczego niepotrzebne. Do wybierania, jakie kolumny zostawić, służy funkcja `select()`. Wrzucamy do niej nazwy albo numery kolumn, które chcemy zostawić w bazie. Rozszerzmy więc naszą poprzednią instrukcję o dodatkową komendę za pomocą potoku.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18)
```

Po pierwsze zauważmy, że wystarczyło dodać potok i kolejną komendę. Teraz cała nasza instrukcja oznacza „Weź `df`, odfiltruj mężczyzn i potem wybierz kolumny `Id` `Wiek (ukończony w latach)`, `Wykształcenie` oraz kolumny od 9. do 18.". Do tego więc służą potoki -- pozwalają naraz wykonać całą serię modyfikacji tego samego obiektu. Wypada tu wyjaśnić dwie sprawy. Po pierwsze kolumna z wiekiem zawiera w nazwie spacje. Jeśli nazwa kolumny zawiera niestandardowe znaki, trzeba ją otoczyć znakami \` (*pol.* grawis, *ang.* *backtick*), który znajduje się na klawiaturze tuż pod Esc. Druga rzecz to `9:18`, co znaczy „liczby od 9 do 18" i jest wygodnym, skrótowym zapisem `seq(9, 18)`.

Ewentualnie możemy chcieć powiedzieć, żeby zostawić wszystkie kolumny *poza* jakąś kolumną. Jeśli chcemy wykluczyć 2 kolumny z 200, to lepiej wskazać te 2 do wywalenia niż pozostałe 198 do zachowania. Możemy to zrobić dodając przed kolumną `-`. Możemy ustawić minus zarówno przed nazwą kolumny lub zakresem, ale warto zauważyć, że zakres pozycji trzeba wziąć w nawias. Inaczej R pomyśli, że chodzi nam np. o kolumny od -2 do 5. Gdzie nie jest to głupie, kolumna -2 oznacza „druga od końca".

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(-(2:5), -`Wyrażam świadomą i dobrowolną zgodę na udział w badaniu.`)
```

## Zmiana nazw kolumn z `dplyr::rename` i `purrr::set_names` {#sec-rename}

Co zrobić, jeśli chcemy w naszej bazie coś pozmieniać? Zacznijmy może od zmiany nazw kolumn, żeby łatwiej nam się pisało dalsze komendy. Do tego służą komendy `rename` z pakietu `dplyr` i `set_names` z pakietu `purrr`[^podstawy_r-14]. `rename` służy do zmiany nazw pojedynczych kolumn i przyjmuje argumenty w postaci `rename("nowa_nazwa" = "stara nazwa")`. Za jednym zamachem możemy zmienić ile nazw chcemy. Jeśli chcemy zmienić wszystkie nazwy, wygodniejsza jest funkcja `set_names` do której **po kolei** wrzucamy nowe nazwy, których potrzebujemy. Znowu -- nazwy to dosłowne ciągi znaków, więc zawsze piszemy je w cudzysłowie.

[^podstawy_r-14]: Zdążyłem zauważyć, że różne funkcje `set_names` występują w różnych pakietach i wiele razy zdarzyło mi się szukać, dlaczego mój kod nie działa, jak wszystko jest dobrze. Okazywało się potem, że R bierze `set_names` nie z `purrr`, tylko np. z `magrittr`, a to są zupełnie inne funkcje. Jeśli R napotyka w dwóch różnych pakietach funkcje o takich samych nazwach, domyślnie wybiera tę załadowaną **później**. Jeśli więc najpierw załadowałem `tidyverse` a potem `magrittr`, to gdy piszę `set_names` R wybiera wersję z `magrittr`. Na szczęście możemy wprost powiedzieć R, z którego pakietu ma brać daną funkcję, używając jej pełnej nazwy `purrr::set_names`. Warto też wiedzieć, że używając takiego zapisu nie musimy ładować całego pakietu, żeby użyć funkcji z danego pakietu, której np. używamy tylko raz w całym kodzie. Osobiście, dla spokoju ducha, akurat `set_names` zawsze zapisuję `purrr::set_names`. Zbyt dużo czasu straciłem na szukanie tego błędu.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_"))
```

Co znowu namieszałem? Czemu znowu coś utrudniam? Cóż, żeby ułatwić. o ile kolejne nazwy `"id"`, `"wiek"` i `"wyksztalcenie"` rozumieją się same przez się, to do czego służy tutaj funkcja `paste`? Jeśli zerkniemy w bazę danych, zauważymy, że kolejne 10 kolumn to to samo pytanie „Takie widoki w przestrzeni publicznej są normalne". Odpowiedź na to pytanie (znajdujące się pod obrazkiem neutralnym lub przedstawiającym parę jednopłciową) traktowaliśmy jako wskaźnik homofobii. Jest bardzo częste, że czyszcząc dane z badania mamy serię odpowiedzi z jednego kwestionariusza. Zazwyczaj wszystkie te pytania nazywamy według jednej konwencji np. wszystkie odpowiedzi z kwestionariusza TIPI nazywamy `TIPI_1`, `TIPI_2`, `TIPI_3` itd. Ale po co pisać te etykiety ręcznie, skoro możemy je wygenerować? Do tego służy funkcja `paste`.

```{r}
paste("H", 1:10, sep = "_")
```

Jak widzimy, `paste` wygenerowało nam 10 kolejnych etykiet łącząc `"H"` i liczby od 1 do 10. Argument `sep = "_"` mówi, żeby między kolejnymi kawałkami wstawiać podkreślnik. Do `paste` możemy wrzucić dowolną liczbę znaków do połączenia. Jeśli nie chcemy żadnego separatora, możemy ustawić `sep = ""`, czyli pusty ciąg znaków w separatorze albo możemy użyć bliźniaczej funkcji `paste0`, która nie ma separatora.

## Zmiana wartości komórek z `dplyr::mutate` i `readr::parse_number` {#sec-values}

Jak widzimy, odpowiedzi na pytania z homofobią zawierają nie tylko liczby, ale też tekst z legendą. My jednak chcemy zostawić same liczby. Jeśli spojrzymy na kolumnę z wiekiem, możemy zauważyć, że jest to również jest kolumna tekstowa. Dziwne, przecież wiek to (tylko) liczba. Przejrzenie danych pozwala stwierdzić, że respondent o id 50 w pytaniu o wiek wpisał „18 (2021)". Nieważne, jak się będziemy przed tym bronić, co dopiszemy do pytania o wiek, zawsze znajdzie się ktoś, kto zrobi w nim elaborat. Ten jeden respondent sprawił, że cała kolumna nie jest traktowana jako kolumna liczbowa, ale jako tekst. Odpowiedź na oba te problemy jest taka sama -- `mutate` i `parse_number`.

Funkcja `mutate` to ogólna funkcja, za pomocą której modyfikujemy kolumny albo dodajemy nowe. Jej składnia wygląda następująco:

```{r}
#| eval: false
zmienna_z_danymi %>%
    mutate(
        kolumna_do_modyfikacji = jakas_funkcja(kolumna_do_modyfikacji),
        nowa_kolumna = inna_funkcja(jak_stworzyc_nowa_kolumne)
    )
```

`mutate` w pewnym sensie zawsze tworzy nową kolumnę. Jeśli nowa kolumna ma taką samą nazwę, jak stara, to zastępuje starą. W naszym przykładzie chcemy do całej kolumny `wiek` zastosować funkcję `parse_number`, która pozbywa się wszystkiego poza pierwszą napotkaną liczbą[^podstawy_r-15]. Taka instrukcja będzie wyglądała następująco:

[^podstawy_r-15]: Jej zachowanie możemy modyfikować, np. mówiąc jej, że w liczbach typu `128 242,78` spację ma traktować jako rozdzielacz dużych liczb, a przecinek jako operator dziesiętny (w krajach anglosaskich tę funkcję pełni zazwyczaj kropka). Inaczej `parse_number("128 242,78")` zwróci nam pierwszą napotkaną liczbę, czyli 128. Szczegóły można znaleźć w dokumentacji, ale w takim wypadku powinniśmy użyć `parse_number("128 242,78", locale = locale(grouping_mark = " ", decimal_mark = ","))`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek)
    )
```

### Przekształcanie wielu kolumn jednocześnie z `dplyr::across` {#sec-across}

Po wykonaniu tej funkcji widzimy, że cała kolumna jest już numeryczna. To samo możemy zrobić dla pytań z homofobią. Moglibyśmy, oczywiście, zapisać `H_1 = parse_number(H_1), H_2 = parse_number(H_2)` itd., ale po co się męczyć? Na początku roku 2020 dostaliśmy cudowną funkcję pomocniczą `across`, która przydaje nam się w takich dokładnie wypadkach, czyli gdy chcemy zmodyfikować serię kolumn w taki sam sposób. Jak jej używać?

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(starts_with("H_"), parse_number)
    )
```

Pierwszą rzeczą, którą ta funkcja przyjmuje, jest zestaw kolumn. Można je wskazać na różne sposoby, np. wypisać ich nazwy albo numery, ale możemy też użyć jednej z cudownych funkcji pomocniczych z zestawu `tidy-select`. Tutaj akurat użyłem `starts_with("H_")`, żeby powiedzieć, że ma to zrobić ze wszystkimi kolumnami, których nazwy zaczynają się od `H_`. Te same funkcje możemy wykorzystywać w funkcji `select`. Kilka innych tego typu funkcji umieściłem w tabeli.

| Funkcja         |                                           Wybierz wszystkie kolumny... |
|------------------|-----------------------------------------------------:|
| `starts_with()` |                                         których nazwy zaczynają się od |
| `ends_with()`   |                                            których nazwy kończą się na |
| `contains()`    |                                        których nazwy zawierają w sobie |
| `matches()`     | których nazwy zawierają w sobie<br>wyrażenie regularne[^podstawy_r-16] |
| `:`             |                           zawierają się w zakresie<br>(np. `H_1:H_10`) |
| `all_of()`      |                w których wszystkie wartości<br>spełniają jakiś warunek |
| `any_of()`      |                       w których jakakolwiek<br>wartość spełnia warunek |
| `everything()`  |                                              w ogóle wszystkie kolumny |
| `where()`       |        gdzie spełniony jest inny warunek<br>(np. `where(is.numeric))`) |

: Funkcje pomocnicze do `select` i `across`.

[^podstawy_r-16]: Wyrażenia regularne (*Regular expressions* w skrócie *RegEx*) to wielki temat, a liźnięcie go polecam każdemu, czy programuje, czy nie. Pozwalają one na wyszukiwanie regularnych wzorców w tekście, np. `"H_\d+"` oznacza „"H\_" a potem jedna lub więcej liczba". Można je wykorzystywać np. w LibreOffice czy Google Docs (Word zrobił własny system, bo przecież czemu wdrażać światowe standardy, jak można zrobić coś tylko dla tego jednego programu?). Wiele razy oszczędzały mi godzin mechanicznej pracy. Obfita instrukcja znajduje się w ściądze do `stringr`.

Drugim argumentem, jaki przyjmuje `across` jest nazwa funkcji, którą chcemy zastosować. Co ważne, musi to być jej nazwa **bez nawiasów**. Jest to częsty błąd i subtelna różnica, polegająca na tym, że jeśli nie używamy nawiasów, podajemy `across` samą funkcję, a jeśli damy nawiasy, to wrzucamy w ten sposób do `across` *wynik działania* tej funkcji. Spowodowałoby to, że w tym wypadku dostalibyśmy błąd, że funkcja `parse_number()` nie dostała wymaganych argumentów. Jeśli chcielibyśmy dorzucić do `parse_number` jakieś argumenty (jak `locale`[^podstawy_r-17]), możemy to zrobić po przecinku. Szczegóły, jak zwykle, znajdziemy w dokumentacji funkcji `across`.

[^podstawy_r-17]: Jej zachowanie możemy modyfikować, np. mówiąc jej, że w liczbach typu `128 242,78` spację ma traktować jako rozdzielacz dużych liczb, a przecinek jako operator dziesiętny (w krajach anglosaskich tę funkcję pełni zazwyczaj kropka). Inaczej `parse_number("128 242,78")` zwróci nam pierwszą napotkaną liczbę, czyli 128. Szczegóły można znaleźć w dokumentacji, ale w takim wypadku powinniśmy użyć `parse_number("128 242,78", locale = locale(grouping_mark = " ", decimal_mark = ","))`.

### Odwracanie punktacji {#sec-recode}

Bardzo często zdarza nam się, że w kwestionariuszach niektóre pozycje mają odwróconą punktację. Na przykład w kwestionariuszu samooceny Rosenberga SES pojawia się pozycja „Czasem czuję się bezużyteczny(-a)". Odpowiada się na skali 1 do 4. Wiadomo, że osoba, która zaznacza przy takiej pozycji 4, nie pokazuje swojej wysokiej samooceny. Jest to pozycja z odwróconą punktacją, czyli 4 należy liczyć jako 1, 3 jako 2 itd. Przekształcenie to można zrobić bardzo łatwo. Najpierw dodajemy skrajne wartości skali, np. dla SES $1 + 4 = 5$. Teraz od 5 odejmujemy odpowiedź osoby badanej i dzięki temu rzeczywiście 4 zamienia się w 1, 3 w 2 itd. Jak odwrócić punktację w R?

Ponieważ jest to modyfikacja kolumny, użyjemy funkcji `mutate`. Załóżmy, że `H_5` ma odwróconą punktację. Oceny były na skali od 1 do 6, więc wyniki osób badanych musimy odjąć od 7. W takiej sytuacji kod wyglądałby następująco:

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        H_5 = 7 - H_5
    )
```

Niektórzy lubią tworzyć nowe kolumny na odwróconą punktację, my jednak po prostu zastąpiliśmy oryginalną kolumnę `H_5`. Jeśli chcemy odwrócić wiele kolumn, możemy użyć `across`. Załóżmy, że `H_7` też ma odwróconą punktację. W takim wypadku nasz kod mógłby wyglądać tak:

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        H_5 = 7 - H_5
        H_7 = 7 - H_7
    )
```

Albo z użyciem `across`:

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x)
    )
```

Pierwszym argumentem jest zestaw kolumn, dlatego nazwy kolumn opakowałem w `c()`. Jest to podstawowa funkcja, która zbiera kilka rzeczy w jeden zestaw. `select` czy `filter` nie potrzebowały, żeby robić takie zestawy, ale wiele funkcji (zwłaszcza spoza `tidyverse`) tego wymaga. Co z drugim argumentem, czyli funkcją? Tutaj wchodzimy głębiej w programistyczne meandry i można, oczywiście, zostać przy wersji bez `across`. Odważnych zapraszam do świata funkcji anonimowych.

### Własne funkcje {#sec-customfunctions}

Drugi argument w `across` to funkcja, jakiej `across` ma użyć do przekształcenia kolumn. Niestety nie ma funkcji, która odejmowałaby od 7. Żeby sobie z tym poradzić, musimy albo taką funkcję wcześniej zadeklarować, albo użyć tzw. funkcji anonimowej (zwanej też *lambda*). Pierwsza opcja jest łatwa do zrozumienia, ale wymaga sporo pisania jak na coś, czego użyjemy tylko raz. Tworzenie własnych funkcji w R jest dość łatwe. Nasza funkcja mogłaby wyglądać tak:

```{r}
odejmij_od_7 <- function(wynik) {
    7 - wynik
}
```

Pierwsza rzecz to nazwa. Obrazowo nazwałem naszą funkcję `odejmij_od_7`. Dalej następuje słowo kluczowe `function` i w nawiasie argumenty naszej funkcji. Do naszej funkcji wrzucamy wynik osoby badanej, więc nasz argument nazwałem obrazowo `wynik`. Jeśli chcemy, dla czytelności, rozbić funkcję na kilka linijek otwieramy teraz nawiasy klamrowe i w nich opisujemy, co funkcja ma robić. Nic nie stoi na przeszkodzie, żeby opisać to wszystko w jednej linijce `function(wynik) 7 - wynik`. Po wykonaniu nasza funkcja rzeczywiście działa, co możemy sprawdzić używając jej w konsoli.

```{r}
odejmij_od_7(3)

odejmij_od_7(12)
```

Jeśli mamy kilka kwestionariuszy z odwróconą punktacją, każdy z inną skalą, możemy od razu zrobić bardziej ogólną funkcję do odwracania.

```{r}
odejmij_od <- function(wynik, od_czego) {
    od_czego - wynik
}

odejmij_od(3, 7)

odejmij_od(2, od_czego = 4)
```

Bardziej ogólna funkcja wymaga podania drugiego argumentu, tzn. od czego trzeba odjąć wynik. Jak widać, ta funkcja też działa i przyjmuje argumenty pozycyjne lub nazwane. Jednej albo drugiej funkcji po zadeklarowaniu możemy użyć w `across`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), odejmij_od, od_czego = 7)
    )
```

#### Funkcje anonimowe {#sec-lambda}

Jeśli funkcja jest prosta i używamy jej tylko raz, często nie chcemy zaśmiecać sobie kodu jej definicją. Wtedy z pomocą przychodzą nam funkcje anonimowe (zwane też funkcjami *lambda*). Anonimowe, bo nie mają swojej nazwy. Podstawowy sposób ich używania to zadeklarowanie ich od razu w miejscu użycia.

```{r}
#| eval: false
across(c(H_5, H_7), function(wynik) 7 - wynik)
```

Zamiast nazwy funkcji użyliśmy tutaj od razu jej definicji. Funkcja jest w pełni sprawna i różni się od `odejmij_od_7` tylko tym, że nie ma nazwy.

Inny, jeszcze bardziej zwięzły, sposób używania funkcji anonimowych dodaje pakiet `purrr` wchodzący w skład `tidyverse`. Polega on na użyciu znaczka `~` (*czyt.* tylda). Nasza funkcja `odejmij_od_7` ma w tej konwencji postać `~ 7 - .x`. Poprzez `.x` oznacza się to, co do funkcji wrzucamy, czyli to, co w `odejmij_od_7` oznaczyliśmy jako `wynik`. Takie coś możemy zapisać w miejscu funkcji w `across`, co zrobiłem na początku.

## Nowe kolumny {#sec-newcols}

Jak wspomniałem, funkcja `mutate` nie tylko pozwala na modyfikowanie istniejących kolumn, ale też na tworzenie nowych. Zazwyczaj robimy to w dwóch przypadkach -- gdy chcemy zagregować dane z wierszy, np. zsumować wyniki kwestionariusza albo gdy chcemy podzielić naszą bazę na kategorie, np. „młodzi", „w średnim wieku", „seniorzy". Omówmy to po kolei.

### Agregowanie danych z wierszy z `dplyr::pick` {#sec-pick}

Załóżmy, że homofobię będziemy liczyć poprzez dodanie `H_1` + `H_2` + `H_3` itd. Czasami będziemy chcieli robić sumy, czasami policzyć średnią. W kwestionariuszach zazwyczaj liczymy sumy, ale dla czasów reakcji często będziemy chcieli policzyć średnią. Jak więc dodać taką sumującą kolumnę w R? Mamy dwa sposoby. Pierwszy to wprost opisanie, co dodajemy, wewnątrz `mutate`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = H_1 + H_2 + H_3 + H_4 + H_5 + H_6 + H_7 + H_8 + H_9 + H_10
    )
```

Jak można się domyślić, istnieje sposób niewymagający tyle pisania, które w skomplikowanych bazach i długich kwestionariuszach naprawdę może być długotrwałe i uciążliwe. `tidyverse` ratuje nas tutaj funkcją `pick`[^podstawy_r-18], a standardowy R dokłada funkcję `rowSums` (i `rowMeans`). Wystarczy, że do funkcji `rowSums` wrzucimy, które kolumny chcemy zsumować, wskazując je właśnie za pomocą `pick` i funkcji pomocniczych.

[^podstawy_r-18]: W chwili kiedy to piszę, czyli luty 2023 r., `pick` jest świeżynką, wprowadzoną do `dplyr` w wersji 1.1.0 na koniec stycznia 2023 r. Jeśli Twój R jej nie znajduje, warto zaktualizować pakiety, albo guzikami w RStudio, albo funkcją `devtools::update_packages()`. Jeśli nie ma takiej możliwości, w miejsce `pick` można użyć `across`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10))
    )
```

### Kategoryzowanie przypadków i przekodowywanie z `dplyr::case_when` {#sec-casewhen}

Czasem zdarza się, że chcemy podzielić dane ilościowe (np. wiek, wzrost, szczęście mierzone kwestionariuszowo) na kategorie (młodzi vs nie-aż-tak-młodzi, wysocy vs niscy, szczęśliwi vs nieszczęśliwi). Zdarza się też, że osoba tworząca ankietę nie buła na tyle przewidująca, żeby w odpowiedziach na pozycję kwestionariuszową do „zdecydowanie się zgadzam" dodać 6, więc nie możemy po prostu użyć `parse_number`. I w jednym, i w drugim wypadku musimy stworzyć wartości na podstawie innych wartości, np. wzrost poniżej 160 cm zamienić na „niski" albo tekst „zdecydowanie się nie zgadzam" zamienić na 1. Do takich celów służy niezwykle przydatna funkcja `case_when` z pakietu `dplyr`. Załóżmy, że chcemy podzielić mężczyzn w naszej bazie na trzy kategorie wykształcenia -- podstawowe, ponadpodstawowe i wyższe. Oznacza to, że osoby z wykształceniem średnim i zawodowym musimy wrzucić do jednego worka. W tym celu rozszerzymy naszą instrukcję o kolejną komendę. Funkcja `case_when` ma dość prostą składnię.

```{r}
#| eval: false
case_when(
    warunek_1 ~ wartosc_jesli_prawda,
    warunek_2 ~ wartosc_jesli_prawda,
    warunek_3 ~ wartosc_jesli_prawda,
    .default = wartosc_dla_calej_reszty
)
```

Funkcja po kolei sprawdza warunki. Jeśli natrafi na jakiś spełniony warunek zatrzyma się i da taką wartość, jaką temu warunkowi przypisaliśmy. Warunek jest logiczny, czyli może to być cokolwiek od `wyksztalcenie == "Średnie"` po `wzrost <= 160`. Należy pamiętać, że jeśli wartość wynikowa ma być tekstem, musimy napisać ją w cudzysłowie, jak każdy dosłowny tekst. `.default = wartosc` może nam służyć do ustalania, co ma być, jeśli żaden z powyższych warunków się nie sprawdzi[^podstawy_r-19]. Jeśli chodzi o przykład z wykształceniem, moglibyśmy rozwiązać go tak:

[^podstawy_r-19]: W starszych tekstach (z mojej perspektywy starszych niż 3 tygodnie) można zamiast argumentu `.default` spotkać konstrukcję typu `TRUE ~ wyksztalcenie`. Aktualnie ona jeszcze działa, ale powoli będzie wycofywana. Lepiej jest przestawiać się na `.default`.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10)),
        wyksztalcenie_grupa = case_when(
            wyksztalcenie == "Średnie" ~ "Ponadpodstawowe",
            wyksztalcenie == "Zawodowe" ~ "Ponadpodstawowe",
            .default = wyksztalcenie
        )
    )
```

W tym wypadku `case_when`, idąc wiersz po wierszu, sprawdza, czy w kolumnie `wyksztalcenie` nie znajduje się wartość `"Średnie"`. Jeśli tak, to w tworzonej właśnie kolumnie `wyksztalcenie_grupa` wstawia wartość `"Ponadpodstawowe"` i przechodzi do kolejnego. Jeśli wykształcenie nie jest średnie, to sprawdza, czy jest zawodowe i w razie czego również wstawia `"Ponadpodstawowe"`. Jeśli nie jest ani średnie, ani zawodowe, to wstawia to, co akurat jest w kolumnie `wyksztalcenie`, czyli dla osób z wykształceniem podstawowym wstawia `"Podstawowe"`, a dla osób z wykształceniem wyższym `"Wyższe"`[^podstawy_r-20]. W ten sposób z 4 kategorii wykształcenia zrobiły nam się 3. W podobny sposób przekodowywalibyśmy klucz w ankiecie na liczby, np. pisząc `H_1 == "Zdecydowanie się zgadzam" ~ 6`.

[^podstawy_r-20]: Warto zauważyć, że zapisane tu warunki moglibyśmy uprościć do postaci `wyksztalcenie == "Średnie" | wyksztalcenie == "Podstawowe"` albo jeszcze bardziej `wyksztalcenie %in% c("Średnie", "Podstawowe")`. Warto też zerknąć w nową siostrę `case_when`, tj. `case_match`, która również mogłaby nam tutaj pomóc. Szczegóły, oczywiście, w dokumentacji.

## Sortowanie i kolejność kolumn {#sortowanie-i-kolejność-kolumn}

Wychodzimy już z potężnej funkcji `mutate` i możemy czyścić dalej. Ostatnia rzecz, którą czasem chcemy zrobić (zazwyczaj ze względów estetycznych), to posortowanie wartości i ustawienie kolumn w określonej kolejności.

### Sortowanie z `dplyr::arrange` {#sec-arrange}

Za sortowanie w `tidyverse` odpowiada funkcja `arrange`. Domyślnie sortuje ona rosnąco, więc jeśli chcemy sortowanie malejące, użyjemy pomocniczej funkcji `desc` (od *descending*). Załóżmy, że chcemy posortować nasze dane najpierw według wykształcenia (od najwyższego, do najniższego), a w obrębie wykształcenia według wieku (od najmłodszych do najstarszych). W pierwszym odruchu chcielibyśmy wpisać `arrange(desc(wyksztalcenie), wiek)`. Jest to dobry odruch, jednak jeśli to zrobimy zorientujemy się, że najwyższym z wykształceń jest wykształcenie zawodowe. Dzieje się tak dlatego, że w tej chwili wykształcenie to zwykły tekst, a więc jest sortowany alfabetycznie, nie według naszego klucza. Żeby to zmienić, musimy poznać nowy rodzaj danych.

#### Factors {#sec-factor}

Czynniki (*factors*) to rodzaj danych, za pomocą których przechowujemy tekst, który ma tylko kilka możliwych wartości albo te wartości mają jakąś kolejność, którą chcemy wziąć pod uwagę. Jeśli mamy etykiety takie jak wykształcenie, czy grupa kontrolna/eksperymentalna, to powinniśmy je przechowywać właśnie w tej postaci. Danymi tego typu w `tidyverse` zarządza pakiet `forcats`. Żeby zmienić wykształcenie z tekstu na factor, dopiszemy jedną linijkę do naszego `mutate` i od razu posortujemy.

```{r}
#| eval: false
df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10)),
        wyksztalcenie_grupa = case_when(
            wyksztalcenie == "Średnie" ~ "Ponadpodstawowe",
            wyksztalcenie == "Zawodowe" ~ "Ponadpodstawowe",
            .default = wyksztalcenie
        ),
        wyksztalcenie = factor(
            wyksztalcenie,
            levels = c("Podstawowe", "Zawodowe", "Średnie", "Wyższe"),
            ordered = TRUE
            )
    ) %>%
    arrange(desc(wyksztalcenie), wiek)
```

Komendę `factor` dla czytelności rozbiłem tutaj na trzy linijki, ale -- oczywiście -- można ją całą zapisać w jednej. Po pierwsze wskazałem, że na factor przerobiona ma być kolumna `wyksztalcenie`. Po drugie wskazałem, jakie wykształcenie może mieć wartości, zbierając je w jeden zestaw funkcją `c` i wrzucając do argumentu `levels`. Na koniec poinformowałem R, że w kolejność jest tutaj ważna, dopisując `ordered = TRUE`. Jeśli tak przerobione dane posortujemy, zobaczymy, że baza rzeczywiście zaczyna się od wykształcenia wyższego.

##### `Questionr` {#sec-questionr}

Przy okazji czynników chciałbym wspomnieć o pierwszym dodatku (*addins*) do RStudio, jaki może nam się przydać. Dodatki to przyjmują różną formę, ale tutaj omówię dwa, które są graficznymi narzędziami pomagającymi pisać nam kod. Można mieć do nich mieszany stosunek, ale póki umiemy też napisać kod ręcznie (lub chociaż wiemy, jak skorzystać z dokumentacji), to mogą być dużą pomocą. Zwłaszcza na początku przygody z R. Pierwszym takim dodatkiem jest `Questionr`, który pozwala nam stworzyć komendy związane z czynnikami (i kategoryzować dane ilościowe, co ręcznie zrobimy w podrozdziale [-@sec-casewhen]).

`Questionr` instalujemy jak każdy inny pakiet (`install.packages("questionr")`). Od tego momentu (lub po zresetowaniu RStudio) w menu `addins` na górnej belce znajdziemy trzy nowe opcje. Ta interesująca nas to Level ordering. Na początku zobaczymy okienko, w którym możemy wybrać kilka rzeczy. W jakiej zmiennej chcemy zmienić kolejność, w jakiej kolumnie i z jakiego pakietu wziąć funkcję do zmiany kolejności (domyślnie jest to `fct_relevel` z `forcats`). W drugiej zakładce możemy graficznie ustawić taką kolejność, jaką chcemy. W ostatniej zakładce otrzymujemy gotowy kod. `Questionr` nie robi nic samodzielnie, ten kod trzeba jeszcze wkleić w skrypt. Z jednym kruczkiem. Nam chodzi o samą komendę `fct_relevel`, bez pierwszej linijki, która służy do zapisania zmian. Ponieważ my tworzymy `mutate`, to wystarczy, że skopiujemy nasze gotowe `fct_relevel()` do funkcji `mutate`, dopisując nazwę kolumny i pierwszy argument. Może być to użyteczne, jeśli mamy dużo czynników lub dużo poziomów w czynniku. Ostatecznie nasza komenda `mutate` z użyciem `questionr` wyglądałaby tak:

```{r}
#| eval: false
mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10)),
        wyksztalcenie_grupa = case_when(
            wyksztalcenie == "Średnie" ~ "Ponadpodstawowe",
            wyksztalcenie == "Zawodowe" ~ "Ponadpodstawowe",
            .default = wyksztalcenie
        ),
        wyksztalcenie = fct_relevel(
            wyksztalcenie,
            "Podstawowe", "Zawodowe", "Średnie", "Wyższe"
        )
)
```

### Kolejność kolumn z `dplyr::relocate` {#sec-relocate}

Możemy chcieć mieć nasze kolumny w określonej kolejności. Są zasadniczo dwa sposoby zmieniania kolejności kolumn. Jest funkcja `relocate`, która służy raczej przestawianiu pojedynczych kolumn lub ich niewielkiej liczby. Jeśli chcemy od nowa określić kolejność kolumn, możemy wykorzystać w tym celu znaną nam już funkcję `select`. Załóżmy, że chcielibyśmy przestawić kolumnę `H_suma` przed kolumny z cząstkowymi wynikami.

```{r}
#| eval: false
# z użyciem relocate
df %>%
    relocate(H_suma, .before = H_1)

# z użyciem select
df %>%
    select(id, wiek, wyksztalcenie, H_suma, everything())
```

Jeśli przestawiamy kolumny z użyciem `relocate`, powinniśmy ustawić argument `.before` albo `.after`. Oba wymagają nazwy kolumny przed którą lub po której chcemy przenieść naszą kolumnę. Jeśli nie ustawimy żadnego, nasze kolumny zostaną przeniesione na początek tabeli. Jeśli używamy `select` musimy wpisać kolejność naszych kolumn ręcznie. Ciekawe może być użycie przeze mnie `everything()`. W tym kontekście znaczy ono „i cała reszta".

## Zapisywanie zmian z `<-` {#sec-save}

W ten sposób uzyskaliśmy cały kod czyszczący. Mamy ów kod zapisany w naszym skrypcie, jeśli go uruchomimy, to widzimy, że działa. Jednak jeśli w konsolę wpiszemy `df`, naszym oczom ciągle ukazuje się stara, brzydka baza. Jak więc zmienić nasze `df` na wyczyszczoną wersję? Tak jak zawsze przypisujemy wartości w R -- operatorem `<-`. Nasz kod na zmianę brudnej bazy w czystą ostatecznie przyjmie postać:

```{r}
df <- df %>%
    filter(Płeć == "Mężczyzna") %>%
    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%
    set_names("id", "wiek", "wyksztalcenie", paste("H", 1:10, sep = "_")) %>%
    mutate(
        wiek = parse_number(wiek),
        across(H_1:H_10, parse_number),
        across(c(H_5, H_7), ~ 7 - .x),
        H_suma = rowSums(pick(H_1:H_10)),
        wyksztalcenie_grupa = case_when(
            wyksztalcenie == "Średnie" ~ "Ponadpodstawowe",
            wyksztalcenie == "Zawodowe" ~ "Ponadpodstawowe",
            .default = wyksztalcenie
        ),
        wyksztalcenie = factor(
            wyksztalcenie,
            levels = c("Podstawowe", "Zawodowe", "Średnie", "Wyższe"),
            ordered = TRUE
            )
    ) %>%
    arrange(desc(wyksztalcenie), wiek) %>%
    relocate(H_suma, .before = H_1)
```

Kod ten możemy uruchomić dla dowolnej ilości danych, w dowolnym momencie. Jest wielokrotnego użytku i spokojnie możemy go wykorzystać, kiedy baza się rozrośnie. Nie musimy go wtedy pisać od nowa, wystarczy, że go uruchomimy. Co więcej, mogę wpaść jeszcze na jakiś pomysł i dopisać linijkę na samym początku, nie musząc całej reszty robić od nowa. Sprawmy sobie tę przyjemność i zerknijmy na naszą wyczyszczoną bazę.

```{r}
df
```

Po zapisaniu zmiennej `df`, tracimy naszą starą zmienną. Tym samym, jeśli uruchomilibyśmy nasz kod jeszcze raz, ale już na nowej zmiennej `df`, wyskoczy nam błąd. W końcu nowa zmienna nie ma tych samych kolumn, co stara zmienna. Co więcej, takiej operacji nie da się cofnąć. Jeśli chcemy dostać swoją starą, brudną bazę, musimy ponownie załadować ją z pliku. To prowadzi nas do ważnego wniosku co do pisania skryptów -- powinniśmy pisać je tak, żeby dało się z nich odtworzyć wszystko, co robiliśmy od samego początku[^podstawy_r-21]. Dzięki temu, jeśli chcemy się wycofać, zaznaczamy i wykonujemy cały kod przed interesującym nas momentem. Brak skrótu Ctrl+Z jest jedną z ważniejszych różnic w analizie w programach typu SPSS czy Statistica a w językach programowania typu R czy Python. Wbrew pozorom, idzie się przyzwyczaić. Ta sama właściwość pozwala na zachowanie przejrzystości w nauce -- pokaż mi swój kod, a będę wiedział bardzo dokładnie, jak prowadziłeś(-aś) swoją analizę.

[^podstawy_r-21]: Dlatego ja zazwyczaj wstępną bazę ładuję do zmiennej typu `df_raw`, i ją przekształcam, zaś do zmiennej `df` zapisuję już wyczyszczoną bazę. Swój kod czyszczący zaczynam więc od `df <- df_raw %>%`. Dzięki temu zawsze mogę na szybko zerknąć do niewyczyszczonej bazy, jakbym czegoś z niej potrzebował.

## Grupowanie (`dplyr::group_by`) i agregowanie (`dplyr::summarise`) {#sec-summarise}

Gdy mamy już bazę, zazwyczaj chcemy policzyć pewne statystyki dla grup badanych, np. dla osób różniących się wykształceniem, płcią czy jakąś manipulacją. Chcemy na przykład poznać średnią homofobię osób o różnym wykształceniu, sprawdzić liczność naszych podgrup czy policzyć inne zbiorcze statystyki. Możemy, oczywiście, odfiltrować najpierw osoby o wykształceniu podstawowym, policzyć dla nich, potem osoby o wykształceniu średnim itd. Są jednak prostsze sposoby, a obejmują one użycie `group_by` i `summarise`[^podstawy_r-22]. Te dwie funkcje zazwyczaj idą ze sobą w parze i zgrupowane dane od razu trafiają do `summarise`. Poniżej przykład.

[^podstawy_r-22]: `tidyverse` dopuszcza zarówno pisownię brytyjską `summarise` jak i amerykańską `summarize`. Podobnie w przypadku innych rozbieżności. Dokumentacja wskazuje jednak, że pisownia brytyjska jest preferowana. Też ją preferuję.

```{r}
df %>%
    group_by(wyksztalcenie) %>%
    summarise(
        n = n(),
        H_M = mean(H_suma),
        H_SD = sd(H_suma),
        H_Me = median(H_suma),
        V = H_SD / H_M
    )
```

Jak widzimy, dostaliśmy tabelkę z wykształceniem i wskazanymi statystykami. Funkcja `n` zliczyła nam przypadki osób z poszczególnym wykształceniem, `mean` policzyła średnią, `sd` odchylenie standardowe, a `median` medianę. `V` to tzw. współczynnik zmienności. Co to jest, nie jest teraz szczególnie ważne. Policzyłem to tutaj, żeby pokazać, że w obliczeniach możemy też wpisywać niestandardowe operacje (jak dzielenie) bez żadnych strasznych funkcji anonimowych, a także że możemy wziąć wartości z innych kolumn jako argumenty do naszych przekształceń. Tutaj `V` to odchylenie standardowe średniej homofobii (`H_SD`) podzielone przez samą średnią (`H_M`). Każdą kolumnę mogliśmy nazwać wedle życzenia. Grupować możemy na podstawie wielu zmiennych. Jak dowiemy się w podrozdziale o eksploracji danych, istnieją funkcje, które najpopularniejsze zastawy danych eksploracyjnych liczą za nas.

Tak robiliśmy to zawsze, jednak `dplyr` 1.1.0. wprowadził inny sposób grupowania. Jeśli nie chcemy zapisywać grup w naszej bazie danych na później (czyli w większości przypadków), nie musimy w ogóle używać funkcji `group_by`. Zamiast tego `summarise` dostał argument `.by`, za pomocą którego możemy wskazać grupy jednorazowo, tylko na potrzeby tego jednego podsumowania. Więcej na temat argumentu `.by` można znaleźć w dokumentacji. Poniżej przykład z innego zbioru danych, w którym pojawia się liczba krzyków w piosence w zależności od typu piosenki i jej autora [@field2012].

```{r}
# załadowanie danych z sieci
df_scream <- read_csv("https://github.com/profandyfield/discovr/blob/master/data-raw/csv_files/escape.csv?raw=true")

# podejrzenie, jak dane wyglądają
glimpse(df_scream)

# pogrupowanie i zliczenie średniej liczby krzyków
df_scream %>%
    summarise(
        M = mean(screams),
        .by = c(song_type, songwriter)
    )
```

Kolumny do grupowania podałem jako zestaw, czyli wewnątrz `c()`. Zgrupowane w ten sposób dane pokazują nam, że Andy pisze bardziej krzykliwe piosenki od Malcolma, ale różnica powiększa się, gdy chodzi o piosenki symfoniczne.

## Format długi i szeroki z `tidyr::pivot_*` {#sec-pivot}

Format długi i szeroki to coś, co rzadko pojawia się w tekstach wprowadzających i nie mam pojęcia czemu. To jest naprawdę ważne. Przełożenie danych z jednego formatu na drugi to często podstawowa operacja, jaką musimy wykonać, kiedy chcemy coś policzyć. Nie mam chyba ani jednego projektu, w którym bym tego nie robił. Do tego współczesne komendy, które to robią, są naprawdę proste. Tym bardziej zaskakujące jest, że np. w Excelu wykonać taką operację jest trudno, jeśli nie umie się korzystać z Power Query. Zacznijmy jednak od tego, co to jest format długi i szeroki.

Terminy te odnoszą się do sposobu, w jaki składujemy dane. Format szeroki jest tym, co odruchowo tworzymy, kiedy robimy czyste tabelki. Jeden wiersz to jedna obserwacja. Wszystkie dane o konkretnej osobie badanej znajdują się w tym jednym wierszu. Każda kolumna to jedna zebrana dana, np. odpowiedź na konkretne pytanie. W takim formacie znajduje się teraz nasza baza. Weźmy z niej kilka kolumn, po czym użyjmy `head`, żeby zobaczyć pierwszych pięć wierszy.

```{r}
df_wide <- df %>% # zapiszę to jako df_wide, na później
    select(id, H_1:H_5) %>%
    mutate(id = 1:nrow(df)) %>% # poprawiam id, żeby były kolejne liczby, zmiana kosmetyczna
    arrange(id) # sortuję wg id

df_wide %>% # zapisane dane trzeba jeszcze wyświetlić
    head(n = 5) # tylko 5 pierwszych wierszy
```

Są to typowe dane w formacie szerokim. Żeby jednak zrozumieć różnicę, między formatem długim, a szerokim, trzeba jeszcze zobaczyć dane długie. Stwórzmy więc takie.

```{r}
df_long <- df_wide %>%
    pivot_longer(H_1:H_5, names_to = "pytanie", values_to = "ocena")

df_long %>%
    head(n = 10)
```

Zacznę od skomentowania samych danych, a potem wyjaśnię funkcję. Dane w formacie długim mają oddzielną kolumnę na numer pytania i odpowiedź. Pięć kolumn z odpowiedziami na pytania zmieniliśmy w dwie. Powoduje to jednak, że każda osoba badana ma pięć wierszy -- w każdym odpowiedź na tylko jedno pytanie. Najpierw następuje 5 wierszy osoby z id 1, potem 5 wierszy osoby z id 2 itd. Widać więc dlaczego formaty te nazywają się szeroki i długi. Szeroki ma wiele kolumn, mało wierszy (1 na osobę), długi mało kolumn, wiele wierszy (1 na każde pytanie).

Po co nam taki format? Zawiera te same informacje, co format długi, a trudniej się to czyta. Po pierwsze umożliwia nam to policzenie niektórych rzeczy, których nie policzylibyśmy z formatu szerokiego. Dla przykładu, teraz mogę grupować dane według pytań, żeby sprawdzić, czy na każde pytanie badani odpowiadają podobnie. Jeśli moja skala jest dobra i każde pytanie rzeczywiście mierzy to samo, to odpowiedzi na wszystkie pytania powinny być podobne. Być może zrobiłem jakieś kontrowersyjne pytanie, na które wszyscy odpowiadają nisko, mimo że nie różnią się, w tym przykładzie, poziomem homofobii. Mogę więc, na oko, sprawdzić rzetelność pozycji testowych[^podstawy_r-23]. Formatu długiego wymagają też niektóre testy statystyczne.

[^podstawy_r-23]: Oczywiście psychometria dysponuje lepszymi wskaźnikami rzetelności testów niż proste średnie odpowiedzi. Fakt pozostaje faktem jednak, że grupować dane po pytaniach można tylko wtedy, gdy są w jednej kolumnie.

```{r}
df_long %>%
    summarise(
        M = mean(ocena),
        SD = sd(ocena),
        .by = c(pytanie)
    )
```

Nawet częściej niż do grupowania po pytaniach, wykorzystujemy format długi do wykresów. Jak się przekonamy dalej, w gramatyce graficznej (*the grammar of graphics*) do jednego obiektu na wykresie możemy przypisać tylko jedną kolumnę. Jeśli więc chcemy zrobić wykres słupkowy np. wyników *przed* i *po*, to do osi X przypiszemy kolumnę z etykietami, a do osi Y kolumnę z wynikami. Nie da się więc sensownie zrobić wykresu, jeśli wyniki *przed* i *po* mamy w osobnych kolumnach.

Omówmy więc funkcję, której użyłem do zmiany formatu. Kiedyś robiło się to skomplikowanymi funkcjami `melt` i `cast`, które często można znaleźć w innych językach programowania. Dziś w R mamy, na szczęście, intuicyjne funkcje `pivot_wider` i `pivot_longer`. Tej pierwszej używamy zmieniając format na szeroki, tą drugą zmieniamy format na długi. Na przykładzie powyżej można stwierdzić, że `pivot_longer` przyjmuje trzy argumenty. Pierwszy to zbiór kolumn, do jakich chcemy tę funkcję zastosować. Można tu skorzystać z funkcji pomocniczych typu `starts_with()` albo `everything()`. Kolejne dwa argumenty funkcji `pivot_longer` to `names_to` i `values_to`. Są to nazwy kolumn, do których mają trafić, jak nazwa wskazuje, nazwy i wartości kolumn. W naszym przykładzie etykiety `H_1`, `H_2` itd. trafiły do kolumny `pytanie`, zaś same odpowiedzi na te pytania do kolumny `ocena`.

```{r}
#| eval: false
df_long %>%
    pivot_wider(names_from = "pytanie", values_from = "ocena")
```

`pivot_wider` ma prostszą składnię, ponieważ nie trzeba w niej wskazywać zakresu kolumn do rozwinięcia, a jedynie w jakiej kolumnie znajdują się nazwy kolumn, a w jakiej jej wartości. Robimy to odpowiednio argumentami `names_from` i `values_from`. Jeśli jakiejś wartości nie ma w formacie długim (np. gdy osoba z numerem 4 nie odpowiedziała na pytanie 2, to w formacie długim może nie być wiersza `4 H_2`), to `pivot_wider` automatycznie wstawi w tę komórkę `NA`[^podstawy_r-24]. Zdarza się, że funkcji tej musimy użyć dlatego, że niektóre programy generują dane w formacie długim.

[^podstawy_r-24]: To zachowanie można zmodyfikować argumentem `values_fill`. Szczegóły można znaleźć w dokumentacji. Swoją drogą to jest piękne w R, jak bardzo jest on giętki. Funkcje mają domyślne zachowania, ale jeśli użytkownik w swojej szczególnej sytuacji potrzebuje czegoś innego, to zawsze może to ustawić. Ta uniwersalność jest sama w sobie dobrym powodem, żeby uczyć się programowania.

## Retesty czyli złączenia (*joins*) {#sec-join}

Złączenia (*joins*) to, jak nazwa wskazuje, metoda łączenia dwóch baz danych. Jest to jedna z podstawowych operacji na bazach danych, znana co najmniej od lat 70. i instrukcji `JOIN` w SQL. Jest to także jedna z operacji niedostępnych w Excelu bez Power Query. W praktyce badawczej może być ona konieczna, gdy mamy badanie wieloczęściowe, w którym musimy stosować wiele baz danych (np. jedną tworzą pomiary z eyetrackera, drugą wyniki w ankiecie, a trzecią test szybkości reakcji). Zdarza się to również często w prostych badaniach ankietowych, kiedy po jakimś czasie musimy wykonać retest. W obu tych przypadkach lądujemy z dwiema (lub więcej) bazami, które -- miejmy nadzieję -- mają jaką wspólną kolumnę, identyfikator osoby badanej, taki sam w każdej z trzech baz[^podstawy_r-25].

[^podstawy_r-25]: Czasami możemy też wykorzystać inną informację o osobie badanej jako identyfikator. Brałem udział w badaniach, w których osoba badana wypełniała ankietę i w tym samym czasie była nagrywana. Potem łączyłem informacje z nagrania z informacjami z ankiety na podstawie czasu wypełniania ankiety i czasu nagrywania. Jak widać, wszystko jest możliwe, choć domyślne złączenia z `dplyr`, w chwili pisania tego tekstu, nie pozwalają na pełną giętkość. Wtedy użyłem złączeń z pakietu `fuzzyjoin`.

Jak więc takie bazy połączyć? Wykorzystajmy tutaj dwie bazy zawierające test i retest, zrobione podczas walidacji kwestionariusza o nazwie KTR. Składał się on z dwóch skal oznaczonych tutaj literkami O i W. Standardową procedurą przy projektowaniu kwestionariusza jest powtórzenie pomiaru po jakimś czasie, żeby sprawdzić, na ile wyniki są stabilne. My taką procedurę wykonaliśmy, przez co dysponujemy dwoma oddzielnymi bazami. Zerknijmy na nie.

```{r}
db_test <- read_csv("./dane/podstawy-R/join-test.csv", show_col_types = FALSE)
db_retest <- read_csv("./dane/podstawy-R/join-retest.csv", show_col_types = FALSE)

db_test

db_retest
```

Pierwsza rzecz, która może zwrócić naszą uwagę, to znacznie mniejsza liczba osób badanych przy reteście. Jest to naturalne, jako że wiele osób, mimo wcześniejszych deklaracji nie wypełnia naszego testu po raz drugi. Widzimy też, że każdy wiersz posiada jakiegoś rodzaju kolumnę z unikatowym identyfikatorem osoby badanej. W języku relacyjnych baz danych takie unikatowe kolumny określa się jako `PRIMARY KEY`. W bazie danych z pierwszego testu kolumna ta nosi nazwę `ID`, a w bazie danych z retestu nazywa się ona `Subject`. Od razu wychodzi na jaw, że identyfikatory są spreparowane, bo nikt się nie pomylił, nie robił dopisków ani nie zdecydował się z jakiegoś powodu NaGlE pIsAć TaK.

Żeby połączyć te bazy, musimy najpierw zdecydować, jak chcemy to zrobić. Możemy albo przyłączyć wyniki z retestu do bazy z testem, albo przyłączyć wyniki z testu do bazy z retestem. Jest to o tyle istotne, że jeśli przyłączymy retest do testu, to będziemy mieli puste wartości u tych osób, które nie wypełniły retestu. Jeśli zrobimy odwrotnie, to z założenia każda osoba, która wypełniła retest, wcześniej wypełniła test, a więc figuruje w pierwotnej bazie. W praktyce bywa różnie. Na przykład ludzie kłamią, że wypełnili test, a jak dostaną link do retestu, to myślą, że w takim razie chociaż to wypełnią. Tak czy inaczej, ta decyzja determinuje typ złączenia, jaki wybierzemy. Najbardziej powszechnym typem jest `LEFT JOIN`, który do każdego wiersza jednej bazy (pisanej jako pierwszej, czyli po lewej) przypisuje pasujący wiersz drugiej bazy (pisanej jako drugiej, czyli po prawej). Jeśli jakiś wiersz w lewej bazie nie ma odpowiednika w prawej bazie, otrzymujemy puste wartości. Jeśli jakiś wiersz w bazie po prawej nie został przypisany do żadnego wiersza po lewej, nie jesteśmy o tym informowani. Więcej o różnych typach złączeń (np. pozwalających uzyskać wszystkie możliwe kombinacje wierszy) można przeczytać i zobaczyć na obrazkach [na przykład tutaj](https://dataschool.com/how-to-teach-people-sql/sql-join-types-explained-visually/).

Ja przyłączę retest do bazy z wynikami pierwszego testu. Widzę jednak dwa problemy, które będę musiał rozwiązać. Po pierwsze, kolumna z identyfikatorem osoby badanej nazywa się inaczej w obu bazach. Po drugie, kolumny `KTR_O` i `KTR_W` nazywają się tak samo w obu bazach. Będę więc musiał wskazać R, na podstawie jakich kolumn ma dokonać złączenia, a także jak ma nazwać kolumny w gotowej bazie, żebym wiedział, które wyniki dotyczą pierwszego testu, a które retestu.

```{r}
db_joined <- db_test %>%
    left_join(
        db_retest,
        by = join_by(ID == Subject),
        suffix = c("", "_retest")
    )

db_joined
```

Pierwszy problem rozwiązałem za pomocą argumentu `by`. Od wersji `dplyr` 1.1.0 przyjmuje on inną funkcję o nazwie `join_by`. W jej nawiasach precyzujemy, na podstawie jakich kolumn należy dokonać złączenia. Identyczne kolumny łączymy znakiem `==`. Drugi problem rozwiązałem dodając w argumencie `suffix` przyrostki do nazw kolumn. Zawsze zapisuje się je jako zestaw, czyli wewnątrz `c()` i zawsze najpierw jest w cudzysłowie przyrostek lewej bazy (u nas `db_test`), a potem przyrostek prawej bazy (u nas `db_retest`). Ja chciałem, by kolumny pierwotnej bazy nie miały przyrostka, więc za przyrostek dałem pusty ciąg znaków (czyli po prostu nic w cudzysłowie), zaś do kolumn bazy z retestem dodałem przyrostek `"_retest"`. Efekt widać na obrazku -- 5 kolumn i puste wartości u osób, które nie wypełniły retestu.

Jak widać złączenia to zaskakująco szeroki temat, który daje duże możliwości. Omówiona tu funkcja `left_join` jest najczęściej stosowana, ale warto zerknąć do dokumentacji i w tutoriale, żeby chociaż dowiedzieć się, co możemy za pomocą złączeń zrobić.

# Eksploracja danych {#sec-exploration}

Dane możemy eksplorować ręcznie za pomocą funkcji `group_by` i `summarise`, jak to opisałem w podrozdziale ([-@sec-summarise]). Ponieważ jednak wiele czynności, takich jak liczenie statystyk opisowych, wykonujemy za każdym razem, kiedy siadamy do analizy, powstały funkcje, które znacznie ten proces przyspieszają i upraszczają. Te, które omówię tutaj, stanowią część świetnego pakietu `rstatix`, który omawiam także w podrozdziale [-@sec-rstatix].

<!-- O ile ogólna możliwość liczenia statystyk została omówiona w podrozdziale o grupowaniu i agregacji ([-@sec-summarise]), tak tutaj chciałbym podrzucić dwie użyteczne funkcje do szybkiego uzyskania statystyk dla całego zestawu danych. Będą to funkcje `describe` i `describeBy` z pakietu `psych` (który trzeba, oczywiście, zainstalować i załadować przed użyciem). Jest to pierwszy omawiany tu pakiet spoza `tidyverse`. `psych` to rozległy zestaw narzędzi zaprojektowanych specjalnie do obliczeń szeroko wykorzystywanych w psychologii. Składa się z kilkudziesięciu funkcji zaprojektowanych jako zestaw narzędzi do badań empirycznych w psychologii. Jest tylko jeden problem. `psych` miał premierę w 2007 r. i o ile co jakiś czas dostaje update, to nie jest opublikowany na GitHubie. Oznacza to, że społeczność nie może wprowadzać poprawek do kodu. Profesor Revelle rzeczywiście zna się na rzeczy, jako że od 40 lat jest profesorem, ale to ciągle jedna osoba. Dlatego czasem trzeba się trochę nagimnastykować, żeby zmusić `psych` do współpracy z `tidyverse`. -->

<!-- ## Statystyki opisowe z `psych::describe` {#sec-describe} -->

<!-- Funkcja `describe` pozwala szybko uzyskać zestaw wielu podstawowych statystyk opisowych dla wszystkich kolumn naraz. Powstaje z tego duża tabela, która zawiera znacznie więcej danych niż nam potrzeba, ale ma tę zaletę, że można ją zrobić szybko. Składnia jest tutaj tak banalna, że ciężko mówić o składni, bo polega na tym, że piszemy `describe(df)`. I to zazwyczaj zupełnie wystarczy. Że to za proste, to pokażę trochę więcej możliwości. -->

<!-- ```{r} -->

<!-- #| warning: false -->

<!-- library("psych") # normalnie wszystkie pakiety ładowałbym w jednym miejscu, gdzieś na początku kodu -->

<!-- descr <- df %>% -->

<!--     describe(quant = c(0.25, 0.75), IQR = TRUE) %>% -->

<!--     as_tibble(rownames = "var") -->

<!-- ``` -->

<!-- Źródłem problemów czasem bywa to, że mamy dwa główne sposoby przechowywania tabelek w R. Pierwszy to `data.frame` czyli natywny typ danych w R. Drugi to `tibble`, czyli `data.frame` na prochach. Jak nietrudno się domyślić, `tibble` pochodzi z ze świata `tidyverse` i właściwie wszystko robi lepiej. Przede wszystkim lepsze jest to, jak wyświetla się w konsoli, ale też kilka rzeczy pod maską. Wszystkie dotychczasowe „ramki danych" (jak to się czasem tłumaczy), z jakimi mieliśmy do czynienia, to właśnie `tibble`. Żeby na własnej skórze przekonać się o wyższości `tibble` nad `data.frame` można wpisać w konsolę `df` (która jest w formacie `tibble`), a potem `as.data.frame(df)`. Dlatego też w ostatniej linijce kodu przerabiam wynik działania `describe` na format `tibble`. -->

<!-- Jedną rzecz muszę jednak dodać. Z wielu praktycznych przyczyn `tibble` nie używa nazwanych wierszy. Tylko kolumny mogą mieć nazwy własne, jeśli chcemy mieć identyfikatory wierszy, powinniśmy zrobić z nich oddzielną kolumnę. `describe` zaś w czystej postaci wyrzuca `data.frame`, w której nazwy opisanych zmiennych to właśnie nazwy wierszy, dlatego muszę powiedzieć funkcji `as_tibble`, do jakiej kolumny ma mi wrzucić nazwy tych zmiennych. Swoją nazwałem `var`. -->

<!-- Powstała tabela jest na tyle rozległa, że nie chciałem jej tu umieszczać, ale można ją sobie samodzielnie wyświetlić. Mam nadzieję, że wiesz już, jak to zrobić. Żeby podejrzeć zawartość naszej tabeli skorzystam z raz już użytej funkcji `glimpse`. -->

<!-- ```{r} -->

<!-- glimpse(descr) -->

<!-- ``` -->

<!-- Nazwy kolumn mówią nam już, co udało nam się wygenerować. Jest nasza kolumna `var` z nazwami zmiennych z bazy. Jest cała seria statystyk opisowych. Warto zauważyć, że swoje statystyki typu średnia dostała też zmienna tekstowa `wyksztalcenie`. Żeby ostrzec, że liczby te nie mają za wiele sensu, nazwa tej kolumny automatycznie dostała gwiazdkę `wyksztalcenie*`. Opis wszystkich policzonych statystyk znajduje się, a jakże, w dokumentacji. Powiem tylko o dwóch rzeczach, które wprost musiałem wywołać argumentami. Pierwsza to kwantyle, czyli wartości dzielące zbiór danych na części. 25. kwantyl to obserwacja, poniżej której znajduje się 25% danych. Jeśli mielibyśmy dokładnie 100 obserwacji, to 25. kwantylem byłaby 25. obserwacja. Mediana to nic innego jak 50. kwantyl. Możemy poprosić `describe` o podanie nam dowolnych kwantyli. Wystarczy, że zbierzemy je wszystkie w `c` jako ułamki dziesiętne i wrzucimy do argumentu `quant`. Druga rzecz, o którą wprost poprosiłem, to IQR, czyli rozstęp międzykwartylowy. Bywa wykorzystywany jako wskaźnik rozproszenia danych. Ponieważ jest to argument typu „licz albo nie licz", to wpisałem w niego `TRUE`, czyli kazałem `describe` ten rozstęp policzyć. -->

<!-- ## Statystyki opisowe dla grup z `psych::describeBy` {#sec-describeBy} -->

<!-- Co jeśli chcielibyśmy policzyć statystyki opisowe w grupach? Niestety `psych` nie współpracuje z `group_by` (patrz [-@sec-summarise]), ale ma własną metodę grupowania danych. Służy temu funkcja `describeBy`. Przypomnijmy sobie tutaj bazę `df_scream` z podrozdziału [-@sec-summarise]. -->

<!-- ```{r} -->

<!-- head(df_scream) -->

<!-- ``` -->

<!-- Mamy dwa sposoby, żeby naszą bazę wrzucić do `describeBy`, z czego ja pokażę ten bardziej intuicyjny. Ma od postać tzw. formuły (*formula*), z którymi spotkamy się jeszcze przy okazji testów statystycznych ([-@sec-tests]), zwłaszcza w `rstatix` ([-@sec-rstatix]). Przy tej okazji omówimy sobie też głębiej działanie potoków. Najpierw jednak pokażę przykład formuły. Jeśli chcielibyśmy dostać statystyki opisowe dla liczby krzyków w zależności od tego, kto tę piosenkę napisał, skorzystalibyśmy z takiej formuły `screams ~ songwriter`. Ten znaczek pośrodku to tylda, używana też w funkcjach anonimowych z `purrr` ([-@sec-lambda]). Jeśli chcielibyśmy dołożyć też do tego podziału typ piosenki, użylibyśmy znaku `+` pisząc `screams ~ songwriter + song_type`. Jak mogłaby wyglądać taka funkcja? -->

<!-- ```{r} -->

<!-- df_scream %>% -->

<!--     as.data.frame() %>% -->

<!--     describeBy(screams ~ songwriter + song_type, data = .) -->

<!-- ``` -->

<!-- Wynikiem działania takiej funkcji jest lista (też nieużywany wcześniej typ danych). W tytule możemy zobaczyć autora i typ piosenki. Jeśli to jest jasne, wyjaśnijmy pozostałą część instrukcji. Po pierwsze znów spotykamy problem, że `describeBy` nie chce współpracować z `tibble`, więc musimy przekształcić naszą bazę na stary dobry format `data.frame`. Robimy to funkcją `as.data.frame`. Druga sprawa to `data = .`. Argument `data` to obowiązkowy argument, do którego wrzucamy naszą bazę danych. Dzięki temu R wie, gdzie ma szukać kolumn `screams`, `songwriter` i `song_type` z naszej formuły. Problem polega jednak na użyciu potoków. Standardowo potok `%>%` wrzuca rzeczy do pierwszego argumentu. Wiele funkcji, tak jak `describeBy`, nie przyjmuje bazy danych w pierwszym argumencie. Jak na złość `data` w `describeBy` to ostatni argument, więc najlepiej wprost go nazwać. Omawiany w podrozdziale [-@sec-rstatix] pakiet `rstatix` za pierwszy cel postawił sobie nawet przerobienie testów statystycznych tak, żeby lepiej współpracowały z potokami. Jak więc poradzić sobie w takiej sytuacji? Jedna opcja, to całkiem zrezygnować z potoków. -->

<!-- ```{r} -->

<!-- #| eval: false -->

<!-- describeBy(screams ~ songwriter + song_type, data = as.data.frame(df_scream)) -->

<!-- ``` -->

<!-- W krótkich instrukcjach takie coś jest nawet czytelne. Warto pamiętać, że potoki nie są obowiązkowe, można równie dobrze zagnieżdżać funkcje. Cierpi na tym tylko czytelność kodu. Druga opcja, to wprost powiedzenie potokowi, gdzie ma wrzucić naszą bazę danych. Do tego właśnie służy `.`. Zapis `data = .` mówi potokowi „halo halo, proszę nie wrzucać bazy do pierwszego argumentu, tylko o tu, gdzie pokazałem(-am) kropką". Ostatecznie oba przykłady z tego podrozdziału są równoważne. -->

## Statystyki opisowe z `rstatix::get_summary_stats` {#statystyki-opisowe-z-rstatixget_summary_stats}

<!-- Muszę się do czegoś przyznać. Przez lata wykorzystywałem `describe` i `describeBy` do statystyk opisowych, ale... są lepsze alternatywy. Chciałem jednak wykorzystać te funkcje jako praktyczny pretekst to pokazania różnic między `data.frame` i `tibble`, używania kropki z potokami i ogólnie jak sobie radzić, jak nowszych alternatyw nie ma. Jednak do liczenia statystyk opisowych takie alternatywy są. I przychodzą do nas z `rstatix`. Jest to świetny, nowoczesny pakiet, który szerzej omówimy w podrozdziale [-@sec-rstatix]. -->

Do liczenia statystyk opisowych w `rstatix` wykorzystujemy funkcję `get_summary_stats`. Dobrze współpracuje ona z `tidyverse`, potokami i funkcją `group_by`. Ma też bardzo prostą składnię. Jedyne, co musimy zrobić, to wrzucić do naszej funkcji naszą bazę danych, a otrzymamy rozległą tabelę ze statystykami opisowymi dla wszystkich zmiennych. Szczegóły, wyjaśnienia i ewentualne możliwości modyfikacji poszczególnych statystyk znajdują się w dokumentacji. Jeśli do podstawowych statystych chcielibyśmy doliczyć coś niestandardowego, zawsze możemy użyć `mutate`.

```{r}
#| warning: false
library("rstatix")

get_summary_stats(df)
```

Jeśli chcemy otrzymać statysyki opisowe dla zgrupowanych danych, czyli np. osobno dla każdego poziomu wykształcenia, wystarczy, że przed użyciem funkcji `get_summary_stats` zgrupujemy dane funkcją `group_by`. W chwili, kiedy to piszę, funkcje `rstatix` nie posiadają argumentu `.by`. Jako przykład wykorzystam bazę `df_scream` z podrozdziału [-@sec-summarise].

```{r}
df_scream %>%
    group_by(songwriter, song_type) %>%
    get_summary_stats()
```

## Tabele liczności z `rstatix::freq_table` {#tabele-liczności-z-rstatixfreq_table}

Dla danych kategorialnych (np. płeć, wykształcenie, klasa) nie liczymy statystyk opisowych, a tabele liczności. Chcemy na przykład wiedzieć, ile w naszej bazie mamy osób z wykształceniem wyższym, ile ze średnim itd. W podrozdziale [-@sec-summarise] pokazałem, jak to zrobić ręcznie funkcjami `summarise` i `n`. Pakiet `rstatix` ma dla nas gotową funkcję `freq_table` właśnie do tego celu. Wymaga ona jedynie podania, które kolumny z naszej tabeli ująć. Robimy to tak samo, jak zrobilibyśmy w funkcji `select` (patrz [-@sec-select]) czy `across` (patrz [-@sec-across]).

```{r}
freq_table(df, wyksztalcenie)
```

## Macierze korelacji i ich istotność z `rstatix::cor_mat` {#macierze-korelacji-i-ich-istotność-z-rstatixcor_mat}

W ramach eksploracji chcemy czasami zrobić macierz korelacji całego naszego zestawu danych. Może nam do tego posłużyć funkcja `cor_mat` z pakietu `rstatix`. Zobaczmy to na przykładzie bazy `db_joined` z podrozdziału [-@sec-join].

```{r}
(korelacje <- cor_mat(db_joined, -ID))
```

Użycie, jak widać, jest bardzo proste. Jedyna dodatkowa informacja, jaką sprecyzowałem, to żeby nie brać pod uwagę kolumny ID. W tym wypadku mógłbym też użyć `starts_with("KTR")`. Efektem działania funkcji jest macierz korelacji. Możemy z niej wyczytać m.in., że korelacja `KTR_O` i `KTR_W` z ich retestami wyniosła odpowiednio `r jedrusiakr::apa_num_pl(cor_mat(db_joined, -ID)[[3,1]], add_equals = FALSE)` i `r jedrusiakr::apa_num_pl(cor_mat(db_joined, -ID)[[4,2]], add_equals = FALSE)`. Nie są to jakoś oszałamiające wyniki jak na testy, które mają mierzyć względnie stałe cechy.

Można zwrócić uwagę na to, że powyższa komenda wyświetliła nam macierz korelacji, pomimo że normalnie musiałbym jeszcze wywołać samą zmienną `korelacje`, do której ją zapisałem. Tak to robiliśmy wcześniej. Wykorzystałem tutaj wygodą sztuczkę -- jeśli weźmie się całe przypisanie w nawiasy, R potraktuje to jako „przypisz i wyświetl".

Korelacje mają jednak swoją istotność, którą możemy tu oznaczyć. Jeśli policzyliśmy już macierz korelacji, możemy ją wrzucić do funkcji `cor_get_pval`. Ewentualnie możemy samą bazę wrzucić do funkcji `cor_pmat`. Efekt jest ostatecznie ten sam.

```{r}
#| label: cor_pmat
#| eval: false

korelacje_p_1 <- cor_get_pval(korelacje)

korelacje_p_2 <- cor_pmat(db_joined, -ID)

identical(korelacje_p_1, korelacje_p_2)

korelacje_p_1
```

Funkcja `identical` informuje nas, że obiekty stworzone obiema funkcjami rzeczywiście są identyczne. Powstała nam macierz istotności korelacji. Może być mylące, że korelacje wyświetlają się w notacji naukowej, co jest wygodnym sposobem oznaczania bardzo małych lub bardzo dużych liczb. Zasada jest tu prosta: $1,22e-4 = 1,22 \times 10^{-4} = 0,000122$. Jeśli jednak chcemy dostać tę macierz w przyjaźniejszej formie, możemy użyć funkcji `cor_mark_significant`[^podstawy_r-26], do której wrzucamy macierz korelacji (nie macierz istotności).

[^podstawy_r-26]: Ewentualnie można zaokrąglić kolumny z korelacjami do 3 lub 4 miejsc po przecinku z użyciem funkcji `mutate` i `round` albo też wyłączając notację naukową komendą `format`; można napisać na przykład `korelacje_p_1 %>% mutate(across(where(is.numeric), ~ format(.x, scientific = FALSE)))`. Tylko to naprawdę dziwnie wygląda.

```{r}
cor_mark_significant(korelacje)
```

Otrzymujemy naszą macierz korelacji wzbogaconą o gwiazdki. Domyślnie mamy standardowy układ[^podstawy_r-27] z dodatkiem \*\*\*\* oznaczającym mniej niż 0,0001. Gwiazdki możemy dostosowywać, a szczegóły znajdują się w dokumentacji.

[^podstawy_r-27]: $* < 0,05$; $** < 0,01$; $*** < 0,001$.

# Wykresy z `ggplot2` {#sec-ggplot}

Wykresy to wielki temat i o samym pakiecie, za pomocą którego będziemy je wykonywać, `ggplot2`, powstawały osobne książki. Naturalnie więc ja pokażę tylko tyle, ile jest konieczne, żeby zacząć. Zwłaszcza, że nie lubię robić wykresów. Na matematyce najbardziej lubiłem algebrę, funkcje, ciągi, rachunek pochodnych, ale planimetria, a tym bardziej stereometria, to mój słaby punkt. Tym bardziej dziwi mnie zawsze jak spotykam ludzi, którzy lubią to robić. Zwłaszcza pozdrawiam Wiktorię J., która mówiła mi kiedyś, że szczególnie lubi się babrać z `ggplot2`. Szacuneczek.

Nie zmienia to jednak faktu, że wykresy to przepotężne narzędzia poznawcze. Dane w postaci liczb w tabelach za wiele nam nie mówią, aż nie naniesiemy ich na rysunek. Na rysunkach widzimy trendy, rozkłady, outliery i mnóstwo innych rzeczy, które dopiero wykres ujawni. Wykresy pojawiają się w prawie każdej pracy naukowej, więc zdolność efektywnego ich wytwarzania i dostosowywania do wymogów (np. APA) jest podstawową umiejętnością analityka. Na szczęście nie musimy ruszać się z R.

Fundamentem teoretycznym, na którym zbudowany jest `ggplot2` jest idea *The Grammar of Graphics*, którą Leland Wilkinson [-@wilkinson_grammar_2005] opisał na 700 stronach (sic!) swojej książki o tym tytule. Tak jak mówiłem, wykresy to duży temat. Nam wystarczą jednak podstawowe idee, a absolutnym fundamentem jest stwierdzenie, że właściwie każdy wykres składa się z trzech elementów, które możemy wyobrazić sobie jako warstwy -- danych, jakiegoś układu współrzędnych i jakiegoś obiektu, który Wilkinson nazywa *geom*, czyli wizualna reprezentacja danych. Takimi obiektami są na przykład punkty na wykresie punktowym, słupki histogramu, linia trendu itd. Dodatkowo obiekt *geom* ma pewne właściwości wizualne, takie jak kolor, wielkość, czy przede wszystkim lokalizacja na osiach X i y. Nazywamy te właściwości estetykami, a wiązanie ich z danymi -- mapowaniem. Teraz może się to wydawać skomplikowane, ale wdrażanie tych idei w praktyce w `ggplot2` nie jest wcale trudne. Jak już raz się to załapie, to nagle takie narzędzia jak wykresy w Excelu zaczną się wydawać wybrakowane i chaotyczne.

## Mapowanie estetyk (`aes`) {#sec-aes}

Przejdźmy więc do praktyki. Pakietu `ggplot2` nie musimy ładować osobno, bo wchodzi w skład, a jakże, `tidyverse`. Żeby zacząć tworzyć wykres, musimy zacząć od wywołania funkcji `ggplot()` (ważne -- nie `ggplot2`, `ggplot2` to nazwa pakietu, funkcja to `ggplot`). W jej obrębie wskazujemy na zbiór danych, na którym chcemy pracować i dokonujemy *mapowania estetyk*, czyli mówimy naszej funkcji jakie kolumny mają przełożyć się na jakie elementy wizualne. Wykorzystajmy sobie tutaj jeden z klasycznych zbiorów danych o nazwie `diamonds`, który powinien stać się dostępny po załadowaniu `ggplot2`. Zerknijmy na niego.

```{r}
glimpse(diamonds)
```

`diamonds` to zbiór różnych danych na temat 53940 diamentów. Szczegółowe dane na jego temat można uzyskać wpisując w konsoli `?diamonds`. Ponieważ jest to gigantyczny zbiór, wybierzmy sobie losowo 100 diamentów za pomocą funkcji `slice_sample`. Użyję jeszcze funkcji `set.seed`, żeby wyniki losowania były za każdym razem takie same. Nie jest ona obowiązkowa, ale użycie jej sprawi że wykresy wyjdą identyczne jak moje.

```{r}
set.seed(123)

df_diamonds <- slice_sample(diamonds, n = 100)
```

Znacznie lepiej. Zrobimy sobie prosty wykres ceny diamentu od jego masy w karatach. Pierwsza rzecz, którą musimy wykonać, to mapowanie kolumn `carat` i `price` do osi X i y. Estetyki mapujemy wrzucając je do funkcji `aes` -- najpierw x, potem y.

```{r}
ggplot(df_diamonds, aes(carat, price))
```

Jak widzimy, powstał nam pusty wykres. To jest właśnie układ współrzędnych, o którym mówiłem wcześniej.

Poza estetykami X i Y mamy do dyspozycji mnóstwo innych estetyk, m.in. *colour*, *fill*, *alpha* (przeźroczystość), *size*, *linetype*, *shape*. Estetyki mają jedną wspólną cechę -- są powiązane z jakimiś danymi. Jeśli stwierdzę, że wszystkie moje punkty mają być czerwone, to nie będzie to estetyka, tylko *atrybut*. O estetyce będę mógł mówić wtedy, gdy kolor będzie zależał np. od przejrzystości diamentu. To rozróżnienie, że atrybuty to stałe właściwości wyglądu, a estetyki to związek wyglądu z danymi, jest o tyle ważne, że nieco inaczej się je definiuje, jak zobaczymy za chwilę.

## Obiekty `geom_*` {#sec-geom}

Pusty układ współrzędnych to jeszcze nie wykres. Musimy jeszcze dodać jakiegoś rodzaju *geom*. W naszym przypadku będą to punkty, gdzie każdy punkt będzie reprezentował inny brylant. Ściąga do `ggplot2` zawiera świetną rozpiskę, jakie można stworzyć wykresy, w zależności od typu zmiennych, jakimi dysponujemy.

Do tworzenia wykresów punktowych mamy dwa rodzaje obiektów *geom* -- `geom_point` i `geom_jitter`. `geom_point` to typowy wykres punktowy. `geom_jitter` przydaje się wtedy, kiedy mamy wiele danych o tych samych współrzędnych, np. wiele brylantów o masie dokładnie 0,2 karata i cenie dokładnie \$ 300. W takim wypadku wszystkie te punkty nałożyłyby się na siebie, ukryły jeden pod drugim i wydawałoby się, że mamy mniej danych, niż w rzeczywistości mamy. `geom_jitter` rozwiązuje ten problem odrobinkę przesuwając każdy punkt w losowym kierunku. Odrobinę tracimy wtedy na dokładności, ale widzimy wszystkie nasze dane. Żeby do naszego wykresu dołożyć kolejne elementy, używamy znaku `+`.

```{r}
ggplot(df_diamonds, aes(carat, price)) +
    geom_point()
```

Wygląda na to, że im większy diament, tym droższy. Bez zaskoczenia. Możemy do naszego wykresy dołożyć linię trendu jako kolejny *geom* -- `geom_smooth`. Jeśli chcemy mieć prostą linię, musimy ustawić argument `method = "lm"`, co jest skrótem od *linear model*.

```{r}
ggplot(df_diamonds, aes(carat, price)) +
    geom_point() +
    geom_smooth(method = "lm")
```

Szare pole wokół niebieskiej linii to przedział ufności. Możemy go wyłączyć ustawiając `se = FALSE`. Przy tej okazji powiedzmy sobie jeszcze raz o estetykach i atrybutach. Mogę chcieć, żeby kolor mojego punktu zależał od jakości wyszlifowania brylantu z kolumny `cut`. Ponieważ jest to związek wyglądu z danymi, to jest to estetyka i ustawiam ją wewnątrz `aes`. mogę to `aes` wrzucić albo do funkcji `ggplot`, jak robiłem wcześniej, albo też do funkcji `geom_point`. Zwyczajowo argumenty X i Y w `aes` są nienazwane, ale wszystkie inne już tak. Mogę też zmienić kolor linii trendu z niebieskiego na czarny i zrobić ją trochę cieńszą. Jest to zmiana wyglądu, ale arbitralna, bez związku z danymi. Jest to więc atrybut i ustawiam go poza `aes`, wewnątrz funkcji, której ten atrybut dotyczy. Zobaczmy to.

```{r}
ggplot(df_diamonds, aes(carat, price, colour = cut)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, linewidth = 0.5, colour = "black")
```

## Tytuły osi i wykresu (`labs`) {#sec-labs}

Kolejną rzeczą, którą moglibyśmy chcieć zmienić, są tytuły osi. Możemy też dodać tytuł do samego wykresu. Najwygodniej jest to zrobić dodając do wykresy kolejny element, `labs`, w którym dopiszemy nasze tytuły.

```{r}
ggplot(df_diamonds, aes(carat, price, colour = cut)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, linewidth = 0.5, colour = "black") +
    labs(
        x = "Masa [karaty]",
        y = "Cena [$]",
        colour = "Szlif",
        title = "Cena brylantu z zależności od masy"
    )

```

Jeśli chcemy zmienić etykiety wartości z legendy, mamy dwie opcje -- albo przekodujemy te etykiety bezpośrednio w bazie danych, chociażby zaprzęgając do pracy `questionr` (zob. [-@sec-questionr]), albo używając jednej z funkcji `scale_*_discrete` i jej argumentu `labels`, gdzie zamiast gwiazdki piszemy nazwę naszej estetyki. Szlif naszych diamentów jest zmapowany do estetyki `colour`, więc użyjemy funkcji `scale_colour_discrete`.

```{r}
ggplot(df_diamonds, aes(carat, price, colour = cut)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, linewidth = 0.5, colour = "black") +
    labs(
        x = "Masa [karaty]",
        y = "Cena [$]",
        colour = "Szlif",
        title = "Cena brylantu z zależności od masy"
    ) +
    scale_colour_discrete(labels = c("Zadowalający", "Dobry", "Bardzo dobry", "Premium", "Idealny"))
```

## Wygląd wykresów (`theme_*`) {#sec-theme}

Wykres ma wszystkie elementy na miejscu, ale nie oszukujmy się, nie jest to dzieło sztuki. Żeby poprawić wygląd naszego wykresu, sięgniemy po dwa narzędzia. Po pierwsze ustalimy jego ogólny styl za pomocą jednego z motywów (*theme*), a potem poprawimy szczegóły z użyciem dodatku do RStudio `esquisse`.

Ogólny styl wykresu ustala się za pomocą elementów zaczynających się słówkiem `theme_`. `ggplot2` ma wbudowane osiem takich motywów, które można przejrzeć [tutaj](https://ggplot2.tidyverse.org/reference/ggtheme.html). W Internecie roi się jednak od niestandardowych motywów, które mogą zaczarować nasze wykresy. Kluczowym ich źródłem może być pakiet `ggthemes`, z motywy z którego można zobaczyć [tutaj](https://github.com/jrnold/ggthemes). Ja jednak chciałem pokazać dwa inne -- `theme_apa` z pakietu `papaja` i `theme_Publication` z repozytorium na GitHubie [koundy/ggplot_theme_Publication](https://github.com/koundy/ggplot_theme_Publication).

`papaja` to skrótowiec od *Preparing APA Journal Articles* i jest to rozległy pakiet pomagający pisać artykuły zgodne ze standardami Amerykańskiego Towarzystwa Psychologicznego (APA). Z tych standardów korzystają nie tylko psychologowie, ale też wiele czasopism z zakresu nauk przyrodniczych. Daje nam on dostęp m.in. do motywu `theme_apa` dostosowującego wykres do standardów APA.

```{r}
#| warning: false
library("papaja")

ggplot(df_diamonds, aes(carat, price, colour = cut)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, linewidth = 0.5, colour = "black") +
    labs(
        x = "Masa [karaty]",
        y = "Cena [$]",
        colour = "Szlif",
        title = "Cena brylantu z zależności od masy"
    ) +
    scale_colour_discrete(
        labels = c("Zadowalający", "Dobry", "Bardzo dobry", "Premium", "Idealny")
    ) +
    theme_apa()
```

Drugi motyw nie jest szałowo popularny w społeczności, ale bardzo go lubię. Żeby zadziałał, musimy wcześniej zainstalować u siebie pakiety `grid`, `scales` i `ggthemes`. Spojrzenie w repozytorium pozwoli nam stwierdzić, że nie jest to pakiet, a po prostu zbiór plików. Wchodzimy więc w plik `ggplot_theme_Publication-2.R`, kopiujemy link i dodajemy na końcu `?raw=true`, co pozwoli R go odczytać. Następnie użyjemy komendy `source`, która pozwala nam uruchamiać z innych plików .R, jak podamy ich ścieżkę lub link. Cały kod mółgby więc wyglądać tak:

```{r}
source("https://github.com/koundy/ggplot_theme_Publication/blob/master/ggplot_theme_Publication-2.R?raw=true")

ggplot(df_diamonds, aes(carat, price, colour = cut)) +
    geom_point() +
    geom_smooth(method = "lm", se = FALSE, linewidth = 0.5, colour = "black") +
    labs(
        x = "Masa [karaty]",
        y = "Cena [$]",
        colour = "Szlif",
        title = "Cena brylantu z zależności od masy"
    ) +
    scale_colour_discrete(
        labels = c("Zadowalający", "Dobry", "Bardzo dobry", "Premium", "Idealny"),
    ) +
    theme_Publication()
```

## `esquisse` {#sec-esquisse}

Wykres w takiej formie można już uznać za zadowalający. Ale co jeśli chcemy poprawić jakieś szczegóły? Na przykład dostosować kolory? Są na to odpowiednie funkcje, ale jeśli mam być szczery, jest ich na tyle dużo, a w tutorialach tak bardzo przeplatają się stare i nowe metody, że bez gruntownego wyszkolenia (którego na przykład ja nie posiadam) bardzo łatwo jest się zgubić i bez zrozumienia kopiować kod znaleziony w Internecie. Na szczęście istnieje pewne narzędzie, które może nam w tej sytuacji pomóc. Nazywa się `esquisse` i jest dodatkiem do RStudio.

Instalujemy `esquisse` jak każdy inny pakiet (`install.packages("esquisse")`). Po zainstalowaniu, jeśli jest taka potrzeba, można spróbować zmienić język na polski komendą `set_i18n("pl")`[^podstawy_r-28]. Po zainstalowaniu pakietu, w menu Addins na górnej belce powinniśmy uzyskać dostęp do opcji *ggplot2 builder*[^podstawy_r-29], które jest narzędziem do interaktywnego konstruowania wykresów. Pozwala na stworzenie kodu w wygodnym, graficznym interfejsie. Po wygenerowaniu możemy skopiować gotowy kod do skryptu.

[^podstawy_r-28]: Piszę „spróbować", bo gdy to piszę polskie tłumaczenie znajduje się w wersji rozwojowej. Wersje stabilne pakietów znajdują się w CRAN i to one się pobierają, gdy piszemy komendę `install.packages()`. Jednak wiele pakietów ma też swoje wersje rozwojowe, które szybciej dostają nowe funkcje, ale bywają niestabilne. Można je pobrać poleceniem `install_github` z pakietu `devtools`, do którego wrzucamy nazwę repozytorium. Na przykład wersję rozwojową `esquisse` można zainstalować komendą `devtools::install_github("dreamRs/esquisse")`. Takie pakiety można potem aktualizować komendą `devtools::update_packages()`.

[^podstawy_r-29]: Narzędzie można wywołać też komendą `esquisse::esquisser()`.

```{r}
ggplot(df_diamonds) +
    aes(x = carat, y = price, colour = clarity) +
    geom_point(shape = "diamond", size = 2L) +
    scale_color_brewer(palette = "YlOrRd", direction = 1) +
    labs(
        x = "Masa [karaty]",
        y = "Cena [$]",
        title = "Cena diamentów od ich masy według szlifu",
        caption = "Źródło danych: ggplot2",
        color = "Przejrzystość"
    ) +
    theme_bw() +
    theme(
        legend.position = "bottom",
        plot.title = element_text(face = "bold")
    ) +
    facet_wrap(vars(cut))
```

Powyżej wykres, który stworzyłem w `equisse`. Warto jednak zwrócić uwagę na jego niedoskonałości, takie jak brak polskich tłumaczeń typów szlifu. Pewnym wyjaśnieniem może być dla nas nazwa francuskiego słowa *equisse* -- szkic. Takie narzędzia jak `equisse` nie zwalniają nas całkowicie z umiejętności kodowania wykresów, ale pozwalają wygodnie tworzyć `szkice` naszego kodu. Ten szkic mogę pozmieniać, np. zamieniając `theme_bw` na `theme_Publication` i dodając polskie tłumaczenia.

```{r}
ggplot(df_diamonds) +
    aes(x = carat, y = price, colour = clarity) +
    geom_point(shape = "diamond", size = 2L) +
    scale_color_brewer(palette = "YlOrRd", direction = 1) +
    labs(
        x = "Masa [karaty]",
        y = "Cena [$]",
        title = "Cena diamentów od ich masy według szlifu",
        caption = "Źródło danych: ggplot2",
        color = "Przejrzystość"
    ) +
    theme_Publication() +
    theme(
        legend.position = "bottom",
        plot.title = element_text(face = "bold")
    ) +
    facet_wrap(
        vars(cut),
        labeller = labeller(cut = c(
            "Fair" = "Zadowalający",
            "Good" = "Dobry",
            "Very Good" = "Bardzo dobry",
            "Premium" = "Premium",
            "Ideal" = "Idealny"
            )
        )
    )
```

Wykresy to olbrzymi temat, który tutaj tylko liznęliśmy z wierzchu. Sądzę jednak, że ta wiedza wystarczy, żeby -- z pomocą dokumentacji i Google -- być w stanie powoli rozbudowywać swoje umiejętności z zakresu `ggplot2`.

# Modele statystyczne {#sec-tests}

Największą trudnością nie jest tworzenie modeli statystycznych, tylko wiedza kiedy zrobić jaki test i jak je interpretować. Stąd też nie będę się tutaj rozwodził nad samymi testami, a omówię raczej ogólną specyfikę wykonywania testów statystycznych w R i ich wyświetlania. Szczegóły co do tego, jak wykonywać konkretne testy, znajdują się w dokumentacji funkcji, zaś informacje jakimi funkcjami robić jakie testy są w innych miejscach, z których kilka polecę.

Modele statystyczne w R można ogólnie podzielić na te, które wykonuje się wprowadzając formuły i na te, w których wskazuje się kolumny. Fundamentalnym przykładem pierwszego typu jest funkcja do wykonywania regresji liniowej `lm`. Drugi typ omówię w podrozdziale [-@sec-rstatix].

Funkcja `lm` to prototyp modeli regresyjnych. W identyczny sposób wykonujemy w R inne podstawowe modele takie jak ANOVA (`aov`). W sposób bardzo podobny wykonujemy w `rstatix` test t, więc nie tylko w regresji używamy formuł. Także co to jest ta formuła? To szczególny typ wskazywania kolumn z użyciem znaku `~` (tylda). Jeśli chcielibyśmy wykonać regresję, w której sprawdzalibyśmy związek długości płatków irysa długością i szerokością jego działek kielicha (słynna baza danych `iris`, dostępna automatycznie w R), zrobilibyśmy coś takiego:

```{r}
# zerkamy w bazę
glimpse(iris)

# tworzymy model
lm_price <- iris %>%
    lm(Petal.Length ~ Sepal.Length + Sepal.Width, data = .)

# próbujemy wyświetlić
lm_price
```

Obiekty tego typu zawsze potrzebują otrzymać argument `data`. Dlaczego jednak zapisałem `data = .`? Problem polega na użyciu potoków. Standardowo potok `%>%` wrzuca rzeczy do pierwszego argumentu. Wiele funkcji, tak jak `lm`, nie przyjmuje bazy danych w pierwszym argumencie. Często nawet nie w drugim, więc najlepiej wprost go nazwać. Omawiany w podrozdziale [-@sec-rstatix] pakiet `rstatix` za pierwszy cel postawił sobie nawet przerobienie testów statystycznych tak, żeby lepiej współpracowały z potokami. Jak więc poradzić sobie w takiej sytuacji? Jedna opcja, to całkiem zrezygnować z potoków i zapisać `data = iris`. Tak też jest dobrze, ale co jeśli potrzebujemy użyć potoku? Na przykład przed stworzeniem modelu wykonujemy serię przekształceń danych? Warto pamiętać, że potoki nie są obowiązkowe, można równie dobrze zagnieżdżać funkcje, Cierpi na tym tylko czytelność kodu. Druga opcja, to wprost powiedzenie potokowi, gdzie ma wrzucić naszą bazę danych. Do tego właśnie służy `.`. Zapis `data = .` mówi potokowi „halo halo, proszę nie wrzucać bazy do pierwszego argumentu, tylko o tu, gdzie pokazałem(-am) kropką".

W powyższym przykładzie widać też, że `lm`, i podobnie wiele innych obiektów modeli, nie daje nam domyślnie sensownego podsumowania modelu. Wiele modeli wymaga od nas, byśmy wrzucili je do funkcji `summary` (nie mylić z `summarise`) żeby powiedziały nam coś sensownego.

```{r}
summary(lm_price)
```

Ten widok już jest możliwy do zinterpretowania. Jeśli wiemy co nieco o regresjach, możemy z niego odczytać, że oba predyktory są istotne statystycznie, podobnie jak statystyka *F*, a także że wyjaśniliśmy 87% wariancji. Nie jest to jednak najlepszy format wyświetlania wyników.

> Pozostała część już wkrótce.

## Czyste wyniki, czyli pakiet `broom` {#sec-broom}

### `glance` {#tidy}

### `tidy` {#glance}

### `augment` {#augment}

## Swiss Army Knife w R, czyli `rstatix` {#sec-rstatix}

# Formatowanie kodu {#sec-formatting}

## `lintr` {#lintr}

## `styler` {#styler}

# Posłowie {#sec-ending}

# Bibliografia {#sec-bibliography}
