[
  {
    "objectID": "toc.html",
    "href": "toc.html",
    "title": "Spis treści",
    "section": "",
    "text": "Tutaj znajduje się lista tekstów z tego bloga, posegregowane tematycznie. Statystykę i naukę R najlepiej czytać po kolei, przydatne narzędzia i psychologia to bardziej niezależne teksty. Niektóre teksty mogą znaleźć się na kilku listach.\n\nStatystyka\n\nWariancja – co to jest wariancja, jak się ją wyjaśnia i co to jest model.\nTesty statystyczne i wartość p – jak działają testy statystyczne, co to jest p i jak je interpretować.\nFunkcje – wprowadzenie do funkcji, uzupełnienie podstaw przed omówieniem regresji.\nKombinatoryka – krótkie omówienie podstawowych terminów kombinatoryki, może być wprowadzeniem do prawdopodobieństwa.\nGęstość prawdopodobieństwa – głębsze spojrzenie na wykresy gęstości prawdopodobieństwa i obliczenia na rozkładach normalnych.\nRozkłady próbkowania – logika Centralnego Twierdzenia Granicznego i rozkładów próbkowania. Do tego omówienie rozkładów t i ręczne liczenie wartości p.\n\n\n\nNauka R\n\nPodstawy R – wszystko, czego potrzeba, żeby zacząć z pisaniem analiz w R.\nListy i automatyzacja – wykonywanie zadań masowo, z wykorzystaniem list.\nWspółpraca w analizach z Git – jak pisać analizy statystyczne razem, wykorzystując GitHub i narzędzia z RStudio.\nMetaprogramowanie – zaawansowany tekst o Tidy Eval, odpowiada na problemy związane z pisaniem własnych funkcji.\nPisanie pakietów – jak zebrać swoje funkcje w jeden wygodny pakiet w R.\n\n\n\nPrzydatne narzędzia\n\nGenerowanie tekstu – jak generować powtarzalny tekst automatycznie, zamiast pisać tego ręcznie.\nWyrażenia regularne – jak wyszukiwać i zmieniać elementy istniejącego tekstu.\nGit – jak korzystać z Gita do współpracy nad kodem.\nNarzędzia AI do szukania artykułów – nowoczesne wyszukiwarki i podsumowywarki artykułów naukowych oparte o sztuczną inteligencję.\n\n\n\nPsychologia\n\nPamięć w służbie ucznia – artykulik pokonferencyjny o różnych metodach uczenia się i ich znaczeniu w systemie edukacji."
  },
  {
    "objectID": "posts/pakiety-w-R.html",
    "href": "posts/pakiety-w-R.html",
    "title": "Tworzenie pakietów w R",
    "section": "",
    "text": "Pakiety to nic innego jak zbiory funkcji. Od zwykłych plików .R z funkcjami różnią się tym, że mają określoną strukturę, dzięki której można je potem zainstalować i szybko zacząć wykorzystywać. Pewnie pierwszym skojarzeniem z pisaniem pakietów jest udostępnianie ich całej społeczności przez CRAN. Owszem, można to zrobić, ale można mieć też prywatne pakiety trzymane na GitHubie. Wtedy taki pakiet może być chałupniczy, niepełny, nieelegancki, ale i tak jest lepszą drogą, niż luźne pliki z definicjami funkcji. Dzięki temu nie musimy od razu wszystkiego zrobić na tip-top, żeby każdy na świecie wygodnie korzystał z naszych trzech funkcji. Naszym celem jest w końcu oszczędzenie sobie pracy, nie dołożenie."
  },
  {
    "objectID": "posts/pakiety-w-R.html#autorzy",
    "href": "posts/pakiety-w-R.html#autorzy",
    "title": "Tworzenie pakietów w R",
    "section": "3.1 Autorzy",
    "text": "3.1 Autorzy\nJeśli chodzi o autorów pakietu, podajemy ich w dość specyficzny sposób. Najpierw jednak kasujemy całą linijkę Maintainer. To część starego standardu, dziś osobę odpowiedzialną za kod podajemy w polu autorów. Samych autorów podamy za pomocą funkcji person(). Pokażmy to sobie na przykładzie.\n\n\n\nDESCRIPTION\n\nAuthors@R: person(\n      \"Jakub\", \"Jędrusiak\",\n      email = \"kuba23031999@gmail.com\",\n      role = c(\"aut\", \"cre\"),\n      comment = c(\n        ORCID = \"0000-0002-6481-8210\",\n        affiliation = \"University of Wrocław\"\n        )\n      )\n\n\nPo pierwsze znacznik Author zmieniamy na Authors@R i to niezależnie od liczby autorów. Jeśli autor jest jeden, podajemy go za pomocą funkcji person(). Podstawowe informacje to imię w pierwszym argumencie3, nazwisko w drugim. Tych argumentów nie musimy nazywać. Dalej mamy inne informacje, które już nazwać warto. email jaki jest, każdy widzi. role to opis funkcji osoby w tworzeniu pakietu. Używamy tutaj kodów opisanych w dokumentacji funkcji person(). Kilka podstawowych kodów ról wymieniam w tabeli poniżej. Jak widać w przykładzie, każda osoba może mieć więcej niż jedną rolę. Kolejnym polem jest comment, który zawsze podajemy w postaci nazwanego wektora. Podstawowymi informacjami, które możemy tutaj umieścić, są numer ORCiD oraz afiliacja.\n\n\n\n\n\n\n\n\nKod\nFunkcja\nOpis\n\n\n\n\n\"aut\"\nAuthor\nPełny autor; wszystkie osoby, które pojawiają się w cytowaniu, powinny mieć przypisaną tę rolę.\n\n\n\"cre\"\nCreator\nOsoba odpowiedzialna za dbanie o kod w pakiecie (maintainer).\n\n\n\"ctb\"\nContributor\nOsoby, które wniosły jakiś istotny wkład w pakiet, ale za mały, by pojawiać się w cytowaniu.\n\n\n\"cph\"\nCopyright holder\nOsoba lub instytucja posiadająca prawa autorskie do pakietu.\n\n\n\nJeśli autorów jest więcej niż jeden, podajemy ich w postaci wektora osób. Poniżej przykład.\n\n\n\nDESCRIPTION\n\nAuthors@R: c(\n    person(\n      \"Boris\", \"Forthmann\",\n      email = \"boris.forthmann@uni-muenster.de\",\n      role = \"aut\",\n      comment = c(\n        ORCID = \"0000-0001-9755-7304\",\n        affiliation = \"University of Münster\"\n      )\n    ),\n    person(\n      \"Maciej\", \"Karwowski\",\n      email = \"maciej.karwowski@uwr.edu.pl\",\n      role = \"aut\",\n      comment = c(\n        ORCID = \"0000-0001-6974-1673\",\n        affiliation = \"University of Wrocław\"\n      )\n    ),\n    person(\n      c(\"Roger\", \"E.\"), \"Beaty\",\n      email = \"rebeaty@psu.edu\",\n      role = \"aut\",\n      comment = c(\n        ORCID = \"0000-0001-6114-5973\",\n        affiliation = \"Pennsylvania State University\"\n        )\n      ),\n    person(\n      \"Jakub\", \"Jędrusiak\",\n      email = \"kuba23031999@gmail.com\",\n      role = c(\"aut\", \"cre\"),\n      comment = c(\n        ORCID = \"0000-0002-6481-8210\",\n        affiliation = \"University of Wrocław\"\n        )\n      )\n    )"
  },
  {
    "objectID": "posts/pakiety-w-R.html#citation",
    "href": "posts/pakiety-w-R.html#citation",
    "title": "Tworzenie pakietów w R",
    "section": "3.2 CITATION",
    "text": "3.2 CITATION\nJeśli chcemy, by nasz pakiet był cytowany w jakiś konkretny sposób, możemy wprost go wskazać w pliku CITATION. Wytwarzamy go już po uzupełnieniu koniecznych informacji w pliku DESCRIPTION za pomocą komendy usethis::use_citation(). Sam plik siedzi sobie w folderze inst. Umieszczamy w nim informacje o cytowaniu w formacie BibTeX (każdy generator potrafi taką wytworzyć), a jeśli chcemy, to także wersję tekstową. Decyduje to o sposobie, w jaki wyświetla się wynik funkcji citation(). Poniżej przykładowy plik CITATION i jego efekt.\n\n\n\nCITATION\n\ncitHeader(\"To cite mtscr in publications use:\")\n\ncitEntry(\n  entry    = \"Manual\",\n  title    = \"Multidimensional Top Scoring for Creativity Research\",\n  author   = \"Boris Forthmann, Maciej Karwowski, Roger E. Beaty, Jakub Jędrusiak\",\n  year     = \"2023\",\n  url      = \"https://github.com/jakub-jedrusiak/mtscr\",\n  textVersion = paste(\n\"Forthmann, B., Karwowski, M., Beaty, R. E., Jędrusiak, J. (2023). Multidimensional Top Scoring for Creativity Research. Retrieved from: https://github.com/jakub-jedrusiak/mtscr\"\n  )\n)\n\n\n\ncitation(\"mtscr\")\n\nTo cite mtscr in publications use:\n\n  Forthmann, B., Karwowski, M., Beaty, R. E., Jędrusiak, J. (2023).\n  Multidimensional Top Scoring for Creativity Research. Retrieved from:\n  https://github.com/jakub-jedrusiak/mtscr\n\nWpis BibTex dla użytkowników LaTeX to\n\n  @Manual{,\n    title = {Multidimensional Top Scoring for Creativity Research},\n    author = {Boris Forthmann and Maciej Karwowski and Roger E. Beaty and Jakub Jędrusiak},\n    year = {2023},\n    url = {https://github.com/jakub-jedrusiak/mtscr},\n  }"
  },
  {
    "objectID": "posts/pakiety-w-R.html#licencja",
    "href": "posts/pakiety-w-R.html#licencja",
    "title": "Tworzenie pakietów w R",
    "section": "3.3 Licencja",
    "text": "3.3 Licencja\nSwój kod warto wprost licencjonować, ale licencji nie dodajemy do pakietu ręcznie. Używamy tutaj odpowiedniej funkcji z pakietu usethis, np. dla licencji MIT będzie to usethis::use_mit_license(), a dla GPL v.3 usethis::use_gpl3_license(). Pełną listę dostępnych licencji można znaleźć tutaj. Jaką licencję wybrać? Najprościej mówiąc, MIT to de facto pójście we w pełni wolny kod. Taki kod może np. stać się częścią płatnego programu, ale nasze prawa autorskie w stosunku do niego ciągle obowiązują. GPL natomiast wymusza, by programy oparte na naszym kodzie też były licencjonowane pod GPL, czyli by zawsze były darmowe. Większość pakietów R (ok. 70%) korzysta z licencji GPL."
  },
  {
    "objectID": "posts/pakiety-w-R.html#sec-dependencies",
    "href": "posts/pakiety-w-R.html#sec-dependencies",
    "title": "Tworzenie pakietów w R",
    "section": "3.4 Zależności",
    "text": "3.4 Zależności\nNajpewniej nie będziemy pisać całego naszego kodu w podstawowym R, a raczej będziemy wykorzystywać w nim inne pakiety. Żeby dodać jakiś pakiet jako zależność, używamy np. usethis::use_package(\"rlang\", \"dplyr\"). Nie należy nigdy podawać tidyverse jako zależności, a raczej konkretne pakiety typu dplyr czy stringr. Do tego możemy wymusić wykorzystanie konkretnej minimalnej wersji danego pakietu argumentem min_version, np. pisząc usethis::use_package(\"dplyr\", min_version = \"1.1.0\").\nJeśli zerkniemy w plik DESCRIPTION po uruchomieniu takiej komendy, zobaczymy, że dodane pakiety znalazły się w sekcji Imports. Tak powinno być. Istnieją jednak inne możliwe sekcje. Jeśli dodajemy pakiet, który nie jest potrzebny do działania naszych funkcji, ale na przykład potrzebujemy go do uruchomienia przykładów (bo chociażby wykorzystujemy w nich dane diamonds z pakietu ggplot2, podczas gdy nie wykorzystujemy samego ggplot2 w naszych funkcjach), dodajemy go w sekcji Suggests. Robimy to pisząc na przykład usethis::use_package(\"ggplot2\", type = \"Suggests\").\nIstnieje seria zależności, które posiada większość pakietów i które można łatwo dodać poprzez specjalne funkcje z pakietu usethis. Wymieńmy sobie te najważniejsze:\n\nusethis::use_pipe() – dodaje obsługę potoków %&gt;% z pakietu magrittr.\nusethis::use_tibble() – dodaje obsługę tibble jako formy, w której nasze funkcje zwracają dane.\nusethis::use_tidy_dependencies() – standardowe zależności typu rlang, cli czy glue. Według mnie powinno się uruchamiać zawsze.\nusethis::use_tidy_eval() – podstawowe pakiety do Tidy Eval. Według mnie powinno się uruchamiać zawsze.\nusethis::use_lifecycle() – dodaje zależność od pakietu lifecycle, dzięki któremu możemy oznaczyć nasze funkcje jako eksperymentalne albo wycofane. Więcej informacji tutaj.\n\nDodatkowo większość pakietów posiada wprost wskazaną zależność od samego R. Taka zależność musi pojawić się w sekcji Depends (zamiast standardowego Imports) i musi uwzględniać minimalną wersję co najmniej 3.5.04. W praktyce najlepiej jest podawać minimalną wersję 4.1.0, bo w tej wersji dodano natywny potok |&gt;, a już w ogóle najlepiej nie cofać się przed 4.2.0, kiedy potoki wzbogacono o _, czyli informację, gdzie ma się znaleźć dany argument, jeśli nie na pierwszym miejscu (odpowiednik . w magrittr). Można to wykonać funkcją usethis::use_package(\"R\", type = \"Depends\", min_version = \"4.2.0\"). Zwrócę tutaj uwagę, że trzecia liczba w zależności od R musi być zero. Nie można podać na przykład zależności z minimalną wersją 4.2.2."
  },
  {
    "objectID": "posts/pakiety-w-R.html#odznaki",
    "href": "posts/pakiety-w-R.html#odznaki",
    "title": "Tworzenie pakietów w R",
    "section": "4.1 Odznaki",
    "text": "4.1 Odznaki\nBardzo często w repozytoriach znajdziemy odznaki. Są to elementy informacyjno-ozdobne, które mówią coś o naszym pakiecie, np. że przechodzi testy, że jest w CRAN albo ile razy w miesiącu ktoś go pobiera. Ciekawe odznaki obejmują:\n\nOdznakę R-CMD-check, o której piszę w podrozdziale 5.1.\nOdznakę lifecycle, którą możemy poinformować, że nasz pakiet jako całość jest eksperymentalny, stabilny lub wycofany. Możemy ją dodać wykorzystując komendę usethis::use_lifecycle_badge(\"stable\"). Argument wybieramy jaki chcemy, zgodnie z listą dostępną tutaj.\nOdznakę CRAN, która informuje, jaka wersja naszego pakietu dostępna jest w CRAN. Jeśli pakiet nie jest dostępny, to odznaka też może o tym poinformować. Dodajemy ją komendą usethis::use_cran_badge().\n\n\n\n\n\nKilka odznak w nagłówku pliku README na GitHubie.\n\n\n\n\n\n\n\nKilka odznak w nagłówku pliku README na GitHubie.\n\n\n\nOdznaki nie są obowiązkową częścią pakietu, ale piszę o nich, bo lubię takie ładne pierdółki. Inne przykłady odznak znajdziemy na shields.io, albo wykorzystując pakiet badger."
  },
  {
    "objectID": "posts/pakiety-w-R.html#sec-GitHubActions",
    "href": "posts/pakiety-w-R.html#sec-GitHubActions",
    "title": "Tworzenie pakietów w R",
    "section": "5.1 R-CMD-check na GitHubie",
    "text": "5.1 R-CMD-check na GitHubie\nKolejną polecaną przeze mnie czynnością, która (według mnie) powinna zostać wykonana w każdym repozytorium, jest uruchomienie tej komendy:\n\nusethis::use_github_action_check_standard()\n\nDodaje ona kilka rzeczy. Po pierwsze sprawia ona, że po każdym naszym commicie GitHub wykonuje zestaw podstawowych testów zwanych R-CMD-check, o których więcej piszę w podrozdziale Sekcja 8. W drugiej kolejności dodaje ona odznakę R-CMD-check do naszego pliku README, żebyśmy mogli błyszczeć przechodzeniem testów."
  },
  {
    "objectID": "posts/pakiety-w-R.html#sec-cran-checks",
    "href": "posts/pakiety-w-R.html#sec-cran-checks",
    "title": "Tworzenie pakietów w R",
    "section": "6.1 Specyficzne wymogi CRAN i R-CMD-check",
    "text": "6.1 Specyficzne wymogi CRAN i R-CMD-check\nZałóżmy, że napisaliśmy już własną funkcję, która liczy podstawowy zestaw statystyk opisowych i zwraca je w ulubionej przez nas formie. Wykorzystam tutaj funkcję z tekstu o metaprogramowaniu. Definicję zapisuję w pliku opisowe.R, jaki wytworzyłem w poprzednim akapicie w folderze R.\n\n\n\nopisowe.R\n\nopisowe &lt;- function(df, group, ...) {\n  kolumny_do_policzenia &lt;- enquos(...)\n\n  df %&gt;%\n    group_by({{ group }}) %&gt;%\n    summarise(\n      across(\n        c(!!!kolumny_do_policzenia),\n        .fns = list(\n          N = \\(x) sum(!is.na(x)),\n          M = \\(x) mean(x, na.rm = TRUE),\n          SD = \\(x) sd(x, na.rm = TRUE),\n          A = agricolae::skewness,\n          K = agricolae::kurtosis,\n          `NA` = \\(x) sum(is.na(x))\n        )\n      )\n    )\n}\n\n\nPonieważ jest to pakiet, musimy trochę przepisać naszą funkcję, jeśli nie chcemy, żeby R-CMD-check później na nas krzyczał. Jeśli nas to nie obchodzi, możemy to pominąć, ale lepiej jest od razu nabierać dobrych nawyków. Takie dopasowywanie funkcji dotyczy zazwyczaj tylko kilku rzeczy, z których najważniejsza to globalne definicje obiektów. R musi wiedzieć dokładnie, czym są pokazywane mu obiekty, żeby się nie buntował, niezależnie od tego, czy tym obiektem jest funkcja, czy baza danych.\n\nFunkcje powinny być zapisane w postaci pełnych nazw, czyli np. dplyr::group_by(). Pakiety, z których pochodzą te funkcje, powinny być dodane jako zależności (por. podrozdział 3.4). Jeśli użyliśmy usethis::use_tidy_eval() (por. podrozdział 3.4), to funkcje z pakietu rlang są ładowane z automatu i nie potrzebują pełnych nazw.\nJeśli w funkcjach typu mutate() wykorzystujemy nazwy kolumn bez cudzysłowu, np. mutate(iris, millimeters = Sepal.Length * 10), R-CMD-check będzie krzyczał, że Sepal.Length nie ma globalnej definicji. W takim wypadku używamy wyrażenia .data z rlang i piszemy dplyr::mutate(iris, millimeters = .data$Sepal.Length * 10).\nJeśli w funkcjach wybierających kolumny (przede wszystkim select()) piszemy nazwy kolumn, to podajemy je w cudzysłowie. Powód jest taki, jak wyżej, brak globalnej definicji, ale .data nie nadaje się do tidyselect. Dla przykładu zamiast select(iris, Sepal.Width, Sepal.Length) powinniśmy napisać dplyr::select(iris, \"Sepal.Width\", \"Sepal.Length\").\nNa ogół lepiej używać natywnego potoku |&gt; niż %&gt;%. Jeśli chcemy używać potoku z magrittr używamy wcześniej usethis::use_pipe(). Natywny potok oszczędza nam dodatkowej zależności.\n\nPoprawiona funkcja mogłaby więc wyglądać tak:\n\n\n\nopisowe.R\n\nopisowe &lt;- function(df, group, ...) {\n  kolumny_do_policzenia &lt;- enquos(...)\n\n  df |&gt;\n    dplyr::group_by({{ group }}) |&gt;\n    dplyr::summarise(\n      dplyr::across(\n        c(!!!kolumny_do_policzenia),\n        .fns = list(\n          N = \\(x) sum(!is.na(x)),\n          M = \\(x) mean(x, na.rm = TRUE),\n          SD = \\(x) stats::sd(x, na.rm = TRUE),\n          A = agricolae::skewness,\n          K = agricolae::kurtosis,\n          `NA` = \\(x) sum(is.na(x))\n        )\n      )\n    )\n}"
  },
  {
    "objectID": "posts/pakiety-w-R.html#dokumentowanie-z-roxygen2",
    "href": "posts/pakiety-w-R.html#dokumentowanie-z-roxygen2",
    "title": "Tworzenie pakietów w R",
    "section": "6.2 Dokumentowanie z roxygen2",
    "text": "6.2 Dokumentowanie z roxygen2\nGdy mamy już naszą funkcję napisaną, przyszedł czas ją udokumentować. Niezależnie, czy robimy mały pakiecik do użytku własnego, czy może coś, co chcemy ostatecznie umieścić w CRAN, warto tego nie zaniedbywać. Dokumentacja w R jest cudowna, łatwa w obsłudze i zawsze dostępna. Potraktuj swoją dokumentację jako ustrukturyzowane notatki, dzięki którym nie zapomnisz, na czym polegały Twoje funkcje i jak ich używać. Tworzymy ją za pomocą specjalnego szkieletu obsługiwanego przez pakiet roxygen2. Możemy albo napisać taki szkielet od podstaw, albo wejść kursorem tesktowym do naszej funkcji i w RStudio z menu Code wybrać Insert Roxygen Skeleton (skrót klawiaturowy Ctrl+Alt+Shift+R). Taki szkielet szkieletu uzupełniamy ręcznie. Pokażmy to sobie na przykładzie.\n\n\n\nopisowe.R\n\n#' Statystyki opisowe\n#'\n#' Funkcja licząca podstawowe statystyki opisowe: liczność, średnią, SD,\n#' skośność, kurtozę i liczbę braków danych.\n#'\n#' @param df ramka danych z kolumnami do policzenia.\n#' @param group kolumna do grupowania.\n#' @param ... nazwy kolumn, dla których mają być policzone statystyki.\n#'\n#' @return ramka danych z kolumnami `N`, `M`, `SD`, `A`, `K` i `NA`. Oryginalne nazwy\n#'     kolumn w formie przedrostka.\n#' @export\n#'\n#' @examples\n#' opisowe(iris, Species, Sepal.Width, Sepal.Length)\n\n\nPierwszym, co zwraca uwagę, jest fakt, że napisałem to po polsku. Owszem, można, ale jeśli chcemy udostępniać nasz kod na zewnątrz, lepiej dokumentować pakiet po angielsku.\nTen podstawowy opis funkcji zawiera kilka rzeczy:\n\nNa samej górze znajduje się tytuł funkcji, który lapidarnie ale konkretnie opisuje jej przeznaczenie.\nDalej opis działania funkcji. Również powinien być krótki i konkretny. Jeśli potrzebujemy więcej niż jednego akapitu, musimy jawnie napisać @description. W innym wypadku wystarczy zejść linijkę poniżej tytułu, tak jak pokazałem w przykładzie.\nNastępnie mamy trzy tagi @param, z których każdy opisuje kolejny argument, jaki funkcja przyjmuje. Warto tutaj opisać postać, w jakiej argument musi zostać podany, np. logical albo dataframe.\nTag @return opisuje, co funkcja zwraca. W tym wypadku jest to opis stworzonej ramki danych ze statystykami opisowymi.\nTag @export nie zawiera żadnych informacji. Umieszcza się go w tych funkcjach, do których użytkownik powinien mieć dostęp.\nNa samym końcu mamy @examples, czyli przykłady użycia funkcji. Przykłady są obowiązkowe i obowiązkowo muszą zadziałać. Jeśli jakiś przykład specjalnie zwraca błąd (możemy w końcu chcieć pokazać, jak naszej funkcji nie używać), należy go umieścić w klamrze \\dontrun{}, np. \\dontrun{opisowe(iris)}.\n\nDo wymienionych wyżej podstawowych tagów możemy dodać także m.in. sekcje @details czy @seealso. Poniżej przykład strony w dokumentacji generowanej przez podstawowe tagi.\n\n\n\n\nDokumentacja funkcji opisowe().\n\n\n\n\n\n\n\nDokumentacja funkcji opisowe().\n\n\n\nŻeby przerobić szkielety roxygen2 na rzeczywiste pliki dokumentacji (czyli pliki .Rd w folderze .man), należy uruchomić komendę devtools::document(). Tutaj zaczynają się ujawniać komendy, których warto używać często, a już zwłaszcza przed wysłaniem aktualizacji na GitHuba. Zbieram je wszystkie na końcu, w podsumowaniu.\nW przykładzie widać teź, że mogę używać w swoich szkieletach formatowania markdown. Jest to jednak możliwe tylko, jeśli wcześniej użyłem usethis::use_roxygen_md(), o czym pisałem w podrozdziale 2."
  },
  {
    "objectID": "posts/pakiety-w-R.html#dokumentacja-całego-pakietu",
    "href": "posts/pakiety-w-R.html#dokumentacja-całego-pakietu",
    "title": "Tworzenie pakietów w R",
    "section": "6.3 Dokumentacja całego pakietu",
    "text": "6.3 Dokumentacja całego pakietu\nMożemy dodać ogólną stronę w dokumentacji dotyczącą naszego pakietu jako całości. Wykonuje się to w specjalnym pliku .R, który można dodać funkcją usethis::use_package_doc(). Jeśli jednak użyłeś(-aś) wcześniej funkcji usethis::use_tidy_dependencies(), to wymusiła ona na Tobie wytworzenie takiego pliku. Niezależnie od wybranej drogi, po uruchomieniu jednej z tych komend znajdziesz w folderze R plik o nazwie typu typu packagename-package.R, gdzie możesz napisać dokumentację pakietu standardowo, jak w każdym innym pliku .R."
  },
  {
    "objectID": "posts/pakiety-w-R.html#surowe-dane-w-data-raw",
    "href": "posts/pakiety-w-R.html#surowe-dane-w-data-raw",
    "title": "Tworzenie pakietów w R",
    "section": "7.1 Surowe dane w data-raw",
    "text": "7.1 Surowe dane w data-raw\nNa początku potrzebujemy surowego pliku z danymi, np. w formacie .csv. Wrzucamy go do folderu data-raw, który tworzymy w głównym folderze naszego pakietu. Następnie tworzymy skrypt czyszczący. Najłatwiej wytworzyć go funkcją usethis::use_data_raw(), która za pierwszy argument przyjmuje nazwę, którą chcemy naszej bazie danych nadać. Mogłoby to być coś w stylu usethis::use_data_raw(\"iris\"). Spowoduje to wytworzenie w folderze data-raw skryptu iris.R, który w tym momencie powinien nam się otworzyć w RStudio. Zawiera on tylko komentarz i jedną komendę usethis::use_data(). Powyżej tej komendy piszemy skrypt, który ładuje i czyści nasze surowe dane do takiej formy, którą chcemy załączyć w pakiecie. Gotowe dane zapisujemy do zmiennej o wybranej wcześniej nazwie. I to wystarczy. Podczas instalacji naszego pakietu zostaną załadowane też dane. Dostęp do nich możemy uzyskać za pomocą komendy, która w pełnej wersji ma postać data(\"diamonds\", package = \"ggplot2\"). Poniżej przykład gotowego skryptu ładującego.\n\n\n\nmtscr_creativity.R\n\nmtscr_creativity &lt;- readr::read_csv(\"data-raw/study2.csv\") |&gt;\n  dplyr::select(-response_nofill, -item_nofill) |&gt;\n  dplyr::filter(\n    item %in% c(\n      \"belt\", \"brick\", \"broom\",\n      \"bucket\", \"candle\", \"clock\",\n      \"comb\", \"knife\", \"lamp\",\n      \"pencil\", \"pillow\",\n      \"purse\", \"sock\"\n    ),\n    any(is.na(SemDis_MEAN))\n  )\n\nusethis::use_data(mtscr_creativity, overwrite = TRUE)"
  },
  {
    "objectID": "posts/pakiety-w-R.html#dokumentacja-plików-z-danymi",
    "href": "posts/pakiety-w-R.html#dokumentacja-plików-z-danymi",
    "title": "Tworzenie pakietów w R",
    "section": "7.2 Dokumentacja plików z danymi",
    "text": "7.2 Dokumentacja plików z danymi\nJeśli załączamy w naszym pakiecie dane, to je również powinniśmy udokumentować. Tworzymy w tym celu specjalny plik data.R w folderze R, w którym opisujemy wszystkie nasze pliki z danymi. Pod szkieletem roxygen2 umieszczamy nazwę naszej bazy danych w cudzysłowie. Poniżej skrócony przykład.\n\n\n\ndata.R\n\n#' Creativity assessment through semantic distance dataset\n#'\n#' A dataset from Forthmann, Karwowski & Beaty ([2023](https://doi.org/10.1037/aca0000571)) paper.\n#' It contains a set of responses in Alternative Uses Task for different items with their\n#' semantic distance assessment.\n#'\n#' @return a [tibble][tibble::tibble-package]\n#' @format ## `mtscr_creativity`\n#' A `tibble` with 4585 rows and 3 columns:\n#' \\describe{\n#'   \\item{id}{patricipants' unique identification number}\n#'   \\item{response}{response in AUT}\n#'   \\item{item}{item for which alternative uses were searched for}\n#' }\n#'\n#' @source &lt;https://osf.io/7rgsp/&gt;\n\"mtscr_creativity\"\n\n\nSpecyficzne tagi to @format, w którym opisujemy każdą kolumnę i @source, w którym podajemy źródło naszych danych. \\describe{} to sposób na dodanie do dokumentacji listy."
  },
  {
    "objectID": "posts/pakiety-w-R.html#typ-argumentu",
    "href": "posts/pakiety-w-R.html#typ-argumentu",
    "title": "Tworzenie pakietów w R",
    "section": "9.1 Typ argumentu",
    "text": "9.1 Typ argumentu\nZazwyczaj najpierw sprawdzamy, czy obiekt podany przez użytkownika jako argument ma odpowiedni typ. Na przykład jeśli naszym pierwszym argumentem jest ramka danych df, możemy chcieć upewnić się, czy to rzeczywiście jest ramka danych.\n\nwinda_do_nieba &lt;- 2 + 1 # coś, co nie jest ramką danych\n\ninput_check &lt;- function(df) {\n    if (!is.data.frame(df)) {\n        cli::cli_abort(\n            c(\n                \"{.arg df} must be a data frame.\",\n                \"x\" = \"Provided {.arg df} is {.obj_type_friendly {df}}\"\n            )\n        )\n    }\n}\n\ninput_check(winda_do_nieba)\n\nError in `input_check()`:\n! `df` must be a data frame.\n✖ Provided `df` is a number\n\n\nKlamra if sprawdza, czy df nie jest ramką danych (zwróćmy uwagę na wykrzyknik !). Jeśli nie jest, uruchamiana jest funkcja cli_abort(). Zatrzymuje ona wykonywanie kodu i pokazuje w konsoli błąd w estetycznej formie.\nJak używać tej funkcji? Ma ona całkiem sporo możliwości. Najprostszą rzeczą, którą może przyjąć, jest sam komunikat z błędem. Może to być zwykły tekst. Mamy też dodatkowe możliwości formatowania, o których jednak później. Komunikat jest opatrzony ikonką, która zależy od tego, czy chcemy wyrzucić błąd, ostrzeżenie czy informację."
  },
  {
    "objectID": "posts/pakiety-w-R.html#punktory",
    "href": "posts/pakiety-w-R.html#punktory",
    "title": "Tworzenie pakietów w R",
    "section": "9.2 Punktory",
    "text": "9.2 Punktory\nMożemy jednak dokładać kolejne komunikaty i kolejne ikonki. W takim wypadku do funkcji cli_abort() wrzucamy wektor komunikatów. Pierwszy komunikat jest nienazwany, zaś w kolejnych wprost wskazujemy, jakie chcemy punktory. Cała lista dostępna jest w dokumentacji cli::cli_bullet(). Dostępne w chwili, gdy to piszę, pokazuję niżej.\n\ncli::cli_bullets(\n    c(\n        \"bez wcięcia\",\n        \" \" = \"wcięcie\",\n        \"*\" = \"punktor\",\n        \"&gt;\" = \"strzałka\",\n        \"v\" = \"sukces\",\n        \"x\" = \"zagrożenie\",\n        \"!\" = \"ostrzeżenie\",\n        \"i\" = \"informacja\"\n    )\n)\n\nbez wcięcia\n\n\n  wcięcie\n\n\n• punktor\n\n\n→ strzałka\n\n\n✔ sukces\n\n\n✖ zagrożenie\n\n\n! ostrzeżenie\n\n\nℹ informacja"
  },
  {
    "objectID": "posts/pakiety-w-R.html#formatowanie-z-glue",
    "href": "posts/pakiety-w-R.html#formatowanie-z-glue",
    "title": "Tworzenie pakietów w R",
    "section": "9.3 Formatowanie z glue",
    "text": "9.3 Formatowanie z glue\nPoza punktorami możemy dodatkowo formatować tekst korzystając ze składni dostarczanej przez glue. cli obsługuje ją automatycznie. Wszystkie szczegóły oraz jak to wygląda rzeczywiście w konsoli znajdziemy na tej stronie dokumentacji. Ja omówię tutaj same podstawy. Jeśli chodzi o to, jak formatować, jak to powinno wyglądać, osobiście często sprawdzam w tidyverse. Próbuję wywołać podobny błąd np. w funkcji dplyr::select() i patrzę, co mi wyskakuje. Traktuję to jako wzorzec, podpowiedź.\nTekst, który chcemy sformatować, bierzemy w nawiasy klamrowe. W nawiasie zaczynamy od określenia, w jaki sposób chcemy dany tekst sformatować. Poniżej lista możliwych formatów.\n\ncli_li(\"Tekst {.emph kursywą}.\")\ncli_li(\"{.strong Pogrubiony} tekst.\")\ncli_li(\"Fragment kodu: {.code sum(a) / length(a)}.\")\ncli_li(\"Nazwa pakietu: {.pkg cli}.\")\ncli_li(\"Nazwa funkcji: {.fn cli_text}.\")\ncli_li(\"Klawisz: wciśnij {.kbd spację}.\")\ncli_li(\"Nazwa lub ścieżka pliku: {.file /usr/bin/env}.\")\ncli_li(\"Adres mailowy: {.email marylala@marylarodowicz.pl}.\")\ncli_li(\"Adres WWW: {.url https://example.com}.\")\ncli_li(\"Zmienna środowiskowa: {.envvar R_LIBS}.\")\ncli_li(\"Typ obiektu: `mtcars` is {.obj_type_friendly {mtcars}}\")\n\nJeśli chcemy, możemy wykonywać też operacje w nawiasach klamrowych. Zazwyczaj będziemy wtedy zagnieżdżać jedne nawiasy w drugich. Najlepiej pokazać to na przykładzie.\n\ncli_warn(\"Złamano {.strong {sum(365, 248)}} postanowień. Ostrzegam cię.\")\n\nWarning: Złamano 613 postanowień. Ostrzegam cię.\n\n\nWewnątrz klamry z pogrubieniem dołożyłem drugą klamrę, w której umieściłem funkcję sumującą. Umieszczenie funkcji w klamrze sprawiło, że w ostrzeżeniu pojawia nam się wynik, 613, a nie tekst sum(365, 248). Nie jest to zbyt użytkowy przykład, ale dobrze jest wiedzieć, że tak się da. W kolejnym przykładzie zobaczymy, jak można tego użyć do dostosowywania informacji zwrotnych pod użytkownika."
  },
  {
    "objectID": "posts/pakiety-w-R.html#obecność-kolumn-w-bazie",
    "href": "posts/pakiety-w-R.html#obecność-kolumn-w-bazie",
    "title": "Tworzenie pakietów w R",
    "section": "9.4 Obecność kolumn w bazie",
    "text": "9.4 Obecność kolumn w bazie\nPoza sprawdzeniem typu argumentu, możemy chcieć się upewnić, że kolumny wskazane przez użytkownika rzeczywiście są obecne w bazie. Pomocna będzie tutaj funkcja rlang::has_name(), która dokładnie to sprawdza. Jeśli obiekt istnieje, has_name() wyrzuca TRUE, a jeśli nie, wyrzuca FALSE. Niestety has_name() wymaga nazwy kolumny w postaci string, dlatego musimy pobawić się trochę z Tidy Eval. Jeśli nie wiesz, jak to robić, odsyłam do tego tekstu.\n\ncol_check &lt;- function(df, id_column) {\n    id_column &lt;- enquo(id_column)\n\n    if (!rlang::has_name(df, rlang::as_name(id_column))) {\n        cli::cli_abort(\n            c(\n                \"All columns must exist in the data.\",\n                \"x\" = \"Column {.var {rlang::as_name(id_column)}} does not exist.\",\n                \"i\" = \"Check the spelling.\"\n            )\n        )\n    }\n}\n\ncol_check(iris, Sepal.Density)\n\nError in `col_check()`:\n! All columns must exist in the data.\n✖ Column `Sepal.Density` does not exist.\nℹ Check the spelling.\n\n\nW powyższym przykładzie widzimy też, do czego może nam się przydać zagnieżdżanie nawiasów klamrowych. Tutaj wykorzystuję je do zawarcia nazwy nieistniejącej kolumny w komunikacie o błędzie."
  },
  {
    "objectID": "posts/pakiety-w-R.html#struktura-testów",
    "href": "posts/pakiety-w-R.html#struktura-testów",
    "title": "Tworzenie pakietów w R",
    "section": "10.1 Struktura testów",
    "text": "10.1 Struktura testów\nWszystkie testy znajdują się w folderze tests, w podfolderze testthat. Możemy je wytwarzać ręcznie, ale zazwyczaj będziemy wykorzystywać komendę usethis::use_test(). W domyśle jeden plik z funkcjami powinien mieć odpowiadający mu plik z testami. W RStudio komenda usethis::use_test() nie potrzebuje żadnych argumentów – domyślnie tworzy plik z testami dla aktualnie otwartego skryptu.\nNowo utworzony plik zawiera przykładowy test, który możemy spokojnie usunąć. Żeby napisać własny test wykorzystamy przede wszystkim komendę test_that() oraz komendy z rodziny expect_*(). Komenda test_that() przyjmuje dwie rzeczy – opis, co test robi (jako string) oraz komendy składające się na test zapisane w nawiasach klamrowych. Warto zauważyć, że nie musimy pisać testthat::test_that(). Cały pakiet jest dostępny od chwili użycia usethat::use_testthat().\n\ntest_that(\"Jakiś element działa jak należy\", {\n    # Tutaj komendy składające się na test\n})\n\nW nawiasach klamrowych możemy zapomnieć, że jesteśmy wewnątrz funkcji i pisać kod normalnie. Możemy na przykład tworzyć przykładowe zbiory danych, na których będziemy wykonywać testy. Najważniejsze są jednak funkcje z rodziny expect_*(), za pomocą których mówimy, czego oczekujemy od danej funkcji. Rodzina ta jest szeroka, bo i funkcje do testowania są bardzo różne. W ramach jednej funkcji test_that() możemy użyć kilku tego typu komend, ale staramy się utrzymać porządek, żeby każda funkcja test_that() testowała coś konkretnego. Poniżej wymieniam kilka najważniejszych funkcji expect_*(), a bardziej rozszerzoną ich listę można znaleźć tutaj.\n\nexpect_identical() i expect_equal() – czy zadane wartości są takie same? Można ustawić tolerancję błędu.\nexpect_length() – czy obiekt ma określoną długość?\nexpect_names() – czy nazwy kolumn (wartości) są takie, jakie mają być? Pozwala zignorować kolejność i wielkość liter.\nexpect_match() – czy w wektorze tekstowym jest tekst odpowiadający wyrażeniu regularnemu?\nexpect_type() – czy obiekt jest określonego typu?\nexpect_error(), expect_warning() i expect_message() – jak nazwa wskazuje; czy dany kod generuje błąd/ostrzeżenie/informację? Pozwala też sprawdzać treść komunikatów za pomocą wyrażeń regularnych.\nexpect_true() i expect_false() – najbardziej uniwersalne funkcje sprawdzające. Jednocześnie lepiej zastąpić je innymi, jeśli jest możliwość, żeby wyniki testów były bardziej czytelne."
  },
  {
    "objectID": "posts/pakiety-w-R.html#przykładowe-testy",
    "href": "posts/pakiety-w-R.html#przykładowe-testy",
    "title": "Tworzenie pakietów w R",
    "section": "10.2 Przykładowe testy",
    "text": "10.2 Przykładowe testy\nZałóżmy, że chcemy napisać testy do naszej funkcji opisowe(). Przypomnijmy jej definicję.\n\n\n\nopisowe.R\n\nopisowe &lt;- function(df, group, ...) {\n    # check if df is a dataframe\n    if (!is.data.frame(df)) {\n        cli::cli_abort(\n            c(\n                \"{.arg df} must be a data frame.\",\n                \"x\" = \"Provided {.arg df} is {.obj_type_friendly {df}}\"\n            )\n        )\n    }\n\n    kolumny_do_policzenia &lt;- enquos(...)\n\n    df %&gt;%\n        group_by({{ group }}) %&gt;%\n        summarise(\n            across(\n                c(!!!kolumny_do_policzenia),\n                .fns = list(\n                    N = \\(x) sum(!is.na(x)),\n                    M = \\(x) mean(x, na.rm = TRUE),\n                    SD = \\(x) sd(x, na.rm = TRUE),\n                    A = agricolae::skewness,\n                    K = agricolae::kurtosis,\n                    `NA` = \\(x) sum(is.na(x))\n                )\n            )\n        )\n}\n\n\nDorzuciłem tutaj sprawdzenie, czy df jest ramką danych. Jakie testy moglibyśmy napisać dla tej funkcji? Poniżej kilka przykładów.\n\n\n\ntest-opisowe.R\n\n# create a test data frame with normally distributed data\nset.seed(123)\ndf &lt;- data.frame(\n    group = rep(c(\"A\", \"B\"), each = 50),\n    x = rnorm(100, mean = 0, sd = 1),\n    y = rnorm(100, mean = 5, sd = 2)\n)\n\n# test if function returns a data frame\ntest_that(\"opisowe returns a data frame\", {\n    expect_s3_class(opisowe(df, group, x, y), \"data.frame\")\n})\n\n# test if function calculates summary statistics correctly\ntest_that(\"opisowe calculates summary statistics correctly\", {\n    result &lt;- opisowe(df, group, x, y)\n    ## N\n    expect_equal(result$group, c(\"A\", \"B\"))\n    expect_equal(result$x_N, c(50, 50))\n    expect_equal(result$y_N, c(50, 50))\n    ## M\n    expect_equal(result$x_M, c(0.0344, 0.1464), tolerance = 0.001)\n    expect_equal(result$y_M, c(4.4921, 5.0776), tolerance = 0.001)\n    ## SD\n    expect_equal(result$x_SD, c(0.9258, 0.9054), tolerance = 0.001)\n    expect_equal(result$y_SD, c(1.9786, 1.8619), tolerance = 0.001)\n    ## A\n    expect_equal(result$x_A, c(0.1729, -0.0462), tolerance = 0.001)\n    expect_equal(result$y_A, c(0.4376, 1.0372), tolerance = 0.001)\n    ## K\n    expect_equal(result$x_K, c(-0.3366, 0.3234), tolerance = 0.001)\n    expect_equal(result$y_K, c(-0.1718, 1.8702), tolerance = 0.001)\n    ## NA\n    expect_equal(result$x_NA, c(0, 0))\n    expect_equal(result$y_NA, c(0, 0))\n})\n\n# test if function throws an error when df is not a data frame\ntest_that(\"opisowe throws an error when df is not a data frame\", {\n    expect_error(opisowe(list(), group, x, y))\n})\n\n\nPo pierwsze stworzyłem tutaj bazę danych, na której będziemy liczyć. Po drugie stworzyłem trzy zestawy testów. Pierwszy sprawdza, czy wynik to ramka danych. Drugi sprawdza, czy statystyki wyglądają, jak mają wyglądać, z dokładnością do 3. miejsca po przecinku. Ostatni sprawdza, czy funkcja wyrzuca błąd, gdy df będzie czymś innym, niż baza danych.\nGdy taki plik zapiszemy, testy są już uzbrojone. Możemy je wykonać używając komendy devtools::test_active_file(). Wszystkie testy zostaną też wykonane podczas devtools::check()."
  },
  {
    "objectID": "posts/pakiety-w-R.html#co-i-jak-testować",
    "href": "posts/pakiety-w-R.html#co-i-jak-testować",
    "title": "Tworzenie pakietów w R",
    "section": "10.3 Co i jak testować?",
    "text": "10.3 Co i jak testować?\nHot take, którego nie ma jeszcze w podręcznikach – sztuczna inteligencja jest świetna w pisaniu testów. Nie ma się co oszukiwać, pisanie testów jest upierdliwe, ale na szczęście Chat GPT może to sprawnie zrobić za nas. Od Chatu GPT lepszy jest GitHub Copilot X, do którego dostęp studenci i pracownicy naukowi mają za darmo. Czasami tylko testy generowane przez AI wykorzystują wycofane już komendy, np. expect_is() zamiast expect_type().\nTesty jest dobrze pisać, bo czasem możemy przegapić, że coś zepsuliśmy. Jeśli zabezpieczyliśmy się testami, devtools::check() nakrzyczy na nas, że coś zepsuliśmy. Nakrzyczy na nas też, jeśli nie wyjdą przykłady, ale przykład musi tylko zadziałać, a test może wymagać konkretnego wyjścia. Bez testów nie wykryjemy, że przykład zwraca bzdurę, bo jeśli zwraca cokolwiek, to R-CMD-check to wystarczy.\nSztuka polega na tym, żeby pisać testy tak, żeby nie trzeba było ich zmieniać za każdym jednym razem, jak będziemy coś zmieniać w funkcji. Zbyt duże struktury testów mogą sprawić, że kod będzie bardzo trudny do utrzymania. Małe poprawki mogą wymusić na nas każdorazowo szerokie zmiany w testach. Trzeba więc znaleźć jakiś punkt pośredni i nie dać się zwariować. Testy mają nam oszczędzić pracy, a nie dołożyć."
  },
  {
    "objectID": "posts/pakiety-w-R.html#sprawdzanie-code-coverage",
    "href": "posts/pakiety-w-R.html#sprawdzanie-code-coverage",
    "title": "Tworzenie pakietów w R",
    "section": "11.1 Sprawdzanie code coverage",
    "text": "11.1 Sprawdzanie code coverage\nCode coverage możemy bardzo łatwo sprawdzić zarówno dla pojedynczego pliku, jak i dla całego pakietu. Robią to odpowiednio funkcje devtools::test_coverage_active_file() i devtools::test_coverage(). Raport z takiego sprawdzenia zawiera ogólny code coverage, także w podziale na pliki oraz informację, jakie konkretne linijki zostały, a jakie nie zostały przetestowane.\n\n\n\nWynik devtools::test_coverage()."
  },
  {
    "objectID": "posts/pakiety-w-R.html#automatyczne-raportowanie",
    "href": "posts/pakiety-w-R.html#automatyczne-raportowanie",
    "title": "Tworzenie pakietów w R",
    "section": "11.2 Automatyczne raportowanie",
    "text": "11.2 Automatyczne raportowanie\nIstnieje cała seria narzędzi do automatycznego raportowania code coverage. Bardzo często repozytoria chwalą się wielkością tego wskaźnika na odznakach. Jeśli pakiet jest dobrze testowany, to może znaczyć, że jest porządnie zrobiony albo chociaż z należytą dbałością.\nNarzędziem, które tutaj omówię i które ma dobrą integrację z R, jest Codecov. Żeby podłączyć swoje repozytorium do Codecov musimy założyć odpowiednie konto i uruchomić dwie komendy. Jeśli chodzi o konto, wchodzimy na codecov.io, zakładamy konto i upoważniamy Codecov do dostępu do naszego konta GitHub. Naszym oczom powinna ukazać się lista wszystkich naszych repozytorów, do której wkrótce wrócimy. Komendy natomiast wymieniam niżej.\n\nusethis::use_coverage(\"codecov\")\nusethis::use_github_action(\"test-coverage\")\n\nPrzygotuje to nasze repozytorium, ale nie w sposób idealny. W momencie, w którym to piszę, trzeba wprowadzić serię poprawek i dodatkowych czynności, żeby wszystko szło, jak powinno.\n\nNa stronie codecov.io wchodzimy w nasze repozytorium i kopiujemy token, który się tam wyświetla. Musimy go wskazać w naszym repozytorium jako sekret. Robimy to w ustawieniach repozytorium, ale szczęśliwie Codecov podpowiada nam link bezpośrednio do ustawień. Nazwa sekretu to CODECOV_TOKEN, a jego wartość to sam ten kod, który skopiowaliśmy.\nJeśli chcemy, możemy upoważnić apkę Codecov na GitHubie. Zamieszcza ona automatyczne raporty o code coverage przy pull requestach.\nPlik .github/workflows/test-coverage.yaml modyfikujemy w taki sposób, żeby góra tego pliku wyglądała tak, jak niżej. Nie usuwamy innych zmiennych środowiskowych, tylko dopisujemy CODECOV_TOKEN.\n\njobs:\n  test-coverage:\n    runs-on: ubuntu-latest\n    env:\n      CODECOV_TOKEN: ${{ secrets.CODECOV_TOKEN }}\n\nJeśli repozytorium jest prywatne, na końcu zmieniamy naszą formułę tak, żeby wyglądała jak niżej.\n\n- name: Upload coverage reports to Codecov\n  uses: codecov/codecov-action@v3\n  with:\n    token: ${{ secrets.CODECOV_TOKEN }}\n\nOdznaka dodawana przez usethis nie działa. Na codecov.io, po wejściu w nasze repozytorium (już po jego uruchomieniu), w ustawieniach, w sekcji „Badges & Graphs” znajdziemy poprawny link, którym zastępujemy ten dodany przez usethis w README.Rmd.\n\nZestaw określony wyżej może się dość szybko zdezaktualizować, dotarłem do niego na zasadzie prób i błędów. Najbardziej newralgiczne są punkty 3. i 4., które mają za zadanie dać Codecov znać, jaki jest token, który dodaliśmy wcześniej do sekretów repozytorium. Po wykonaniu tych czynności raporty code coverage staną się jawne, będziemy mogli się nimi chwalić na głównej stronie naszego repozytorium, a także będą wykonywane automatycznie przez GitHub za każdym razem, gdy wypchniemy nowy commit."
  },
  {
    "objectID": "posts/pakiety-w-R.html#zakładanie-repozytorium",
    "href": "posts/pakiety-w-R.html#zakładanie-repozytorium",
    "title": "Tworzenie pakietów w R",
    "section": "13.1 Zakładanie repozytorium",
    "text": "13.1 Zakładanie repozytorium\n\nroxygen2::roxygenise() # dodaj roxygen2, najpeirw usuń NAMESPACE\nusethis::use_roxygen_md() # obsługa markdown w roxygen2\nusethis::use_gpl3_license() # licencja GPL v. 3\nusethis::use_package(\"R\", type = \"Depends\", min_version = \"4.2.0\") # dodaj zależność od R\nusethis::use_tidy_dependencies() # podstawowe zależności tidyverse\nusethis::use_tidy_eval() # narzędzia do Tidy Eval\nusethis::use_tibble() # dodaj tibble\nusethis::use_pipe() # dodaj potoki z magrittr %&gt;%\nusethis::use_lifecycle() # dodaj oznaczniki lifecycle\nusethis::use_readme_rmd() # dodaj README.Rmd"
  },
  {
    "objectID": "posts/pakiety-w-R.html#podłączanie-githuba",
    "href": "posts/pakiety-w-R.html#podłączanie-githuba",
    "title": "Tworzenie pakietów w R",
    "section": "13.2 Podłączanie GitHuba",
    "text": "13.2 Podłączanie GitHuba\n\nusethis::use_git() # popraw/załóż pliki repozytorium git\nusethis::use_github(private = TRUE) # załóż repozytorium prywatne na GitHubie\nusethis::use_github_action_check_standard() # dodaj automatyczne R-CMD-check"
  },
  {
    "objectID": "posts/pakiety-w-R.html#mniejsze-opcje",
    "href": "posts/pakiety-w-R.html#mniejsze-opcje",
    "title": "Tworzenie pakietów w R",
    "section": "13.3 Mniejsze opcje",
    "text": "13.3 Mniejsze opcje\n\nusethis::use_package_doc() # dokumentacja całego pakietu\nusethis::use_cran_badge() # odznaka CRAN\nusethis::use_lifecycle_badge(\"experimental\") # odznaka lifecycle experimental\nusethis::use_citation() # dodaj plik CITATION"
  },
  {
    "objectID": "posts/pakiety-w-R.html#testy",
    "href": "posts/pakiety-w-R.html#testy",
    "title": "Tworzenie pakietów w R",
    "section": "13.4 Testy",
    "text": "13.4 Testy\n\ndevtools::load_all() # załaduj pakiet, żeby testować ręcznie\nusethat::use_testthat() # dodaj testthat\nusethis::use_coverage(\"codecov\") # dodaj codecov\nusethis::use_github_action(\"test-coverage\") # dodaj automatyczne raporty codecov"
  },
  {
    "objectID": "posts/pakiety-w-R.html#używanie-na-co-dzień",
    "href": "posts/pakiety-w-R.html#używanie-na-co-dzień",
    "title": "Tworzenie pakietów w R",
    "section": "13.5 Używanie na co dzień",
    "text": "13.5 Używanie na co dzień\n\ndevtools::load_all() # załaduj pakiet, żeby testować ręcznie\nusethis::use_r(\"nazwa_funkcji\") # stwórz plik z funkcjami\nusethis::use_test() # dodaj plik z testami\nusethis::use_data_raw(\"nazwa bazy\") # stwórz plik z danymi"
  },
  {
    "objectID": "posts/pakiety-w-R.html#konserwacja",
    "href": "posts/pakiety-w-R.html#konserwacja",
    "title": "Tworzenie pakietów w R",
    "section": "13.6 Konserwacja",
    "text": "13.6 Konserwacja\n\ndevtools::document() # zaktualizuj dokumentację\ndevtools::load_all() # załaduj pakiet\ndevtools::check() # zrób R-CMD-check\ndevtools::test_coverage() # sprawdź code coverage\ndevtools::build_readme() # zaktualizuj README.md\nstyler:::style_active_pkg() # formatuj wszystkie pliki w pakiecie"
  },
  {
    "objectID": "posts/pakiety-w-R.html#błędy",
    "href": "posts/pakiety-w-R.html#błędy",
    "title": "Tworzenie pakietów w R",
    "section": "13.7 Błędy",
    "text": "13.7 Błędy\n\ncli::cli_abort(\"Treść błędu\") # błąd\ncli::cli_warn(\"Treść ostrzeżenia\") # ostrzeżenie\ncli::cli_inform(\"Treść infromacji\") # informacja"
  },
  {
    "objectID": "posts/pakiety-w-R.html#przed-każdym-wypchnięciem",
    "href": "posts/pakiety-w-R.html#przed-każdym-wypchnięciem",
    "title": "Tworzenie pakietów w R",
    "section": "13.8 Przed każdym wypchnięciem",
    "text": "13.8 Przed każdym wypchnięciem\n\ndevtools::load_all() # załaduj pakiet, żeby testować ręcznie\ndevtools::check() # zrób R-CMD-check\ndevtools::test_coverage() # sprawdź code coverage\ndevtools::build_readme() # zaktualizuj README.md"
  },
  {
    "objectID": "posts/pakiety-w-R.html#inne-rzeczy-do-wykonania",
    "href": "posts/pakiety-w-R.html#inne-rzeczy-do-wykonania",
    "title": "Tworzenie pakietów w R",
    "section": "13.9 Inne rzeczy do wykonania",
    "text": "13.9 Inne rzeczy do wykonania\n\nUzupełnij ręcznie plik DESCRIPTION. Pamiętaj o tym, by autorów wskazywać funkcją.\nUzupełnij plik CITATION, jeśli z niego korzystasz.\nNapisz porządny plik README.\nPamiętaj o pisaniu pełnych nazw funkcji i używaniu .data do nazw kolumn bez cudzysłowu.\nKażdą funkcję dokumentuj szkieletem roxygen2. Nie zapomnij o tagu @export.\nJeśli dodajesz pliki z danymi, pamiętaj, żeby je udokumentować w pliku data.R. Opisz wszystkie kolumny za pomocą @format.\nJeśli masz czas i chęć, zabezpieczaj funkcje przed nieprawidłowym wykorzystaniem. Przede wszystkim sprawdzaj typy argumentów oraz czy kolumny istnieją w bazie. Pamiętaj, że możesz formatować komunikaty o błędach."
  },
  {
    "objectID": "posts/podstawy_R.html",
    "href": "posts/podstawy_R.html",
    "title": "Podstawy programowania w R",
    "section": "",
    "text": "W kilku miejscach wrzucam informacje, jak to, co omawiamy teoretycznie, można zrobić w R. Absolutnie nie jest to konieczne do zrozumienia statystyki! Jest to tylko jedna z możliwości, jak można opisaną dalej teorię przekuć w praktykę. O R można myśleć jako o programie do robienia statystyki, podobnie jak SPSS, Statistica, Stata czy (oparte na R, darmowe i otwartoźródłowe) jamovi. Jeśli jednak Czytelnik widział kiedyś program statystyczny, spodziewać się będzie ekranu podobnego do Excela, gdzie na górnej belce wybiera się testy statystyczne, jakie chce się przeprowadzić. Praca w R tak nie wygląda. Największa wada i zaleta R polega na tym, że jest on jednocześnie językiem programowania. A to daje bardzo ciekawe możliwości, o których niżej. Także praca w R wygląda tak, że w specjalnym języku piszemy komputerowi, co ma zrobić, potem uruchamiamy te instrukcje i gotowe.\nW tym miejscu spróbuję opisać, jak zacząć pracę z R. Nie mam ambicji zrobić pełnego wprowadzenia, bo wyszedłby z tego osobny podręcznik. Mam ambicje dać jakikolwiek fundament, który pozwoli Czytelnikowi wyczyścić dane i zrobić podstawowe testy. Jeśli po przeczytaniu tego tekstu ktoś stwierdzi „Może warto się zagłębić”, to znajdzie mnóstwo materiałów, które mu na to pozwolą. Ze swojej strony mogę polecić podręcznik „Język R. Kompletny zestaw narzędzi dla analityków danych” Wickhama i Grolemunda (2020), interaktywne kursy na DataCamp, a pomocy w rozwiązaniu konkretnych problemów zawsze można szukać na StackOverflow."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-console",
    "href": "posts/podstawy_R.html#sec-console",
    "title": "Podstawy programowania w R",
    "section": "3.1 Konsola, zmienne i matematyka",
    "text": "3.1 Konsola, zmienne i matematyka\nEkran RStudio składa się z trzech okienek. Duże okienko po lewej i dwa mniejsze po prawej. Skierujmy naszą uwagę na okienko po lewej, czyli konsolę. Wita nas ona ciepłą informacją, że R jest zainstalowany. Znakiem zachęty &gt; zachęca nas do wydawania jej poleceń. Konsola R to miejsce, w którym możemy mówić R, żeby coś dla nas liczył. Można to potraktować jako super-kalkulator. Spróbuj – wpisz w konsolę 2+3*5, zatwierdź enterem i zwróć uwagę, że R stosuje poprawną kolejność wykonywania działań. Spacje nie mają znaczenia, także możemy wpisać również bardziej estetyczną wersję 2 + 3 * 5. Nie wiem po co, skoro to tylko obliczenie na szybko, ale można.\nWynik takiego działania nigdzie się nie zapisuje, tylko wyświetla się w konsoli. Jeśli chcemy zapisać nasz wynik, możemy to zrobić stosując znaczek &lt;-6. Przydatnym skrótem jest tu w RStudio jest Alt+-, który od razu wstawia nam tę strzałeczkę. Wyjaśnijmy to na przykładzie.\n\nwynik &lt;- 2 + 3 * 5\n\na &lt;- wynik * 3^3\n\nPowyżej zapisałem dwa polecenia, które do konsoli powinniśmy wpisać jedno po drugim i każde z nich zatwierdzić enterem. Pierwsze polecenie mówi konsoli – policz 2 + 3 * 5 i zapisz to w zmiennej wynik. Po zatwierdzeniu tego polecenia możemy zauważyć, że wynik działania nam się nie wyświetlił. Za to w prawym górnym okienku pojawiło się słowo wynik i obok wartość 17. Od tego momentu możemy używać słowa wynik zamiast liczby 17. Spróbuj wpisać w konsolę samo słowo wynik i zatwierdzić enterem. Wyskoczy 17. Jeśli teraz wpiszesz np. wynik * 2, to konsola zwróci to samo, co zwróciłaby po wpisaniu 17 * 2, czyli 34. Co więc robi drugie polecenie z przyładu? Możemy je odczytać jako „W zmiennej o nazwie a zapisz (a &lt;-) wynik mnożenia zmiennej wynik i 3 do potęgi 3 (wynik * 3^3)“. Operator ^ oznacza potęgowanie. Jeśli potem wpiszemy w konsolę a, naszym oczom ukaże się 459, czyli \\(17 \\times 3^3\\).\nJak się potem okaże, w zmiennych możemy zapisywać dużo więcej, niż tylko wyniki prostych działań matematycznych. W identyczny sposób do odpowiednich zmiennych trafią wyniki testów statystycznych albo całe bazy danych. Ale o tym dalej."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-functions",
    "href": "posts/podstawy_R.html#sec-functions",
    "title": "Podstawy programowania w R",
    "section": "3.2 Funkcje",
    "text": "3.2 Funkcje\nTym, co robi robotę w R (jak i w każdym innym języku programowania) są funkcje. Funkcje to maszyny, do których wrzucamy jakiś obiekt (np. liczbę), funkcja nam to przekształca i wyrzuca z siebie coś innego. Tak samo jak funkcje w matematyce. Skojarzenie jest słuszne, bo funkcje w programowaniu zapisujemy konwencją f(x), czyli nazwa(co_wrzucam_do_funkcji). Dla przykładu funkcja o nazwie seq pozwala nam wytwarzać regularne sekwencje liczb, np. od 1 do 10 co 2. Musimy do tej funkcji wrzucić (1) od jakiej liczby chcemy zacząć, (2) na jakiej chcemy skończyć i (3) jaki chcemy mieć krok. Dla przykładu:\n\nseq(5, 62, 3)\n\n#&gt;  [1]  5  8 11 14 17 20 23 26 29 32 35 38 41 44 47 50 53 56 59 62\n\n\n\nTo, co wrzucamy do funkcji, nazywamy argumentami. Tutaj były nimi liczby. Funkcja seq wie, że ma zacząć od 5 i skończyć na 62, a nie zacząć od 62 i skończyć na 5, bo ma pod maską zapisane, w jakiej kolejności będzie dostawać te liczby. Takie argumenty nazywamy pozycyjnymi – funkcja wie, co to jest i co ma z tym zrobić, na podstawie pozycji. W R każdy argument możemy też jednak nazwać. Dla przykładu wiemy, że argumenty funkcji seq nazywają się from, to i by. Możemy więc wprost powiedzieć funkcji, że oto dajemy jej from, to i by.\n\nseq(from = 5, to = 62, by = 3)\n\n#&gt;  [1]  5  8 11 14 17 20 23 26 29 32 35 38 41 44 47 50 53 56 59 62\n\n\n\nseq(to = 62, by = 3, from = 5) # jeśli nazywamy argumenty, kolejność nie ma znaczenia\n\n#&gt;  [1]  5  8 11 14 17 20 23 26 29 32 35 38 41 44 47 50 53 56 59 62\n\n\n\nTego typu argumenty nazywamy kluczowymi (keyword) albo nazwanymi (named). W praktyce wykorzystuje się mieszankę jednego i drugiego typu argumentów. Nazywanie argumentów zwiększa czytelność kodu, ale czasem pozycja jest wystarczająco jasna. Dla przykładu mogę napisać sqrt(x = 9), żeby wyciągnąć pierwiastek kwadratowy (square root) z 9, ale czy zapis sqrt(9) jest jakkolwiek mniej jasny?\nCzasami też używamy argumentów kluczowych, żeby zmienić jakieś ustawienia domyślne albo odblokować nowe możliwości. Dla przykładu funkcja seq dysponuje dodatkowym argumentem length.out. Jeśli ustawimy length.out, możemy ustalić liczbę elementów w naszym wyniku zamiast punktu końcowego albo kroku.\n\nseq(5, by = 3, length.out = 10) # daj mi 10 kolejnych liczb zaczynając od 5 i co 3\n\n#&gt;  [1]  5  8 11 14 17 20 23 26 29 32\n\n\n\nseq(1, 100, length.out = 10) # podaj 10 liczb między 1 a 100\n\n#&gt;  [1]   1  12  23  34  45  56  67  78  89 100"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-docs",
    "href": "posts/podstawy_R.html#sec-docs",
    "title": "Podstawy programowania w R",
    "section": "3.3 Dokumentacja",
    "text": "3.3 Dokumentacja\nRóżne funkcje przyjmują różne argumenty. Podobnie jak nie powiemy piekarzowi, żeby stosował białą fugę do naszego chleba, tak samo do funkcji seq nie wrzucimy słów zamiast liczb. Tak jak musimy wiedzieć, że piekarz zajmuje się pieczywem, tak samo musimy znać funkcje, których używamy. W poprzednim podrozdziale wiedzieliśmy, co można wrzucić do funkcji seq i jak nazywają się jej argumenty, bo to napisałem. Tak samo podałem ot tak, że argument funkcji sqrt nazywa się x. Skąd mam to jednak wiedzieć?\nNie bez powodu mówimy o językach programowania. Wiele funkcji nauczymy się na pamięć i będziemy po prostu wiedzieć, jak z nich korzystać. Jednak znacznie częściej (w wielu przypadkach też dla funkcji, które znamy) będziemy korzystać z dokumentacji. R dysponuje świetną dokumentacją dla każdej funkcji7. Zawiera ona opis, co dana funkcja robi, jakie argumenty przyjmuje, a często nawet tło teoretyczne jej działania. Żeby dostać się do dokumentacji danej funkcji, wywołujemy ją w konsoli ze znakiem zapytania, np. ?seq. Powoduje to, że w okienku Help po prawej wyświetla nam się pełna dokumentacja tej funkcji. Nie trzeba więc sięgać do Google, żeby uzyskać odpowiedź na podstawowe problemy. O ile wiemy, jakiej funkcji chcemy użyć. Także zachęcam do częstego sięgania do dokumentacji. To absolutnie podstawowe narzędzie w programowaniu czegokolwiek.\nŚwietnym źródłem informacji o funkcjach, pozwalającym również znaleźć odpowiednią funkcję do realizacji naszego celu, są ściągi (cheat sheets). Pakiety tidyverse mają nawet swoje oficjalne ściągi, które na początku swojej nauki R wydrukowałem i zalaminowałem. Polecam je gorąco, zwłaszcza do pakietów dplyr, ggplot2 i stringr. Można je znaleźć bezpośrednio w RStudio wybierając Help → Cheat sheets → Browse all cheat sheets albo na stronie Posit, czyli firmy, która wypuszcza RStudio. Jak przejdziemy dalej, do części praktycznej, polecam, żeby mieć te ściągi już przygotowane, wydrukowane lub w .pdf."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-scripts",
    "href": "posts/podstawy_R.html#sec-scripts",
    "title": "Podstawy programowania w R",
    "section": "3.4 Skrypty",
    "text": "3.4 Skrypty\nWpisaliśmy w konsolę już sporo rzeczy. Historię naszych komend możemy zobaczyć przechodząc do odpowiedniej zakładki w prawym górnym okienku. Jednak wyjście z programu może nam skutecznie skasować tę historię. Jeśli mamy całą sporą analizę statystyczną, która składa się z 200 linijek kodu, to chcielibyśmy mieć jakiś sposób na zapisanie tego na przyszłość, żeby nie musieć za każdym razem wklepywać tego kodu z pamięci. Zaopatrujemy się więc w gruby zeszyt w linie i wszystkie komendy piszemy również tam. Żarcik. Do przechowywania kodu służą specjalne pliki zwane skryptami. Tak jak mamy pliki .pdf, .txt, .docx, tak w plikach .R zapisujemy kod R.\nNajprościej stworzyć nowy skrypt klikając w biały kwadracik z plusem w lewym górnym rogu RStudio. Spowoduje to otworzenie listy rzeczy, które możemy wytworzyć. Nas w tej chwili interesuje R Script. Gdy utworzymy nasz skrypt, otworzy się on nad konsolą. Warto od razu zapisać go na dysku skrótem Ctrl+S (lub File → Save). Warto się upewnić, że zapisywany plik rzeczywiście kończy się rozszerzeniem .R.\nNa razie nasz skrypt jest pusty, ale możemy w nim pisać dowolne polecenia tak samo, jak napisalibyśmy w konsoli. Różnica jest taka, że nie są one od razu wykonywane. Skrypt to tekst. Jeśli chcemy wykonać jakieś polecenie ze skryptu, to albo kopiujemy je do konsoli, albo umieszczamy na nim kursor i klikamy Ctrl+Enter. Możemy też myszką zaznaczyć większy fragment kodu i kliknąć Ctrl+Enter. Jeśli chcielibyśmy wykonać cały nasz skrypt, to zaznaczamy cały kod (Ctrl+A) i ponownie używamy Ctrl+Enter. Ewentualnie możemy skorzystać ze skrótu Ctrl+Shift+S8.\nTo jest najważniejsza różnica między skryptem a konsolą – cokolwiek wpisane w konsolę jest wykonywane natychmiast i znika. Z konsoli korzystamy, kiedy chcemy zrobić jakieś jednorazowe operacje albo coś sobie przetestować. W skrypt wpisujemy to, co chcemy zachować. Ewentualnie szkic, który potem będziemy naprawiać. Gdy ludzie przechodzą nagle z konsoli do skryptu, bardzo często zaczynają wpisywać w swój skrypt różne śmieci, które wcześniej wpisaliby w konsolę. Konsola nie zniknęła, ciągle jest do naszej dyspozycji. Skrypt w swojej ostatecznej postaci powinien jednak działać tak, że jak go uruchomimy, to cały przeleci bez błędów. No, przynajmniej do tego dążymy. Czyli konsola do testów, skrypt do prawdziwego kodu."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-comments",
    "href": "posts/podstawy_R.html#sec-comments",
    "title": "Podstawy programowania w R",
    "section": "3.5 Komentarze",
    "text": "3.5 Komentarze\nJeśli chcielibyśmy zrobić w skrypcie jakąś notatkę, która nie jest kodem, używamy znaczka #. Jest to tzw. komentarz. Możemy na przykład napisać:\n\nprint(\"Hello world!\") # czuję się programistą\n\nJeśli wykonamy taką linijkę, konsola zignoruje wszystko po znaku #. Pozwala to nam zostawiać sobie notatki w rodzaju # hipoteza 1 albo # nie wiem, czemu to działa, ale działa. Komentowanie kodu może nam (i naszym współpracownikom) ułatwić zrozumienie, o co nam chodziło, gdy to pisaliśmy.\nJeśli chcemy zaopatrzyć nasz kod w nagłówki, żeby podzielić go na sekcje, konwencja mówi, żeby formatować je tak:\n\n# Przygotowanie ----\n\n## Ładowanie danych ----\n\n# kod ładujący dane\n\n## Ładowanie bibliotek ----\n\n# kod ładujący biblioteki\n\nKażdy znaczek # to niższy poziom nagłówka, czyli wytworzyłem sekcję Przygotowanie, a w niej dwie podsekcje Ładowanie danych i Ładowanie bibliotek. Takich poziomów nagłówków możemy mieć, ile chcemy. Nagłówek tym się różni od zwykłego komentarza, że zapisujemy po nim cztery myślniki ---- lub inne znaki. Tak sformatowane nagłówki wyświetlają się w bocznym panelu outline w RStudio i pozwalają się lepiej ogarnąć i poruszać w długim kodzie. Panel outline możemy rozwinąć skrótem Ctrl+Shift+O albo klikając skrajną prawą ikonkę nad edytorem skryptu (poziome kreski na prawo od guzika Source)."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-projects",
    "href": "posts/podstawy_R.html#sec-projects",
    "title": "Podstawy programowania w R",
    "section": "3.6 Projekty",
    "text": "3.6 Projekty\nZazwyczaj projekt badawczy składa się z wielu plików. Nie jest to tylko kod R, ale też chociażby pliki z danymi, instrukcje do metod badawczych itd. Zazwyczaj trzymamy to wszystko w jednym folderze. O ile utrzymujemy jakikolwiek porządek w plikach. Możemy też mieć całe studia luzem na pulpicie, nie oceniam. RStudio pomaga nam w zarządzaniu takimi grupami plików poprzez projekty. Projekty w RStudio robią kilka rzeczy, m.in. pozwalają ustawić niestandardowe opcje (np. zmienić język słownika na angielski tylko dla tego jednego projektu), zapamiętać otwarte okna i ich układ, ale przede wszystkim pomagają nam lokalizować pliki znajdujące się w tym samym folderze9. Zawsze, kiedy planujemy zachować jakiś zbiór powiązanych plików na dłużej, warto jest wytworzyć projekt.\nProjekty tworzymy i otwieramy przez guzik w prawym górnym rogu. Rozwijane menu pozwala nam stworzyć nowy projekt, a wyskakujące okienko pyta, czy wytworzyć go w już istniejącym folderze, stworzyć nowy folder, czy może pobrać repozytorium Git. Jeśli wybraliśmy nowy folder, mamy kilka typów projektów do wyboru, ale w większości przypadków wybieramy po prostu New Project. Okienko pozwala nam nadać projektowi nazwę, wybrać jego lokalizację, a także wytworzyć puste repozytorium Git10. RStudio wytworzy nam w ten sposób plik .Rproj organizujący nasz projekt."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-libs",
    "href": "posts/podstawy_R.html#sec-libs",
    "title": "Podstawy programowania w R",
    "section": "3.7 Pakiety",
    "text": "3.7 Pakiety\nPakiety (packages lub libraries) to niejako dodatki do R, które rozszerzają jego możliwości. Dla przykładu – R w swojej podstawowej wersji nie ma funkcji liczącej skośność. Nie jest to jednak żaden problem, bo możemy R rozszerzyć np. o pakiet o nazwie e1071 albo moments. Oba te pakiety dodają nam do R możliwość szybkiego i prostego policzenia skośności. Pakiety – w olbrzymiej większości – są darmowe.\nAbsolutnie podstawowym pakietem, czy właściwie zbiorem pakietów, jest tidyverse. tidyverse usprawnia R właściwie we wszystkim, co w podstawowej wersji jest niewygodne – readr (czyt. rider) pozwala łatwo ładować dane, dplyr (czyt. diplajer) niesamowicie usprawnia czyszczenie danych, lubridate i stringr (czyt. stringer) to podstawowe narzędzie do pracy odpowiednio z datami i z tekstem11, nie mówiąc już o ggplot2, czyli najpotężniejszym narzędziu do tworzenia wykresów. Współcześnie tidyverse to podstawowy sposób programowania w R. Na szczęście nie musimy wszystkich tych pakietów przywoływać z osobna, bo możemy załadować je wszystkie naraz, ładując jeden zbiorczy pakiet tidyverse. Pakiety ładujemy za pomocą funkcji library, do której wrzucamy nazwę pakietu w cudzysłowie. Nasz skrypt zaczniemy więc od takiej instrukcji:\n\nlibrary(\"tidyverse\")\n\nJeśli robimy to po raz pierwszy, to po wykonaniu polecenia konsola wyrzuci nam błąd Błąd w poleceniu 'library(\"tidyverse\")':nie ma pakietu o nazwie ‘tidyverse’. Wynika to z faktu, że instrukcja library tylko ładuje pakiet, ale nie pobiera ich wcześniej. Na szczęście robimy to tylko raz. Zawsze później wystarczy samo library. Dlatego też nie będziemy wpisywać komendy instalującej pakiet do skryptu, tylko bezpośrednio do konsoli. Nie chcemy w końcu, żeby pakiet tidyverse instalował się za każdym razem, kiedy będziemy uruchamiać skrypt. Będzie to niemiłosiernie spowalniało skrypt i wymuszało dostęp do Internetu. Dlatego też do konsoli wpisujemy:\n\ninstall.packages(\"tidyverse\")\n\nInnym sposobem instalowania pakietów jest skierowanie się w prawe dolne okienko w RStudio, przejście do zakładki Packages, kliknięcie guzika Install, wpisanie nazwy pakietu w wyskakującym okienku (już bez cudzysłowu) i zatwierdzenie guzikiem Install.\nGdy zainstalujemy już pakiet tidyverse – dowolną z metod – ponownie próbujemy go załadować, tym razem już bez błędu. Konsola poinformuje nas wtedy co dokładnie załadowała.\n\nlibrary(\"tidyverse\")\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.2     ✔ purrr     1.0.1\n#&gt; ✔ forcats   1.0.0     ✔ readr     2.1.4\n#&gt; ✔ ggplot2   3.4.2     ✔ stringr   1.5.0\n#&gt; ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-choosing",
    "href": "posts/podstawy_R.html#sec-choosing",
    "title": "Podstawy programowania w R",
    "section": "5.1 Wybieranie kolumn i wierszy",
    "text": "5.1 Wybieranie kolumn i wierszy\nBardzo często będziemy potrzebowali tylko określonych kolumn albo tylko określonych wierszy. Przeglądając nasze dane zauważamy, że składają się w dużej części z niepotrzebnych kolumn, jakie wygenerował dla nas program do ankiet. Kolumny takie jak godziny wypełniania są nam niepotrzebne do analizy. Co więcej nasze dane to fragment bazy danych z badania, w którym mówiliśmy mężczyznom, że są mało męscy. Potem patrzyliśmy, jak to wpłynie na nich homofobię. Badaliśmy więc wyłącznie mężczyzn, a mimo to ankietę próbowało też wypełnić kilka kobiet i osób o innej płci. Ponieważ ankieta nie dopuściła ich nawet do metryczki, widzimy w ich przypadkach wartości NA, co w R oznacza „brak danych”.\n\n5.1.1 Filtrowanie wierszy z dplyr::filter\nZacznijmy od tego, że w naszej bazie zostawimy tylko mężczyzn. Wszystkie komendy poniżej wpisuję w konsoli, dla testów. Jeśli wpisujemy komendy modyfikujące dane i nie dodamy specjalnej instrukcji zapisującej, to nie zapisujemy zmian, więc możemy bezpiecznie sprawdzić, co się stanie, jak rzeczywiście to zrobimy. Dopiero na koniec podam, jak nasze zmiany rzeczywiście zapisać. Zachęcam do tego, żeby kolejne kroki pisać samodzielnie (może być w skrypcie) i potem wykonywać, żeby widzieć wszystkie pośrednie kroki. Jak więc odfiltrować nie-mężczyzn? Robimy to za pomocą komendy filter()12. tidyverse opiera się o intuicyjnie brzmiące czasowniki takie jak filter, select, group_by, summarise itd. Komenda filter przyjmuje naszą bazę i jakieś warunki, np. płeć męska. W naszym wypadku będzie to wyglądać tak:\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\")\n\nDobra, co to jest %&gt;%? Nie było o tym mowy. Owszem, nie było, ale to bardzo wygodna rzecz. Nazywa się pipe (tłumaczone jako potok, bo rura nieładnie brzmi). Mówi mniej więcej „wrzuć to do tego”. W naszym przykładzie df %&gt;% filter() oznacza „wrzuć bazę danych df do funkcji filter“, czyli dokładnie to samo, co filter(df). Po co więc w ogóle bawić się w potoki? Bo pozwalają nam wygodnie łączyć komendy w ciągi, jak zobaczymy za chwilę. Do wstawiania potoków służy nam wygodny skrót klawiszowy Ctrl+Shift+M, który jest chyba najczęściej stosowanym skrótem przy pisaniu dowolnego programu.\nDruga kwestia to podział na linijki. Rozbiłem tę komendę na dwie linijki dla czytelności, ale spokojnie mógłbym zapisać to w jednej linijce. Warto jednak pisać kod tak, żeby dało się go potem łatwo czytać. RStudio podpowiada nam też wcięcia, żebyśmy widzieli, że te linijki tworzą jedną całość. W podrozdziale 9 powiem więcej o formatowaniu kodu.\nTa komenda oznacza „weź zmienną df, wrzuć ją do komendy filter i zostaw tylko te przypadki, w których w kolumnie Płeć jest wartość \"Mężczyzna\".” Nazwy kolumn piszemy bez cudzysłowu, ale jeśli wartość komórki to tekst, to zawsze piszemy go w cudzysłowie. Inaczej R pomyśli, że podajemy mu jakąś zmienną, z której ma dopiero odczytać, co ma być w kolumnie Płeć. Nam chodzi o dosłowny tekst \"Mężczyzna\".\nOstatecznie zostaje operator logiczny. Dlaczego piszę == zamiast =? W programowaniu znak = służy do przypisywania wartości do zmiennych. Zapis a = 5 oznacza „niech a ma wartość 5”. Sprawdzenie czy a ma wartość 5 odbywa się poprzez komendę a == 5. Konsola wyrzuci nam wtedy TRUE, FALSE albo BŁĄD: nie znaleziono obiektu 'a'. Kilka innych operatorów logicznych prezentuje tabela.\n\nCzęść operatorów logicznych dostępnych w R.\n\n\n\n\n\n\n\nOperator\nZnaczenie\nPrzykład\n\n\n\n\n==\nrówna się\nPłeć == \"Mężczyzna\"\n\n\n!=\nnie równa się\nPłeć != \"Kobieta\"\n\n\n&gt; (&gt;=)\nwiększe niż(większe lub równe)\nWiek &gt; 40\n\n\n&lt; (&lt;=)\nmniejsze niż(mniejsze lub równe)\nWiek &lt; 40\n\n\n|\nlub\nWiek &lt; 18 | Wiek &gt; 60\n\n\n&\ni\nPłeć == \"Mężczyzna\" & Wiek &gt; 40\n\n\n%in%\nzawiera się w zbiorze\nPłeć %in% c(\"Kobieta\", \"Inna\")\n\n\n!\nzaprzeczenie\n! Płeć %in% c(\"Kobieta\", \"Inna\")\n\n\n\n\n\n5.1.2 Wybieranie kolumn z dplyr::select\nOdfiltrowaliśmy więc nie-mężczyzn. Kolejny problem to cała seria niepotrzebnych kolumn. Godziny, adres, zgoda etyczna (która była obowiązkowa, więc cała zawiera identyczne wartości) i płeć (już jednakowa dla wszystkich) są nam do niczego niepotrzebne. Do wybierania, jakie kolumny zostawić, służy funkcja select(). Wrzucamy do niej nazwy albo numery kolumn, które chcemy zostawić w bazie. Rozszerzmy więc naszą poprzednią instrukcję o dodatkową komendę za pomocą potoku.\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18)\n\nPo pierwsze zauważmy, że wystarczyło dodać potok i kolejną komendę. Teraz cała nasza instrukcja oznacza „Weź df, odfiltruj mężczyzn i potem wybierz kolumny Id, Wiek (ukończony w latach), Wykształcenie oraz kolumny od 9. do 18.”. Do tego więc służą potoki – pozwalają naraz wykonać całą serię modyfikacji tego samego obiektu.\nWypada tu wyjaśnić dwie sprawy. Po pierwsze kolumna z wiekiem zawiera w nazwie spacje. Jeśli nazwa kolumny zawiera niestandardowe znaki, trzeba ją otoczyć znakami ` (pol. grawis, ang. backtick), który znajduje się na klawiaturze tuż pod Esc. Druga rzecz to 9:18, co znaczy „liczby od 9 do 18” i jest wygodnym, skrótowym zapisem seq(9, 18).\nEwentualnie możemy chcieć powiedzieć, żeby zostawić wszystkie kolumny poza jakąś kolumną. Jeśli chcemy wykluczyć 2 kolumny z 200, to lepiej wskazać te 2 do wywalenia niż pozostałe 198 do zachowania. Możemy to zrobić z użyciem znaku -, który wstawiamy przed kolumną. Możemy ustawić minus zarówno przed nazwą kolumny, jak i zakresem kolumn. Warto jednak zauważyć, że zakres pozycji trzeba wziąć w nawias. Inaczej zapis -2:5 R zinterpretuje jako „kolumny od -2 do 5”. Nie jest to głupie, bo „kolumna -2” oznacza „druga od końca”.\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(-(2:5), -`Wyrażam świadomą i dobrowolną zgodę na udział w badaniu.`)"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-rename",
    "href": "posts/podstawy_R.html#sec-rename",
    "title": "Podstawy programowania w R",
    "section": "5.2 Zmiana nazw kolumn z dplyr::rename i purrr::set_names",
    "text": "5.2 Zmiana nazw kolumn z dplyr::rename i purrr::set_names\nZ selekcji istniejących infromacji przejdźmy do dodawania nowych. Zacznijmy może od zmiany nazw kolumn, żeby łatwiej nam się pisało dalsze komendy. Do tego służą funkcje rename z pakietu dplyr i set_names z pakietu purrr13. rename służy do zmiany nazw raczej pojedynczych kolumn i przyjmuje argumenty w postaci rename(\"nowa_nazwa\" = \"stara nazwa\"). Za jednym zamachem możemy zmienić ile nazw chcemy, jeśli jednak chcemy zmienić wszystkie nazwy, wygodniejsza jest funkcja set_names. Wrzucamy do niej po kolei same nowe nazwy. Znowu – nazwy to dosłowne ciągi znaków14, więc zawsze piszemy je w cudzysłowie.\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\"))\n\nCo znowu namieszałem? Czemu znowu coś utrudniam? Cóż, żeby ułatwić. O ile kolejne nazwy \"id\", \"wiek\" i \"wyksztalcenie\" rozumieją się same przez się, to do czego służy tutaj funkcja paste? Jeśli zerkniemy w bazę danych, zauważymy, że kolejne 10 kolumn to to samo pytanie „Takie widoki w przestrzeni publicznej są normalne”. Odpowiedź na to pytanie (znajdujące się pod obrazkiem neutralnym lub przedstawiającym parę jednopłciową) traktowaliśmy jako wskaźnik homofobii. Jest bardzo częste, że czyszcząc dane z badania mamy serię odpowiedzi z jednego kwestionariusza. Zazwyczaj wszystkie te pytania nazywamy według jednej konwencji np. wszystkie odpowiedzi z kwestionariusza TIPI nazywamy TIPI_1, TIPI_2, TIPI_3 itd. Ale po co pisać te etykiety ręcznie, skoro możemy je wygenerować? Do tego służy funkcja paste. Jeśli widzimy jakąś funkcję zagnieżdżoną w innej funkcji, a nie wiemy, jaką funkcję tam spełnia, można spróbować samą tę wewnętrzną funkcję wpisać w konsolę i zobaczyć, co robi.\n\npaste(\"H\", 1:10, sep = \"_\")\n\n#&gt;  [1] \"H_1\"  \"H_2\"  \"H_3\"  \"H_4\"  \"H_5\"  \"H_6\"  \"H_7\"  \"H_8\"  \"H_9\"  \"H_10\"\n\n\n\nJak widzimy, paste wygenerowało nam 10 kolejnych etykiet łącząc \"H\" i liczby od 1 do 10. Argument sep = \"_\" mówi, żeby między kolejnymi kawałkami wstawiać podkreślnik. Do paste możemy wrzucić dowolną liczbę znaków do połączenia. Jeśli nie chcemy żadnego separatora, możemy ustawić sep = \"\", czyli pusty ciąg znaków w separatorze albo możemy użyć bliźniaczej funkcji paste0, która nie ma separatora. Użycie tej funkcji wewnątrz set_names jest równoważne temu, jakbym napisał te etykietki ręcznie. To jest siła programowania, że dosłowne dane możemy zastępować zmiennymi albo funkcjami, które nam te dane wygenerują. To pozwala nam na przykład zastosować to samo czyszczenie do 100 różnych plików. Wystarczy, że zamiast nazwy danego pliku wszędzie wrzucimy zmienną, a potem będziemy tylko wrzucać do tej zmiennej nazwy kolejnych plików.\nCo ciekawe, zmieniać nazwy możemy też za pomocą funkcji select, ale wyłącznie pojedynczo, nie można wtedy użyć czegoś w stylu 9:18. Składnia jest taka sama, jak rename, ale select zmienia też kolejność kolumn i skład wynikowej tabeli, więc trzeba uważać. Jednak do małych tabel, gdzie nie jest problemem wypisać wszystkie kolumny, można spokojnie za jednym zamachem kolumny wybrać i nazwać."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-values",
    "href": "posts/podstawy_R.html#sec-values",
    "title": "Podstawy programowania w R",
    "section": "5.3 Zmiana wartości komórek z dplyr::mutate i readr::parse_number",
    "text": "5.3 Zmiana wartości komórek z dplyr::mutate i readr::parse_number\nJak widzimy, odpowiedzi na pytania z homofobią zawierają nie tylko liczby, ale też tekst z wyjaśnieniem tych liczb na skali. My jednak chcemy zostawić same liczby, żeby móc na nich liczyć. Podobny problem mamy z kolumną z wiekiem, gdzie możemy zauważyć, że jest to kolumna tekstowa. Dziwne, w końcu wiek to (tylko) liczba. Przejrzenie danych pozwala stwierdzić, że respondent o id 50 w pytaniu o wiek wpisał „18 (2021)“. Nieważne, jak się będziemy przed tym bronić, co dopiszemy do pytania o wiek, jak bardzo wprost będziemy błagać, żeby wpisywać tam tylko liczby, zawsze znajdzie się ktoś, kto zrobi w nim elaborat. Ten jeden respondent sprawił, że cała ta kolumna została zinterpretowana jako kolumna tekstowa, a nie liczbowa. Odpowiedź na oba te problemy jest taka sama – mutate i parse_number.\nFunkcja mutate to ogólna funkcja, za pomocą której modyfikujemy kolumny albo dodajemy nowe. Będziemy ją wykorzystywać bardzo często, za każdym razem, gdy baza danych będzie wymagała poprawek lub dodatkowych obliczeń. Jej składnia wygląda następująco:\n\nzmienna_z_danymi %&gt;%\n    mutate(\n        kolumna_do_modyfikacji = jakas_funkcja(kolumna_do_modyfikacji),\n        nowa_kolumna = inna_funkcja(jak_stworzyc_nowa_kolumne)\n    )\n\nmutate służy do tworzenia nowych kolumn. Możemy ją jednak wykorzystywać do modyfikowania kolumn już istniejących, bo jeśli nowa kolumna ma taką samą nazwę, jak stara, to nowa zastępuje starą. W naszym przykładzie chcemy do kolumny wiek zastosować funkcję parse_number, która pozbywa się z komórek wszystkiego, poza pierwszą napotkaną liczbą15. Taka instrukcja będzie wyglądała następująco:\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek)\n    )\n\n\n5.3.1 Przekształcanie wielu kolumn jednocześnie z dplyr::across\nPo wykonaniu tej funkcji widzimy, że cała kolumna jest już numeryczna. To samo możemy zrobić dla pytań z homofobią. Moglibyśmy, oczywiście, zapisać H_1 = parse_number(H_1), H_2 = parse_number(H_2) itd., ale po co się męczyć? Na początku roku 2020 dostaliśmy cudowną funkcję pomocniczą across, która przydaje nam się w takich dokładnie wypadkach, gdy chcemy w taki sam sposób zmodyfikować więcej niż jedną kolumnę, bez zbędnego przepisywania tego samego. Jak jej używać?\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(starts_with(\"H_\"), parse_number)\n    )\n\nPierwszą rzeczą, którą ta funkcja przyjmuje, jest zestaw kolumn. Można je wskazać na różne sposoby, np. wypisać ich nazwy albo numery. Gdy używam zestaw, mam na myśli, że trzeba je wpakować w funkcję c() (od concatenate), np. c(1, 8, 10:16). Możemy też użyć jednej z cudownych funkcji pomocniczych z zestawu tidy-select. Tutaj akurat użyłem starts_with(\"H_\"), żeby powiedzieć across, że chodzi mi o wszystkie kolumny, których nazwy zaczynają się od H_. Te same funkcje możemy wykorzystywać w funkcji select przy wybieraniu kolumn (por. Sekcja 5.1.2). Kilka innych tego typu funkcji umieściłem w tabeli.\n\nFunkcje pomocnicze do select i across.\n\n\n\n\n\n\nFunkcja\nWybierz wszystkie kolumny…\n\n\n\n\nstarts_with()\nktórych nazwy zaczynają się od\n\n\nends_with()\nktórych nazwy kończą się na\n\n\ncontains()\nktórych nazwy zawierają w sobie\n\n\nmatches()\nktórych nazwy zawierają w sobiewyrażenie regularne16\n\n\n:\nzawierają się w zakresie(np. H_1:H_10)\n\n\nall_of()\nw których wszystkie wartościspełniają jakiś warunek\n\n\nany_of()\nw których jakakolwiekwartość spełnia warunek\n\n\neverything()\nw ogóle wszystkie kolumny\n\n\nwhere()\ngdzie spełniony jest inny warunek(np. where(is.numeric)))\n\n\n\nDrugim argumentem, jaki przyjmuje across, jest nazwa funkcji, którą chcemy zastosować. Co ważne, musi to być jej nazwa bez nawiasów. Jest to częsty błąd, sam wiele razy się łapałem na tym, że odruchowo dodawałem do funkcji nawiasy. Wynika to z subtelnej różnicy, polegającej na tym, że jeśli nie używamy nawiasów, podajemy across samą funkcję, obiekt, który ją zawiera, a jeśli damy nawiasy, to wrzucamy w ten sposób do across wynik działania tej funkcji. Spowodowałoby to, że w tym wypadku dostalibyśmy błąd, że funkcja parse_number() nie dostała wymaganych argumentów. Jeśli chcielibyśmy dorzucić do parse_number jakieś argumenty (jak locale17), możemy to zrobić po przecinku18. Szczegóły, jak zwykle, znajdziemy w dokumentacji funkcji across.\n\n\n5.3.2 Odwracanie punktacji\nBardzo często zdarza nam się, że w kwestionariuszach niektóre pozycje mają odwróconą punktację. Na przykład w kwestionariuszu samooceny Rosenberga SES pojawia się pozycja „Czasem czuję się bezużyteczny(-a)“. Odpowiada się na skali 1 do 4. Wiadomo, że osoba, która zaznacza przy takiej pozycji 4, nie pokazuje swojej wysokiej samooceny. Jest to pozycja z odwróconą punktacją, czyli 4 należy liczyć jako 1, 3 jako 2 itd. Przekształcenie to można zrobić bardzo łatwo. Najpierw dodajemy skrajne wartości skali, np. dla SES \\(1 + 4 = 5\\). Teraz od 5 odejmujemy odpowiedź osoby badanej i dzięki temu rzeczywiście 4 zamienia się w 1, 3 w 2 itd. Jak odwrócić punktację w R?\nPonieważ jest to modyfikacja kolumny, użyjemy funkcji mutate. Załóżmy, że H_5 ma odwróconą punktację. Oceny były na skali od 1 do 6, więc wyniki osób badanych musimy odjąć od 7. W takiej sytuacji kod wyglądałby następująco:\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        H_5 = 7 - H_5\n    )\n\nNiektórzy lubią tworzyć nowe kolumny na odwróconą punktację, my jednak po prostu zastąpiliśmy oryginalną kolumnę H_5. Zawsze można jednak użyć innej nazwy, np. H_5_odwr =. Jeśli chcemy odwrócić wiele kolumn, możemy użyć across. Załóżmy, że H_7 też ma odwróconą punktację. W takim wypadku nasz kod mógłby wyglądać tak:\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        H_5 = 7 - H_5,\n        H_7 = 7 - H_7\n    )\n\nJeśli mamy wielki kwestionariusz i nie chce nam się 100 razy pisać tego samego, możemy użyć across:\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), \\(x) {7 - x})\n    )\n\nPierwszym argumentem jest zestaw kolumn, dlatego nazwy kolumn opakowałem w c(). select czy filter nie potrzebowały, żeby robić takie zestawy, ale wiele funkcji (zwłaszcza spoza tidyverse) tego wymaga. Co z drugim argumentem, czyli funkcją? Tutaj wchodzimy głębiej w programistyczne meandry. Można, oczywiście, zostać przy wersji bez across, ale odważnych zapraszam do świata funkcji anonimowych.\n\n\n5.3.3 Własne funkcje\nDrugi argument w across to funkcja, jakiej across ma użyć do przekształcenia kolumn. Niestety nie ma funkcji, która odejmowałaby od 7. Żeby sobie z tym poradzić, musimy albo taką funkcję wcześniej samodzielnie napisać, albo użyć tzw. funkcji anonimowej (zwanej też lambda). Pierwsza opcja jest łatwa do zrozumienia, ale wymaga sporo pisania jak na coś, czego użyjemy tylko raz. Tworzenie własnych funkcji w R jest dość łatwe. Nasza funkcja mogłaby wyglądać tak:\n\nodejmij_od_7 &lt;- function(wynik) {\n    7 - wynik\n}\n\nPierwsza rzecz to nazwa. Obrazowo nazwałem naszą funkcję odejmij_od_7. Dalej następuje słowo kluczowe function i w nawiasie argumenty naszej funkcji. My wrzucamy do funkcji wynik osoby badanej, więc nasz argument nazwałem obrazowo wynik. Pewnie w rzeczywistej sytuacji użyłbym x, bo nazwa funkcji jest wystarczająco sugerująca. Jeśli chcemy, dla czytelności, rozbić funkcję na kilka linijek, otwieramy nawiasy klamrowe i w nich opisujemy, co funkcja ma robić. Nic nie stoi na przeszkodzie, żeby opisać to wszystko w jednej linijce: function(wynik) 7 - wynik. Po wykonaniu powyższego kodu nasza funkcja rzeczywiście działa, co możemy sprawdzić używając jej w konsoli.\n\nodejmij_od_7(3)\n\n#&gt; [1] 4\n\n\n\nodejmij_od_7(12)\n\n#&gt; [1] -5\n\n\n\nJeśli mamy kilka kwestionariuszy z odwróconą punktacją, każdy z inną skalą, możemy od razu zrobić bardziej ogólną funkcję do odwracania.\n\nodejmij_od &lt;- function(wynik, od_czego) {\n    od_czego - wynik\n}\n\nodejmij_od(3, 7)\n\n#&gt; [1] 4\n\n\n\nodejmij_od(2, od_czego = 4)\n\n#&gt; [1] 2\n\n\n\nBardziej ogólna funkcja wymaga podania drugiego argumentu, tzn. od czego trzeba odjąć wynik. Jak widać, ta funkcja też działa i przyjmuje argumenty pozycyjne lub nazwane. Obu funkcji, po zadeklarowaniu, możemy użyć w across.\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), odejmij_od, od_czego = 7)\n    )\n\nTutaj nam się to nie przyda, ale nic nie stoi na przeszkodzie, żeby wewnątrz funkcji używać innych funkcji i wielokrotnie używane serie komend przerabiać na pojedynczą komendę. Hadley Wickham wykorzystuje zasadę, że jeśli kopiujesz kod więcej niż dwa razy, to znaczy, że trzeba zrobić z niego funkcję.\n\n5.3.3.1 Funkcje anonimowe\nJeśli funkcja jest prosta i używamy jej tylko raz, często nie chcemy zaśmiecać sobie kodu jej definicją. Wtedy z pomocą przychodzą nam funkcje anonimowe (zwane też funkcjami lambda). Anonimowe, bo nie mają swojej nazwy. Podstawowy sposób ich używania to zadeklarowanie ich od razu w miejscu użycia.\n\nacross(c(H_5, H_7), function(wynik) 7 - wynik)\n\nZamiast nazwy funkcji użyliśmy tutaj od razu jej definicji. Funkcja jest w pełni sprawna i różni się od odejmij_od_7 tylko tym, że nie ma nazwy.\nInny, jeszcze bardziej zwięzły, sposób używania funkcji anonimowych wprowadza R w wersji 4.1.0. Polega on na zapisaniu naszej funkcji w nawiasach klamrowych. Dla przykładu odemowanie od 7 zapiszemy jako {7 - x}. Dlaczego akurat x a nie np. y albo wynik, jak robiliśmy wcześniej? Bo to ustaliłem pisząc przed naszą anonimową funkcją \\(x). Zamiast x mógłbym swoją zmienną zapisać w dowolny sposób. Takie coś w całości, czyli \\(x) {7 - x}19, możemy zapisać w miejscu funkcji w across, co zrobiłem na początku, jak chciałem wzbudzić ciekawość.\nJak wspomniałem w jednym z przypisów, dokumentacja dplyr od niedawna sugeruje, że jeśli funkcja potrzebuje jakichś dodatkowych argumentów, to żeby używać funkcji anonimowej, zamiast wpisywać arguemnty po przecinku. Co prawda wpisywanie po przecinku działa, ale moglibyśmy chcieć na wszelki wypadek dostosować się do nowych standardów. W takim wypadku moglibyśmy zmodyfikować przykład z poprzedniego podrozdziału, gdzie używaliśmy funkcji odejmij_od i dodatkowego arguemntu od_czego.\n\n# źle, choć jeszcze działa\nacross(c(H_5, H_7), odejmij_od, od_czego = 7)\n\n# dobrze\nacross(c(H_5, H_7), \\(x) {odejmij_od(x, od_czego = 7)})\n\n\n\n5.3.3.2 Problem z Tidy Evaluation\nUżywanie nazw kolumn jako argumentów takich własnych funkcji może spowodować niespodziewane problemy. Załóżmy, że stworzymy funkcję normalizacja_kolumny(baza, nazwa_kolumny). Jeśli potem korzystamy z funkcji typu select(), to nie wiedzą one, że chodzi nam o argument nazwa_kolumny, a nie o kolumnę o nazwie nazwa_kolumny. W związku z tym wypuszczą błąd, że takiej kolumny w bazie danych nie ma (chyba że jest, ale wtedy konsekwencje mogą być jeszcze bardziej niespodziewane). Więcej o tym piszę w bardziej zaawansowanym tekście tutaj, ale ad hoc można sobie z tym poradzić pisząc nazwy argumentów w takich funkcjach w podwójnych nawiasach klamrowych, np. select({{ nazwa_kolumny }})."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-newcols",
    "href": "posts/podstawy_R.html#sec-newcols",
    "title": "Podstawy programowania w R",
    "section": "5.4 Nowe kolumny",
    "text": "5.4 Nowe kolumny\nJak wspomniałem, funkcja mutate nie tylko pozwala na modyfikowanie istniejących kolumn, ale też na tworzenie nowych. Zazwyczaj robimy to w dwóch przypadkach – gdy chcemy zagregować dane z wierszy, np. zsumować wyniki z danego kwestionariusza albo gdy chcemy podzielić naszą bazę na kategorie, np. „młodzi”, „w średnim wieku”, „seniorzy”. Omówmy to po kolei.\n\n5.4.1 Agregowanie danych z wierszy z dplyr::pick\nZałóżmy, że homofobię będziemy liczyć poprzez dodanie H_1 + H_2 + H_3 itd. Czasami będziemy chcieli robić sumy, czasami policzyć średnią20. W kwestionariuszach zazwyczaj liczymy sumy, ale np. dla czasów reakcji zazwyczaj będziemy chcieli policzyć średnią. Jak więc dodać taką kolumnę z sumami w R? Mamy dwa sposoby. Pierwszy to wprost opisanie, co dodajemy, wewnątrz mutate.\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = H_1 + H_2 + H_3 + H_4 + H_5 + H_6 + H_7 + H_8 + H_9 + H_10\n    )\n\nJak można się domyślić, istnieje sposób niewymagający tyle pisania, które w skomplikowanych bazach i długich kwestionariuszach naprawdę może być długotrwałe i uciążliwe. tidyverse ratuje nas tutaj funkcją pick21, a bazowy R dokłada funkcję rowSums (i rowMeans). Wystarczy, że do funkcji rowSums wrzucimy, które kolumny chcemy zsumować, wskazując je właśnie za pomocą pick i ewentualnie funkcji pomocniczych, tak jak robiliśmy w select (por. 5.1.2) i across (por. 5.3.1).\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10))\n    )\n\n\n\n5.4.2 Kategoryzowanie przypadków i przekodowywanie z dplyr::case_when\nCzasem zdarza się, że chcemy podzielić dane ilościowe (np. wiek, wzrost, szczęście mierzone kwestionariuszowo) na kategorie (młodzi vs nie-aż-tak-młodzi, wysocy vs niscy, szczęśliwi vs nieszczęśliwi). Zdarza się też, że osoba tworząca ankietę nie była na tyle przewidująca, żeby w odpowiedziach do „zdecydowanie się zgadzam” dodać 6, więc nie możemy po prostu użyć parse_number. I w jednym, i w drugim wypadku musimy stworzyć wartości na podstawie innych wartości, np. wzrost poniżej 160 cm zamienić na „niski” albo tekst „zdecydowanie się nie zgadzam” zamienić na 1. Do takich celów służy niezwykle przydatna funkcja case_when z pakietu dplyr. Załóżmy, że chcemy podzielić mężczyzn w naszej bazie na trzy kategorie wykształcenia – podstawowe, ponadpodstawowe i wyższe. Oznacza to, że osoby z wykształceniem średnim i zawodowym musimy wrzucić do jednego worka. W tym celu rozszerzymy naszą instrukcję o kolejną komendę. Funkcja case_when ma dość prostą składnię.\n\ncase_when(\n    warunek_1 ~ wartosc_jesli_prawda,\n    warunek_2 ~ wartosc_jesli_prawda,\n    warunek_3 ~ wartosc_jesli_prawda,\n    .default = wartosc_dla_calej_reszty\n)\n\nFunkcja po kolei sprawdza warunki. Jeśli natrafi na jakiś spełniony warunek, zatrzyma się i da taką wartość, jaką temu warunkowi przypisaliśmy. Warunek jest logiczny, czyli może to być cokolwiek od wyksztalcenie == \"Średnie\" po wzrost &lt;= 160. Należy pamiętać, że jeśli wartość wynikowa ma być tekstem, musimy napisać ją w cudzysłowie, jak każdy dosłowny tekst. .default = wartosc może nam służyć do ustalania, co ma być, jeśli żaden z powyższych warunków się nie sprawdzi22. Jeśli chodzi o przykład z wykształceniem, moglibyśmy rozwiązać go tak:\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        )\n    )\n\nW tym wypadku case_when, idąc wiersz po wierszu, sprawdza, czy w kolumnie wyksztalcenie nie znajduje się wartość \"Średnie\". Jeśli tak, to w tworzonej właśnie kolumnie wyksztalcenie_grupa wstawia wartość \"Ponadpodstawowe\" i przechodzi do kolejnego wiersza. Jeśli wykształcenie nie jest średnie, to sprawdza, czy jest zawodowe i w razie czego również wstawia \"Ponadpodstawowe\". Jeśli nie jest ani średnie, ani zawodowe, to wstawia to, co akurat jest w kolumnie wyksztalcenie, czyli dla osób z wykształceniem podstawowym wstawia \"Podstawowe\", a dla osób z wykształceniem wyższym \"Wyższe\"23. W ten sposób z 4 kategorii wykształcenia zrobiły nam się 3. W podobny sposób przekodowywalibyśmy klucz w ankiecie na liczby, np. pisząc H_1 == \"Zdecydowanie się zgadzam\" ~ 6. Możemy ułatwić sobie to przekodowywanie za pomocą questionr, który opisuję w podrozdziale 5.5.1.1.1."
  },
  {
    "objectID": "posts/podstawy_R.html#sortowanie-i-kolejność-kolumn",
    "href": "posts/podstawy_R.html#sortowanie-i-kolejność-kolumn",
    "title": "Podstawy programowania w R",
    "section": "5.5 Sortowanie i kolejność kolumn",
    "text": "5.5 Sortowanie i kolejność kolumn\nWychodzimy już z potężnej funkcji mutate i możemy czyścić dalej. Ostatnia rzecz, którą czasem chcemy zrobić (zazwyczaj ze względów estetycznych), to posortowanie wartości i ustawienie kolumn w określonej kolejności.\n\n5.5.1 Sortowanie z dplyr::arrange\nZa sortowanie w tidyverse odpowiada funkcja arrange. Domyślnie sortuje ona rosnąco, więc jeśli chcemy zastosować sortowanie malejące, użyjemy pomocniczej funkcji desc (od descending). Załóżmy, że chcemy posortować nasze dane najpierw według wykształcenia (od najwyższego, do najniższego), a w obrębie wykształcenia według wieku (od najmłodszych do najstarszych). W pierwszym odruchu chcielibyśmy wpisać arrange(desc(wyksztalcenie), wiek). Jest to dobry odruch, jednak jeśli to zrobimy, zorientujemy się, że najwyższym z wykształceń jest wykształcenie zawodowe. Dzieje się tak dlatego, że w tej chwili wykształcenie to zwykły tekst, a więc jest sortowany alfabetycznie, nie według naszego klucza. Żeby to zmienić, musimy poznać nowy rodzaj danych.\n\n5.5.1.1 Factors\nCzynniki (factors) to rodzaj danych, za pomocą których przechowujemy tekst, który ma tylko kilka możliwych wartości albo gdy te wartości mają jakąś kolejność, którą chcemy wziąć pod uwagę. Jeśli mamy etykiety takie jak wykształcenie, czy grupa kontrolna/eksperymentalna, to powinniśmy je przechowywać właśnie w tej postaci. Danymi tego typu w tidyverse zajmuje się pakiet forcats. Żeby zmienić wykształcenie z czystego tekstu na czynnik, dopiszemy jedną linijkę do naszego mutate i od razu posortujemy.\n\ndf %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        ),\n        wyksztalcenie = factor(\n            wyksztalcenie,\n            levels = c(\"Podstawowe\", \"Zawodowe\", \"Średnie\", \"Wyższe\"),\n            ordered = TRUE\n            )\n    ) %&gt;%\n    arrange(desc(wyksztalcenie), wiek)\n\nKomendę factor dla czytelności rozbiłem tutaj na trzy linijki, ale – oczywiście – można ją całą zapisać w jednej. Po pierwsze wskazałem, że na factor przerobiona ma być kolumna wyksztalcenie. Po drugie wskazałem, jakie wykształcenie może mieć wartości, zbierając je w jeden zestaw funkcją c i wrzucając do argumentu levels. Na koniec poinformowałem R, że w kolejność jest tutaj ważna, dopisując ordered = TRUE. Jeśli tak przerobione dane posortujemy, zobaczymy, że baza rzeczywiście zaczyna się od wykształcenia wyższego.\n\n5.5.1.1.1 questionr\nPrzy okazji czynników chciałbym wspomnieć o pierwszym dodatku (addin) do RStudio, jaki może nam się przydać. Dodatki przyjmują różną formę, ale tutaj omówię dwa, które są graficznymi narzędziami pomagającymi nam pisać kod. Można mieć do nich mieszany stosunek, ale póki umiemy też napisać kod ręcznie (lub chociaż wiemy, jak skorzystać z dokumentacji), to mogą być dużą pomocą, zwłaszcza na początku przygody z R. Pierwszym takim dodatkiem jest questionr, który pozwala nam stworzyć komendy związane z czynnikami (i kategoryzować dane ilościowe, co ręcznie robiliśmy w podrozdziale 5.4.2).\nquestionr instalujemy jak każdy inny pakiet (install.packages(\"questionr\")). Od tego momentu (lub po zresetowaniu RStudio) w menu Addins na górnej belce znajdziemy trzy nowe opcje. Ta interesująca nas to Level ordering. Na początku zobaczymy okienko, w którym możemy wybrać kilka rzeczy. W jakiej zmiennej chcemy zmienić kolejność, w jakiej kolumnie i z jakiego pakietu wziąć funkcję do zmiany kolejności (domyślnie jest to fct_relevel z forcats). W drugiej zakładce możemy graficznie ustawić taką kolejność, jaką chcemy. W ostatniej zakładce otrzymujemy gotowy kod. Questionr nie robi nic samodzielnie, ten kod trzeba jeszcze wkleić w skrypt. Z jednym kruczkiem. Nam chodzi o samą komendę fct_relevel, bez pierwszej linijki, która służy do zapisania zmian. Ponieważ my tworzymy mutate, to wystarczy, że skopiujemy nasze gotowe fct_relevel() do funkcji mutate, dopisując nazwę kolumny i pierwszy argument. Może być to użyteczne, jeśli mamy dużo czynników lub dużo poziomów w czynniku. Ostatecznie nasza komenda mutate z użyciem questionr wyglądałaby tak:\n\nmutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        ),\n        wyksztalcenie = fct_relevel(\n            wyksztalcenie,\n            \"Podstawowe\", \"Zawodowe\", \"Średnie\", \"Wyższe\"\n        )\n)\n\n\n\n\n\n\n\nRysunek 3: Przykład użycia questionr\n\n\n\n\n\n\n\n5.5.2 Kolejność kolumn z dplyr::relocate\nMożemy chcieć mieć nasze kolumny w określonej kolejności. Są zasadniczo dwa sposoby zmieniania kolejności kolumn. Jest funkcja relocate, która służy raczej przestawianiu pojedynczych kolumn lub ich niewielkiej liczby. Jeśli chcemy od nowa określić kolejność kolumn, możemy wykorzystać w tym celu znaną nam już funkcję select (por. 5.1.2). Załóżmy, że chcielibyśmy przestawić kolumnę H_suma przed kolumny z cząstkowymi wynikami.\n\n# z użyciem relocate\ndf %&gt;%\n    relocate(H_suma, .before = H_1)\n\n# z użyciem select\ndf %&gt;%\n    select(id, wiek, wyksztalcenie, H_suma, everything())\n\nJeśli przestawiamy kolumny z użyciem relocate, powinniśmy ustawić argument .before albo .after. Oba wymagają nazwy kolumny przed którą lub po której chcemy mieć naszą kolumnę. Jeśli nie ustawimy żadnego, nasze kolumny zostaną przeniesione na początek tabeli. Jeśli używamy select, musimy wpisać kolejność naszych kolumn ręcznie. Zastanawiające może być użycie przeze mnie everything(). W tym kontekście znaczy ono „i potem cała reszta”."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-save",
    "href": "posts/podstawy_R.html#sec-save",
    "title": "Podstawy programowania w R",
    "section": "5.6 Zapisywanie zmian z <-",
    "text": "5.6 Zapisywanie zmian z &lt;-\nW ten sposób uzyskaliśmy cały kod czyszczący. Mamy ów kod zapisany w naszym skrypcie. Jeśli go uruchomimy, to widzimy, że działa. Jednak jeśli w konsolę wpiszemy samo df, naszym oczom ciągle ukazuje się stara, brzydka baza. Jak więc zmienić nasze df na wyczyszczoną wersję? Tak jak zawsze przypisujemy wartości w R – operatorem &lt;-. Nasz kod na zmianę brudnej bazy w czystą ostatecznie przyjmie postać:\n\ndf &lt;- df %&gt;%\n    filter(Płeć == \"Mężczyzna\") %&gt;%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %&gt;%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %&gt;%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        ),\n        wyksztalcenie = factor(\n            wyksztalcenie,\n            levels = c(\"Podstawowe\", \"Zawodowe\", \"Średnie\", \"Wyższe\"),\n            ordered = TRUE\n            )\n    ) %&gt;%\n    arrange(desc(wyksztalcenie), wiek) %&gt;%\n    relocate(H_suma, .before = H_1)\n\nKod ten możemy uruchomić dla dowolnej ilości danych, w dowolnym momencie. Jest wielokrotnego użytku i spokojnie możemy go wykorzystać, kiedy baza się rozrośnie. Nie musimy go wtedy pisać od nowa, a wystarczy, że go uruchomimy. Co więcej, mogę wpaść jeszcze na jakiś pomysł i dopisać linijkę na samym początku, nie musząc całej reszty robić od nowa. Sprawmy sobie tę przyjemność i zerknijmy na naszą wyczyszczoną bazę.\n\ndf\n\n#&gt; # A tibble: 45 × 15\n#&gt;       id  wiek wyksztalcenie H_suma   H_1   H_2   H_3   H_4   H_5   H_6   H_7\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1    54    21 Wyższe            44     5     5     5     5     2     5     2\n#&gt;  2    33    22 Wyższe            43     5     5     4     5     2     5     2\n#&gt;  3    30    23 Wyższe            45     6     5     5     5     2     5     3\n#&gt;  4    49    23 Wyższe            45     6     6     6     6     1     6     1\n#&gt;  5    31    24 Wyższe            39     3     4     4     4     3     4     4\n#&gt;  6    38    25 Wyższe            44     5     5     5     5     2     5     2\n#&gt;  7    46    25 Wyższe            33     5     5     1     5     2     1     3\n#&gt;  8    43    26 Wyższe            45     6     6     6     6     1     6     1\n#&gt;  9    35    29 Wyższe            50     6     6     6     6     1     6     1\n#&gt; 10     2    52 Wyższe            44     5     5     5     5     2     5     2\n#&gt; # ℹ 35 more rows\n#&gt; # ℹ 4 more variables: H_8 &lt;dbl&gt;, H_9 &lt;dbl&gt;, H_10 &lt;dbl&gt;,\n#&gt; #   wyksztalcenie_grupa &lt;chr&gt;\n\n\n\nPo zapisaniu zmiennej df, tracimy naszą starą bazę danych, która była w niej zapisana. To jest tak, jakbyśmy kopiowali plik do folderu, w którym znajduje się już plik o takiej samej nazwie. Operator &lt;- nadpisuje starą zmienną.\nTym samym, jeśli uruchomilibyśmy nasz kod jeszcze raz, ale już na nowej zmiennej df, wyskoczy nam błąd. W końcu nowa zmienna nie ma tych samych kolumn, co stara zmienna. Co więcej, takiej operacji przypisania nie da się w prosty sposób cofnąć. Jeśli chcemy dostać swoją starą, brudną bazę, musimy ponownie załadować ją z pliku. To prowadzi nas do ważnego wniosku co do pisania skryptów – powinniśmy pisać je tak, żeby dało się z nich odtworzyć wszystko, co robiliśmy od samego początku24. Dzięki temu, jeśli chcemy się z czegoś wycofać, zaznaczamy i wykonujemy cały kod przed interesującym nas momentem. Brak skrótu Ctrl+Z jest jedną z ważniejszych różnic między analizą w programach typu SPSS czy Statistica a analizą w językach programowania typu R czy Python. Wbrew pozorom idzie się przyzwyczaić. Ta sama właściwość pozwala na zachowanie przejrzystości w nauce – pokaż mi swój kod, a będę wiedział bardzo dokładnie, jak prowadziłeś(-aś) swoją analizę."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-summarise",
    "href": "posts/podstawy_R.html#sec-summarise",
    "title": "Podstawy programowania w R",
    "section": "5.7 Grupowanie (dplyr::group_by) i agregowanie (dplyr::summarise)",
    "text": "5.7 Grupowanie (dplyr::group_by) i agregowanie (dplyr::summarise)\nGdy mamy już bazę, zazwyczaj chcemy policzyć pewne statystyki dla podgrup osób badanych, np. dla osób różniących się wykształceniem, płcią czy jakąś manipulacją. Chcemy na przykład poznać średnią homofobię osób o różnym wykształceniu, sprawdzić liczebność naszych podgrup czy policzyć inne zbiorcze statystyki. Możemy, oczywiście, odfiltrować najpierw osoby o wykształceniu podstawowym, policzyć dla nich, potem osoby o wykształceniu średnim itd. Są jednak prostsze sposoby, a obejmują one użycie group_by i summarise25. Te dwie funkcje zazwyczaj idą ze sobą w parze i zgrupowane dane od razu trafiają do summarise. Poniżej przykład.\n\ndf %&gt;%\n    group_by(wyksztalcenie) %&gt;%\n    summarise(\n        n = n(),\n        H_M = mean(H_suma),\n        H_SD = sd(H_suma),\n        H_Me = median(H_suma),\n        V = H_SD / H_M\n    )\n\n#&gt; # A tibble: 4 × 6\n#&gt;   wyksztalcenie     n   H_M  H_SD  H_Me      V\n#&gt;   &lt;ord&gt;         &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 Podstawowe        3  47    3.61    48 0.0767\n#&gt; 2 Zawodowe          2  46    2.83    46 0.0615\n#&gt; 3 Średnie          29  42.3  5.00    43 0.118 \n#&gt; 4 Wyższe           11  42.9  4.35    44 0.101\n\n\n\nJak widzimy, dostaliśmy tabelkę z wykształceniem i wskazanymi statystykami. Funkcja n zliczyła nam przypadki osób z poszczególnym wykształceniem, mean policzyła średnią, sd odchylenie standardowe, a median medianę. V to tzw. współczynnik zmienności. Co to jest, nie jest teraz szczególnie ważne. Policzyłem to tutaj, żeby pokazać, że w obliczeniach możemy też wpisywać niestandardowe operacje (jak dzielenie) bez żadnych strasznych funkcji anonimowych, a także że możemy wziąć wartości z innych kolumn jako argumenty do naszych przekształceń. Tutaj V to odchylenie standardowe średniej homofobii (H_SD) podzielone przez samą średnią (H_M). Każdą kolumnę mogliśmy nazwać wedle życzenia. Jak dowiemy się w podrozdziale 6.1, istnieją funkcje, które najpopularniejsze zastawy statystyk opisowych liczą za nas.\nTak robiliśmy to zawsze, jednak dplyr 1.1.0. wprowadził inny sposób grupowania. Jeśli nie chcemy zapisywać grup w naszej bazie danych na później (czyli w większości przypadków), nie musimy w ogóle używać funkcji group_by. Zamiast tego summarise dostał argument .by, za pomocą którego możemy wskazać grupy jednorazowo, tylko na potrzeby tego jednego podsumowania. Więcej na temat argumentu .by można znaleźć w dokumentacji. Poniżej przykład z innego zbioru danych, w którym pojawia się średnia liczba krzyków w piosence, w zależności od typu piosenki i jej autora (Field, Miles, & Field, 2012). Przy okazji pokazuję też, że można grupować na podstawie wielu zmiennych naraz.\n\n# załadowanie danych z sieci\ndf_scream &lt;- read_csv(\"https://raw.githubusercontent.com/profandyfield/discovr/master/data-raw/csv_files/escape.csv\")\n\n#&gt; Rows: 68 Columns: 4\n#&gt; ── Column specification ────────────────────────────────────────────────────────\n#&gt; Delimiter: \",\"\n#&gt; chr (3): id, song_type, songwriter\n#&gt; dbl (1): screams\n#&gt; \n#&gt; ℹ Use `spec()` to retrieve the full column specification for this data.\n#&gt; ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n# podejrzenie, jak dane wyglądają\nglimpse(df_scream)\n\n#&gt; Rows: 68\n#&gt; Columns: 4\n#&gt; $ id         &lt;chr&gt; \"271\", \"q5b\", \"23x\", \"1ai\", \"7st\", \"fug\", \"v28\", \"64f\", \"c3…\n#&gt; $ song_type  &lt;chr&gt; \"Fly song\", \"Fly song\", \"Fly song\", \"Fly song\", \"Fly song\",…\n#&gt; $ songwriter &lt;chr&gt; \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"An…\n#&gt; $ screams    &lt;dbl&gt; 5, 7, 3, 5, 7, 7, 7, 11, 6, 8, 4, 10, 8, 5, 5, 6, 5, 6, 8, …\n\n\n\n# pogrupowanie i zliczenie średniej liczby krzyków\ndf_scream %&gt;%\n    summarise(\n        M = mean(screams),\n        .by = c(song_type, songwriter)\n    )\n\n#&gt; # A tibble: 4 × 3\n#&gt;   song_type songwriter     M\n#&gt;   &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;\n#&gt; 1 Fly song  Andy        6.41\n#&gt; 2 Fly song  Malcolm     6   \n#&gt; 3 Symphony  Andy        9.53\n#&gt; 4 Symphony  Malcolm     7.06\n\n\n\nKolumny do grupowania podałem jako zestaw, czyli wewnątrz c(). Zgrupowane w ten sposób dane pokazują nam, że Andy pisze bardziej krzykliwe piosenki od Malcolma, ale różnica powiększa się, gdy chodzi o piosenki symfoniczne."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-pivot",
    "href": "posts/podstawy_R.html#sec-pivot",
    "title": "Podstawy programowania w R",
    "section": "5.8 Format długi i szeroki z tidyr::pivot_*",
    "text": "5.8 Format długi i szeroki z tidyr::pivot_*\nFormat długi i szeroki to coś, co rzadko pojawia się w tekstach wprowadzających i nie mam pojęcia czemu. To jest naprawdę ważne. Przełożenie danych z jednego formatu na drugi to często podstawowa operacja, jaką musimy wykonać, kiedy chcemy coś policzyć. Nie mam chyba ani jednego projektu, w którym bym tego nie robił. Do tego współczesne komendy, które to robią, są naprawdę proste. Tym bardziej zaskakujące jest, że np. w Excelu wykonać taką operację jest trudno, jeśli nie umie się korzystać z Power Query. Zacznijmy jednak od tego, co to jest format długi i szeroki.\nTerminy te odnoszą się do sposobu, w jaki składujemy dane. Format szeroki jest tym, co odruchowo tworzymy, kiedy robimy czyste tabelki. Jeden wiersz to jedna obserwacja. Wszystkie dane o konkretnej osobie badanej znajdują się w tym jednym wierszu. Każda kolumna to jedna zebrana dana, np. odpowiedź na konkretne pytanie. W takim formacie znajduje się teraz nasza baza. Weźmy z niej kilka kolumn, po czym użyjmy head, żeby zobaczyć pierwszych pięć wierszy.\n\ndf_wide &lt;- df %&gt;% # zapiszę to jako df_wide, na później\n    select(id, H_1:H_5) %&gt;%\n    mutate(id = 1:nrow(df)) %&gt;% # poprawiam id, żeby były kolejne liczby, zmiana kosmetyczna\n    arrange(id) # sortuję wg id\n\ndf_wide %&gt;% # zapisane dane trzeba jeszcze wyświetlić\n    head(n = 5) # tylko 5 pierwszych wierszy\n\n#&gt; # A tibble: 5 × 6\n#&gt;      id   H_1   H_2   H_3   H_4   H_5\n#&gt;   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     1     5     5     5     5     2\n#&gt; 2     2     5     5     4     5     2\n#&gt; 3     3     6     5     5     5     2\n#&gt; 4     4     6     6     6     6     1\n#&gt; 5     5     3     4     4     4     3\n\n\n\nSą to typowe dane w formacie szerokim. Żeby jednak zrozumieć różnicę, między formatem długim, a szerokim, trzeba jeszcze zobaczyć dane długie. Stwórzmy więc takie.\n\ndf_long &lt;- df_wide %&gt;%\n    pivot_longer(H_1:H_5, names_to = \"pytanie\", values_to = \"ocena\")\n\ndf_long %&gt;%\n    head(n = 10)\n\n#&gt; # A tibble: 10 × 3\n#&gt;       id pytanie ocena\n#&gt;    &lt;int&gt; &lt;chr&gt;   &lt;dbl&gt;\n#&gt;  1     1 H_1         5\n#&gt;  2     1 H_2         5\n#&gt;  3     1 H_3         5\n#&gt;  4     1 H_4         5\n#&gt;  5     1 H_5         2\n#&gt;  6     2 H_1         5\n#&gt;  7     2 H_2         5\n#&gt;  8     2 H_3         4\n#&gt;  9     2 H_4         5\n#&gt; 10     2 H_5         2\n\n\n\nZacznę od skomentowania samych danych, a potem wyjaśnię funkcję. Dane w formacie długim mają oddzielne kolumny na numer pytania i odpowiedź. Pięć kolumn z odpowiedziami na pytania z formatu szerokiego zmieniliśmy w dwie. Powoduje to jednak, że każda osoba badana ma teraz pięć wierszy – w każdym odpowiedź na tylko jedno pytanie. Najpierw następuje 5 wierszy osoby z id 1, potem 5 wierszy osoby z id 2 itd. Widać więc dlaczego formaty te nazywają się szeroki i długi. Szeroki ma wiele kolumn, a mało wierszy (1 na osobę), długi mało kolumn, a wiele wierszy (1 na każde pytanie).\nPo co nam taki format? Zawiera te same informacje, co format szeroki, a trudniej się to czyta. Po pierwsze umożliwia nam to policzenie niektórych rzeczy, których nie policzylibyśmy z formatu szerokiego. Dla przykładu teraz mogę grupować dane według pytań, żeby sprawdzić, czy na każde pytanie badani odpowiadają podobnie. Jeśli moja skala jest dobra i każde pytanie rzeczywiście mierzy to samo, to odpowiedzi na wszystkie pytania powinny być podobne. Być może zrobiłem jakieś kontrowersyjne pytanie, na które wszyscy odpowiadają nisko, mimo że nie różnią się, w tym przykładzie, rzeczywistym poziomem homofobii. Mogę więc, na oko, sprawdzić rzetelność pozycji testowych26. Formatu długiego wymagają też niektóre testy statystyczne.\n\ndf_long %&gt;%\n    summarise(\n        M = mean(ocena),\n        SD = sd(ocena),\n        .by = c(pytanie)\n    )\n\n#&gt; # A tibble: 5 × 3\n#&gt;   pytanie     M    SD\n#&gt;   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 H_1      5.16 0.767\n#&gt; 2 H_2      5    0.929\n#&gt; 3 H_3      4.47 1.46 \n#&gt; 4 H_4      5.49 0.661\n#&gt; 5 H_5      1.53 0.694\n\n\n\nNawet częściej, niż do grupowania po pytaniach, wykorzystujemy format długi do wykresów. Jak się przekonamy dalej (w podrozdziale 7), w gramatyce grafik (The Grammar of Graphics) do jednego obiektu na wykresie możemy przypisać tylko jedną kolumnę. Jeśli więc chcemy zrobić wykres słupkowy np. wyników przed i po, to do osi X przypiszemy kolumnę z etykietami, a do osi Y kolumnę z wynikami. Nie da się więc sensownie zrobić wykresu, jeśli wyniki przed i po mamy w osobnych kolumnach.\nOmówmy więc funkcję, której użyłem do zmiany formatu. Kiedyś robiło się to skomplikowanymi funkcjami melt i cast, które często można znaleźć w innych językach programowania. Dziś w R, na szczęście, mamy intuicyjne funkcje pivot_wider i pivot_longer. Tej pierwszej używamy zmieniając format na szeroki, tą drugą zmieniamy format na długi. Na przykładzie powyżej można stwierdzić, że pivot_longer przyjmuje trzy argumenty. Pierwszy to zbiór kolumn, do jakich chcemy tę funkcję zastosować, jakie chcemy zwinąć. Można tu skorzystać z funkcji pomocniczych typu starts_with() czy everything(). Kolejne dwa argumenty funkcji pivot_longer to names_to i values_to. Są to nazwy kolumn, do których mają trafić, jak nazwa wskazuje, nazwy i wartości z naszych obecnych kolumn. W naszym przykładzie etykiety H_1, H_2 itd. trafiły do kolumny pytanie, zaś same odpowiedzi na te pytania do kolumny ocena.\n\ndf_long %&gt;%\n    pivot_wider(names_from = \"pytanie\", values_from = \"ocena\")\n\npivot_wider ma prostszą składnię, ponieważ nie trzeba w niej wskazywać zakresu kolumn do rozwinięcia, a jedynie gdzie znajdują się nazwy kolumn, a gdzie ich wartości. Robimy to odpowiednio argumentami names_from i values_from. Jeśli jakiejś wartości nie ma w formacie długim (np. gdy osoba z numerem 4 nie odpowiedziała na pytanie 2, to w formacie długim może nie być wiersza 4 H_2), to pivot_wider automatycznie wstawi w tę komórkę NA27. Zdarza się, że funkcji tej musimy użyć dlatego, że niektóre programy generują dane w formacie długim."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-join",
    "href": "posts/podstawy_R.html#sec-join",
    "title": "Podstawy programowania w R",
    "section": "5.9 Retesty czyli złączenia (joins)",
    "text": "5.9 Retesty czyli złączenia (joins)\nZłączenia (joins) to, jak nazwa wskazuje, metoda łączenia dwóch baz danych. Jest to jedna z podstawowych operacji na bazach danych, znana co najmniej od lat 70. i instrukcji JOIN w SQL. Jest to także jedna z operacji niedostępnych w Excelu bez Power Query. W praktyce badawczej może być ona konieczna, gdy mamy badanie wieloczęściowe, w którym musimy stosować wiele baz danych (np. jedną tworzą pomiary z eyetrackera, drugą wyniki w ankiecie, a trzecią test szybkości reakcji). Często zdarza się to też w prostych badaniach ankietowych, w których po jakimś czasie musimy wykonać retest. W obu tych przypadkach lądujemy z dwiema (lub więcej) bazami, które – miejmy nadzieję – mają jakąś wspólną kolumnę, identyfikator osoby badanej, taki sam w każdej z trzech baz28.\nJak więc takie bazy połączyć? Wykorzystajmy tutaj dwie bazy zawierające test i retest, zrobione podczas walidacji kwestionariusza o nazwie KTR. Składał się on z dwóch skal oznaczonych tutaj literkami O i W. Standardową procedurą przy projektowaniu kwestionariusza jest powtórzenie pomiaru po jakimś czasie, żeby sprawdzić, na ile wyniki są stabilne. My taką procedurę wykonaliśmy, przez co dysponujemy dwiema oddzielnymi bazami. Zerknijmy na nie.\n\ndb_test &lt;- read_csv(\"./dane/podstawy-R/join-test.csv\", show_col_types = FALSE)\ndb_retest &lt;- read_csv(\"./dane/podstawy-R/join-retest.csv\", show_col_types = FALSE)\n\ndb_test\n\n#&gt; # A tibble: 76 × 3\n#&gt;    ID    KTR_O KTR_W\n#&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 B3RP     26    30\n#&gt;  2 v4Eb     31    36\n#&gt;  3 j3vB     20    31\n#&gt;  4 wced     27    37\n#&gt;  5 RhPy     15    31\n#&gt;  6 aoEF     32    31\n#&gt;  7 CjRB     23    28\n#&gt;  8 bYhC     28    41\n#&gt;  9 zCdZ     17    27\n#&gt; 10 wspA     21    34\n#&gt; # ℹ 66 more rows\n\n\n\ndb_retest\n\n#&gt; # A tibble: 66 × 3\n#&gt;    Subject KTR_O KTR_W\n#&gt;    &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 j3vB       23    31\n#&gt;  2 wced       25    38\n#&gt;  3 aoEF       33    31\n#&gt;  4 CjRB       24    34\n#&gt;  5 bYhC       29    41\n#&gt;  6 zCdZ       17    28\n#&gt;  7 wspA       24    29\n#&gt;  8 GGwI       18    35\n#&gt;  9 L9ZW       23    35\n#&gt; 10 1stk       24    33\n#&gt; # ℹ 56 more rows\n\n\n\nPierwsza rzecz, która może zwrócić naszą uwagę, to znacznie mniejsza liczba osób badanych przy reteście. Jest to naturalne, jako że wiele osób, mimo wcześniejszych deklaracji, nie wypełnia naszego testu po raz drugi. Widzimy też, że każdy wiersz posiada jakiegoś rodzaju kolumnę z unikatowym identyfikatorem osoby badanej. W języku relacyjnych baz danych takie unikatowe kolumny określa się jako PRIMARY KEY. W bazie danych z pierwszego testu kolumna ta nosi nazwę ID, a w bazie danych z retestu nazywa się ona Subject. Od razu wychodzi na jaw, że identyfikatory są spreparowane, bo nikt się nie pomylił, nie robił dopisków ani nie zdecydował się z jakiegoś powodu NaGlE pIsAć TaK. Moja praktyczna porada jest taka, żeby przed łączeniem baz danych zastosować na kolumnach z id funkcje str_to_lower i str_trim, które odpowiednio zmienią pisownię na same małe litery i usuną ewentualne spacje na początku i na końcu id. Ewentualnie można wykorzystać str_replace_all do usunięcia wszystkich spacji. Resztę identyfikatorów można poprawić ręcznie funkcją case_when (patrz 5.4.2).\nŻeby połączyć nasze bazy, musimy najpierw zdecydować, jak chcemy to zrobić. Możemy albo przyłączyć wyniki z retestu do bazy z testem, albo przyłączyć wyniki z testu do bazy z retestem. Jest to o tyle istotne, że jeśli przyłączymy retest do testu, to będziemy mieli puste wartości u tych osób, które nie wypełniły retestu. Jeśli zrobimy odwrotnie, to z założenia każda osoba, która wypełniła retest, wcześniej wypełniła test, a więc figuruje w pierwotnej bazie. W praktyce bywa różnie. Na przykład ludzie kłamią, że wypełnili test, a jak dostaną link do retestu, to myślą, że w takim razie chociaż to wypełnią. Tak czy inaczej, ta decyzja determinuje typ złączenia, jaki wybierzemy. Najbardziej powszechnym typem jest LEFT JOIN, który do każdego wiersza jednej bazy (pisanej jako pierwszej, czyli po lewej) przypisuje pasujący wiersz drugiej bazy (pisanej jako drugiej, czyli po prawej). Jeśli jakiś wiersz w lewej bazie nie ma odpowiednika w prawej bazie, otrzymujemy puste wartości. Jeśli jakiś wiersz w bazie po prawej nie został przypisany do żadnego wiersza po lewej, nie jesteśmy o tym informowani. Więcej o różnych typach złączeń (np. pozwalających uzyskać wszystkie możliwe kombinacje wierszy) można przeczytać i zobaczyć na obrazkach na przykład tutaj.\nJa przyłączę retest do bazy z wynikami pierwszego testu. Widzę jednak dwa problemy, które będę musiał rozwiązać. Po pierwsze, kolumna z identyfikatorem osoby badanej nazywa się inaczej w obu bazach. Po drugie, kolumny KTR_O i KTR_W nazywają się tak samo w obu bazach. Będę więc musiał wskazać R, na podstawie jakich kolumn ma dokonać złączenia, a także jak ma nazwać kolumny w gotowej bazie, żebym wiedział, które wyniki dotyczą pierwszego testu, a które retestu.\n\ndb_joined &lt;- db_test %&gt;%\n    left_join(\n        db_retest,\n        by = join_by(ID == Subject),\n        suffix = c(\"\", \"_retest\")\n    )\n\ndb_joined\n\n#&gt; # A tibble: 76 × 5\n#&gt;    ID    KTR_O KTR_W KTR_O_retest KTR_W_retest\n#&gt;    &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt;  1 B3RP     26    30           NA           NA\n#&gt;  2 v4Eb     31    36           NA           NA\n#&gt;  3 j3vB     20    31           23           31\n#&gt;  4 wced     27    37           25           38\n#&gt;  5 RhPy     15    31           NA           NA\n#&gt;  6 aoEF     32    31           33           31\n#&gt;  7 CjRB     23    28           24           34\n#&gt;  8 bYhC     28    41           29           41\n#&gt;  9 zCdZ     17    27           17           28\n#&gt; 10 wspA     21    34           24           29\n#&gt; # ℹ 66 more rows\n\n\n\nPierwszy problem rozwiązałem za pomocą argumentu by. Od wersji dplyr 1.1.0 przyjmuje on inną funkcję o nazwie join_by. W jej nawiasach precyzujemy, na podstawie jakich kolumn należy dokonać złączenia. Identyczne kolumny łączymy znakiem ==. Drugi problem rozwiązałem dodając w argumencie suffix przyrostki do nazw kolumn. Zawsze zapisuje się je jako zestaw, czyli wewnątrz c() i zawsze najpierw jest w cudzysłowie przyrostek lewej bazy (u nas db_test), a potem przyrostek prawej bazy (u nas db_retest). Ja chciałem, by kolumny pierwotnej bazy nie miały przyrostka, więc za przyrostek dałem pusty ciąg znaków (czyli po prostu nic w cudzysłowie), zaś do kolumn bazy z retestem dodałem przyrostek \"_retest\". Efekt widać na wydruku z konsoli – 5 kolumn i puste wartości u osób, które nie wypełniły retestu.\nZłączenia to zaskakująco szeroki temat, który daje duże możliwości. Omówiona tu funkcja left_join jest najczęściej stosowana, ale warto zerknąć do dokumentacji i w tutoriale, żeby chociaż dowiedzieć się, co możemy za pomocą złączeń zrobić."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-get_summary_stats",
    "href": "posts/podstawy_R.html#sec-get_summary_stats",
    "title": "Podstawy programowania w R",
    "section": "6.1 Statystyki opisowe z rstatix::get_summary_stats",
    "text": "6.1 Statystyki opisowe z rstatix::get_summary_stats\n\nDo liczenia statystyk opisowych w rstatix wykorzystujemy funkcję get_summary_stats. Dobrze współpracuje ona z tidyverse, potokami i funkcją group_by. Ma też bardzo prostą składnię. Jedyne, co musimy zrobić, to wrzucić do naszej funkcji naszą bazę danych, a otrzymamy rozległą tabelę ze statystykami opisowymi dla wszystkich zmiennych. Szczegóły, wyjaśnienia i ewentualne możliwości modyfikacji poszczególnych statystyk znajdują się w dokumentacji. Jeśli do podstawowych statystych chcielibyśmy doliczyć coś niestandardowego, jak współczynnik zmienności z podrozdziału 5.7, zawsze możemy na gotowej tabeli ze statystykami opisowymi użyć użyć mutate.\n\nlibrary(\"rstatix\")\n\nget_summary_stats(df) %&gt;%\n    mutate(v = sd / mean) # dodaję współczynnik zmienności\n\n#&gt; # A tibble: 13 × 14\n#&gt;    variable     n   min   max median    q1    q3   iqr   mad  mean     sd    se\n#&gt;    &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 id          45     1    55     31    18    43    25 19.3  30.1  15.8   2.36 \n#&gt;  2 wiek        45    17    64     21    20    23     3  1.48 23.2   8.04  1.20 \n#&gt;  3 H_suma      45    30    50     43    40    46     6  4.45 42.9   4.77  0.712\n#&gt;  4 H_1         45     3     6      5     5     6     1  1.48  5.16  0.767 0.114\n#&gt;  5 H_2         45     1     6      5     5     6     1  0     5     0.929 0.139\n#&gt;  6 H_3         45     1     6      5     4     6     2  1.48  4.47  1.46  0.217\n#&gt;  7 H_4         45     4     6      6     5     6     1  0     5.49  0.661 0.099\n#&gt;  8 H_5         45     1     4      1     1     2     1  0     1.53  0.694 0.103\n#&gt;  9 H_6         45     1     6      5     4     6     2  1.48  4.73  1.34  0.199\n#&gt; 10 H_7         45     1     6      2     1     2     1  1.48  1.87  1.08  0.161\n#&gt; 11 H_8         45     2     6      5     5     6     1  1.48  5.13  0.991 0.148\n#&gt; 12 H_9         45     1     6      5     4     6     2  1.48  4.49  1.47  0.219\n#&gt; 13 H_10        45     1     6      5     5     6     1  1.48  5.07  1.29  0.192\n#&gt; # ℹ 2 more variables: ci &lt;dbl&gt;, v &lt;dbl&gt;\n\n\n\nJeśli chcemy otrzymać statysyki opisowe dla zgrupowanych danych, czyli np. osobno dla każdego poziomu wykształcenia, wystarczy, że przed użyciem funkcji get_summary_stats zgrupujemy dane funkcją group_by. W chwili, kiedy to piszę, funkcje rstatix nie posiadają argumentu .by. Jako przykład wykorzystam bazę df_scream z podrozdziału 5.7.\n\ndf_scream %&gt;%\n    group_by(songwriter, song_type) %&gt;%\n    get_summary_stats()\n\n#&gt; # A tibble: 4 × 15\n#&gt;   song_type songwriter variable     n   min   max median    q1    q3   iqr   mad\n#&gt;   &lt;chr&gt;     &lt;chr&gt;      &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1 Fly song  Andy       screams     17     3    11      6     5     7     2  1.48\n#&gt; 2 Symphony  Andy       screams     17     7    13     10     8    10     2  2.96\n#&gt; 3 Fly song  Malcolm    screams     17     3    11      6     5     7     2  1.48\n#&gt; 4 Symphony  Malcolm    screams     17     4    11      7     6     8     2  1.48\n#&gt; # ℹ 4 more variables: mean &lt;dbl&gt;, sd &lt;dbl&gt;, se &lt;dbl&gt;, ci &lt;dbl&gt;"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-freq_table",
    "href": "posts/podstawy_R.html#sec-freq_table",
    "title": "Podstawy programowania w R",
    "section": "6.2 Tabele liczności z rstatix::freq_table",
    "text": "6.2 Tabele liczności z rstatix::freq_table\nDla danych kategorialnych (np. płeć, wykształcenie, klasa) nie liczymy statystyk opisowych, a tabele liczności. Chcemy na przykład wiedzieć, ile w naszej bazie mamy osób z wykształceniem wyższym, ile ze średnim itd. W podrozdziale 5.7 pokazałem, jak to zrobić ręcznie funkcjami summarise i n. Pakiet rstatix ma dla nas gotową funkcję freq_table właśnie do tego celu. Wymaga ona jedynie podania, które kolumny z naszej tabeli zliczyć. Robimy to tak samo, jak zrobilibyśmy w funkcji select (patrz 5.1.2) czy across (patrz 5.3.1).\n\nfreq_table(df, wyksztalcenie)\n\n#&gt; # A tibble: 4 × 3\n#&gt;   wyksztalcenie     n  prop\n#&gt;   &lt;ord&gt;         &lt;int&gt; &lt;dbl&gt;\n#&gt; 1 Podstawowe        3   6.7\n#&gt; 2 Zawodowe          2   4.4\n#&gt; 3 Średnie          29  64.4\n#&gt; 4 Wyższe           11  24.4"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-cor_mat",
    "href": "posts/podstawy_R.html#sec-cor_mat",
    "title": "Podstawy programowania w R",
    "section": "6.3 Macierze korelacji i ich istotność z rstatix::cor_mat",
    "text": "6.3 Macierze korelacji i ich istotność z rstatix::cor_mat\nW ramach eksploracji chcemy czasami zrobić macierz korelacji całego naszego zestawu danych. Może nam do tego posłużyć funkcja cor_mat z pakietu rstatix. Zobaczmy to na przykładzie bazy db_joined z podrozdziału 5.9.\n\n(korelacje &lt;- cor_mat(db_joined, -ID))\n\n#&gt; # A tibble: 4 × 5\n#&gt;   rowname      KTR_O KTR_W KTR_O_retest KTR_W_retest\n#&gt; * &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n#&gt; 1 KTR_O         1     0.43         0.79         0.45\n#&gt; 2 KTR_W         0.43  1            0.35         0.71\n#&gt; 3 KTR_O_retest  0.79  0.35         1            0.39\n#&gt; 4 KTR_W_retest  0.45  0.71         0.39         1\n\n\n\nUżycie, jak widać, jest bardzo proste. Jedyna dodatkowa informacja, jaką sprecyzowałem, to żeby nie brać pod uwagę kolumny ID. W tym wypadku mógłbym też użyć starts_with(\"KTR\"). Efektem działania funkcji jest macierz korelacji. Możemy z niej wyczytać m.in., że korelacja KTR_O i KTR_W z ich retestami wyniosła odpowiednio KTR_O_retest i 0,45. Nie są to jakoś oszałamiające wyniki jak na testy, które mają mierzyć względnie stałe cechy.\nMożna zwrócić uwagę na to, że powyższa komenda wyświetliła nam macierz korelacji, pomimo że normalnie musiałbym jeszcze wywołać samą zmienną korelacje, do której ją zapisałem. Tak to robiliśmy wcześniej. Wykorzystałem tutaj wygodą sztuczkę – jeśli weźmie się całe przypisanie w nawiasy, R potraktuje to jako „przypisz i wyświetl”.\nKorelacje mają swoją istotność, którą możemy chcieć poznać. Jeśli policzyliśmy już macierz korelacji, możemy ją wrzucić do funkcji cor_get_pval. Ewentualnie możemy samą bazę wrzucić do funkcji cor_pmat. Efekt jest ostatecznie ten sam.\n\nkorelacje_p_1 &lt;- cor_get_pval(korelacje)\n\nkorelacje_p_2 &lt;- cor_pmat(db_joined, -ID)\n\nidentical(korelacje_p_1, korelacje_p_2)\n\nkorelacje_p_1\n\nFunkcja identical informuje nas, że obiekty stworzone obiema funkcjami rzeczywiście są identyczne. Powstała nam macierz istotności korelacji. Może być mylące, że korelacje wyświetlają się w notacji naukowej, co jest wygodnym sposobem oznaczania bardzo małych lub bardzo dużych liczb. Zasada jest tu prosta: \\(1,22e-4 = 1,22 \\times 10^{-4} = 0,000122\\). Jeśli jednak chcemy dostać tę informację w przyjaźniejszej formie, możemy użyć funkcji cor_mark_significant29, do której wrzucamy macierz korelacji (nie macierz istotności).\n\ncor_mark_significant(korelacje)\n\n#&gt;        rowname    KTR_O    KTR_W KTR_O_retest KTR_W_retest\n#&gt; 1        KTR_O                                            \n#&gt; 2        KTR_W  0.43***                                   \n#&gt; 3 KTR_O_retest 0.79****   0.35**                          \n#&gt; 4 KTR_W_retest  0.45*** 0.71****       0.39**\n\n\n\nOtrzymujemy naszą macierz korelacji wzbogaconą o gwiazdki. Domyślnie są to zwykłe swiazdki30 z dodatkiem **** oznaczającym mniej niż 0,0001. Gwiazdki możemy dostosowywać, a szczegóły znajdują się w dokumentacji."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-aes",
    "href": "posts/podstawy_R.html#sec-aes",
    "title": "Podstawy programowania w R",
    "section": "7.1 Mapowanie estetyk (aes)",
    "text": "7.1 Mapowanie estetyk (aes)\nPrzejdźmy więc do praktyki. Pakietu ggplot2 nie musimy ładować osobno, bo wchodzi w skład, a jakże, tidyverse. Żeby zacząć tworzyć wykres, musimy zacząć od wywołania funkcji ggplot() (ważne – nie ggplot2, ggplot2 to nazwa pakietu, funkcja to ggplot). W jej obrębie wskazujemy na zbiór danych, na którym chcemy pracować i dokonujemy mapowania estetyk, czyli mówimy naszej funkcji, jakie kolumny mają przełożyć się na jakie elementy wizualne. Wykorzystajmy sobie tutaj jeden z klasycznych zbiorów danych o nazwie diamonds, który automatycznie powinien stać się dostępny po załadowaniu ggplot2. Zerknijmy na niego.\n\nglimpse(diamonds)\n\n#&gt; Rows: 53,940\n#&gt; Columns: 10\n#&gt; $ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n#&gt; $ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n#&gt; $ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n#&gt; $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n#&gt; $ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n#&gt; $ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n#&gt; $ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n#&gt; $ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n#&gt; $ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n#&gt; $ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\ndiamonds to zbiór różnych danych na temat 53 940 brylantów. Szczegółowe dane na jego temat można uzyskać wpisując w konsoli ?diamonds. Ponieważ jest to gigantyczny zbiór, wybierzmy sobie losowo 100 diamentów za pomocą funkcji slice_sample. Użyję jeszcze funkcji set.seed, żeby wyniki losowania były za każdym razem takie same. Nie jest ona obowiązkowa, ale użycie jej sprawi że Twoje wykresy bedą identyczne jak moje.\n\nset.seed(123)\n\ndf_diamonds &lt;- slice_sample(diamonds, n = 100)\n\nZnacznie lepiej. Zrobimy sobie prosty wykres ceny diamentu od jego masy w karatach. Pierwsza rzecz, którą musimy wykonać, to mapowanie kolumn carat i price do osi X i Y. Estetyki mapujemy wrzucając je do funkcji aes – najpierw X, potem Y.\n\nggplot(df_diamonds, aes(carat, price))\n\n\n\n\n\n\n\n\nJak widzimy, powstał nam pusty wykres. To jest właśnie układ współrzędnych, o którym mówiłem wcześniej.\nPoza estetykami X i Y mamy do dyspozycji mnóstwo innych estetyk, m.in. colour, fill, alpha (przeźroczystość), size, linewidth, linetype, shape. Estetyki mają jedną wspólną cechę – są powiązane z jakimiś danymi. Jeśli stwierdzę, że wszystkie moje punkty mają być czerwone, to nie będzie to estetyka, tylko atrybut. O estetyce będę mógł mówić wtedy, gdy kolor będzie zależał np. od przejrzystości diamentu. To rozróżnienie, że atrybuty to stałe właściwości wyglądu, a estetyki to związek wyglądu z danymi, jest o tyle ważne, że nieco inaczej się je definiuje, jak zobaczymy za chwilę."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-geom",
    "href": "posts/podstawy_R.html#sec-geom",
    "title": "Podstawy programowania w R",
    "section": "7.2 Obiekty geom_*",
    "text": "7.2 Obiekty geom_*\nPusty układ współrzędnych to jeszcze nie wykres. Musimy jeszcze dodać jakiegoś rodzaju geom. W naszym przypadku będą to punkty, gdzie każdy punkt będzie reprezentował inny brylant. Ściąga do ggplot2 zawiera świetną rozpiskę, jakie można stworzyć wykresy, w zależności od typu zmiennych, jakimi dysponujemy. Może się ona przydać nawet wtedy, gdy robimy wykresy w innym programie.\nDo tworzenia wykresów punktowych mamy dwa rodzaje obiektów geom – geom_point i geom_jitter. geom_point to typowy wykres punktowy. geom_jitter przydaje się wtedy, kiedy mamy wiele danych o tych samych współrzędnych, np. wiele brylantów o masie dokładnie 0,2 karata i cenie dokładnie $ 300. W takim wypadku wszystkie te punkty nałożyłyby się na siebie, ukryły jeden pod drugim i wydawałoby się, że mamy mniej danych, niż w rzeczywistości mamy. geom_jitter rozwiązuje ten problem odrobinkę przesuwając każdy punkt w losowym kierunku. Lekko tracimy wtedy na dokładności, ale widzimy wszystkie nasze dane. Żeby do naszego wykresu dołożyć kolejne elementy, używamy znaku +.\n\nggplot(df_diamonds, aes(carat, price)) +\n    geom_point()\n\n\n\n\n\n\n\n\nWygląda na to, że im większy diament, tym droższy. Bez zaskoczenia. Możemy do naszego wykresy dołożyć linię trendu jako kolejny geom – geom_smooth. Jeśli chcemy mieć prostą linię, musimy ustawić argument method = \"lm\", co jest skrótem od linear model.\n\nggplot(df_diamonds, aes(carat, price)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nSzare pole wokół niebieskiej linii to przedział ufności. Możemy go wyłączyć ustawiając se = FALSE. Przy tej okazji powiedzmy sobie jeszcze raz o estetykach i atrybutach. Mogę chcieć, żeby kolor mojego punktu zależał od jakości wyszlifowania brylantu z kolumny cut. Ponieważ jest to związek wyglądu z danymi, to jest to estetyka i ustawiam ją wewnątrz aes. Mogę to aes wrzucić albo do funkcji ggplot, jak robiłem wcześniej, albo też do funkcji geom_point, bo to jej mapowanie dotyczy. Zwyczajowo argumenty X i Y w aes mogą być nienazwane, ale wszystkie inne już powinny. Mogę też zmienić kolor linii trendu z niebieskiego na czarny i zrobić ją trochę cieńszą. Jest to zmiana wyglądu, ale arbitralna, bez związku z danymi. Jest to więc atrybut i ustawiam go poza aes, wewnątrz funkcji, której ten atrybut dotyczy. Zobaczmy to.\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\")\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-labs",
    "href": "posts/podstawy_R.html#sec-labs",
    "title": "Podstawy programowania w R",
    "section": "7.3 Tytuły osi i wykresu (labs)",
    "text": "7.3 Tytuły osi i wykresu (labs)\nKolejną rzeczą, którą moglibyśmy chcieć zmienić, są tytuły osi. Możemy też dodać tytuł do samego wykresu. Najwygodniej jest to zrobić dodając kolejny element, labs, w którym dopiszemy nasze tytuły.\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu w zależności od masy\"\n    )\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nJeśli chcemy zmienić etykiety wartości z legendy, mamy dwie opcje – albo przekodujemy te etykiety bezpośrednio w bazie danych, chociażby zaprzęgając do pracy questionr (zob. 5.5.1.1.1), albo użyjemy jednej z funkcji scale_*_discrete i jej argumentu labels, gdzie zamiast gwiazdki piszemy nazwę naszej estetyki. Szlif naszych diamentów jest zmapowany do estetyki colour, więc użyjemy funkcji scale_colour_discrete.\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu w zależności od masy\"\n    ) +\n    scale_colour_discrete(labels = c(\"Zadowalający\", \"Dobry\", \"Bardzo dobry\", \"Premium\", \"Idealny\"))\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-theme",
    "href": "posts/podstawy_R.html#sec-theme",
    "title": "Podstawy programowania w R",
    "section": "7.4 Wygląd wykresów (theme_*)",
    "text": "7.4 Wygląd wykresów (theme_*)\nWykres ma wszystkie elementy na miejscu, ale nie oszukujmy się, nie jest to dzieło sztuki. Żeby poprawić wygląd naszego wykresu, sięgniemy po dwa narzędzia. Po pierwsze ustalimy jego ogólny styl za pomocą jednego z motywów (themes), a potem poprawimy szczegóły z użyciem dodatku do RStudio esquisse.\nOgólny styl wykresu ustala się za pomocą elementów zaczynających się słówkiem theme_. ggplot2 ma wbudowane osiem takich motywów, które można przejrzeć tutaj. W Internecie roi się jednak od niestandardowych motywów, które mogą zaczarować nasze wykresy. Kluczowym ich źródłem może być pakiet ggthemes, motywy z którego można zobaczyć tutaj. Ja jednak chciałem pokazać dwa inne – theme_apa z pakietu papaja i theme_Publication z repozytorium na GitHubie koundy/ggplot_theme_Publication.\npapaja to skrótowiec od Preparing APA Journal Articles i jest to rozległy pakiet pomagający pisać artykuły zgodne ze standardami Amerykańskiego Towarzystwa Psychologicznego (APA). Z tych standardów korzystają nie tylko psychologowie, ale też wiele czasopism z zakresu nauk przyrodniczych. Daje nam on dostęp m.in. do motywu theme_apa dostosowującego wykres do standardów APA.\n\nlibrary(\"papaja\")\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu w zależności od masy\"\n    ) +\n    scale_colour_discrete(\n        labels = c(\"Zadowalający\", \"Dobry\", \"Bardzo dobry\", \"Premium\", \"Idealny\")\n    ) +\n    theme_apa()\n\n\n\n\n\n\n\n\nDrugi motyw nie jest szałowo popularny w społeczności, ale bardzo go lubię. Żeby zadziałał, musimy wcześniej zainstalować u siebie pakiety grid, scales i ggthemes. Spojrzenie w repozytorium pozwoli nam stwierdzić, że nie jest to pakiet, a po prostu zbiór plików. Wchodzimy więc w plik ggplot_theme_Publication-2.R, klikamy raw i kopiujemy link. Następnie użyjemy komendy source, która pozwala nam uruchamiać kod z innych plików .R w naszym skrypcie, jak podamy ich ścieżkę lub link do nich. Cały kod mógłby więc wyglądać tak:\n\nsource(\"https://raw.githubusercontent.com/koundy/ggplot_theme_Publication/master/ggplot_theme_Publication-2.R\")\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu w zależności od masy\"\n    ) +\n    scale_colour_discrete(\n        labels = c(\"Zadowalający\", \"Dobry\", \"Bardzo dobry\", \"Premium\", \"Idealny\"),\n    ) +\n    theme_Publication()\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-esquisse",
    "href": "posts/podstawy_R.html#sec-esquisse",
    "title": "Podstawy programowania w R",
    "section": "7.5 esquisse",
    "text": "7.5 esquisse\nWykres w takiej formie można już uznać za zadowalający. Ale co jeśli chcemy poprawić jakieś szczegóły? Na przykład dostosować kolory? Są na to odpowiednie funkcje, ale jeśli mam być szczery, jest ich na tyle dużo, a w tutorialach tak bardzo przeplatają się stare i nowe metody, że bez gruntownego wyszkolenia (którego na przykład ja nie posiadam) bardzo łatwo jest się zgubić i bez zrozumienia kopiować kod znaleziony w Internecie. Na szczęście istnieje pewne narzędzie, które może nam w tej sytuacji pomóc. Nazywa się esquisse i jest dodatkiem do RStudio.\nInstalujemy esquisse jak każdy inny pakiet (install.packages(\"esquisse\")). Po zainstalowaniu, jeśli jest taka potrzeba, można spróbować zmienić język na polski komendą set_i18n(\"pl\")31. Po zainstalowaniu pakietu, w menu Addins na górnej belce, powinniśmy uzyskać dostęp do opcji ggplot2 builder32, które jest narzędziem do interaktywnego konstruowania wykresów. Pozwala na stworzenie kodu w wygodnym, graficznym interfejsie. Polecam zapoznać się z tym interfejsem, ale nie będę go omawiał, ponieważ jest dość intiucyjny. Po wygenerowaniu naszego wykresu możemy skopiować gotowy kod do skryptu.\n\nggplot(df_diamonds) +\n    aes(x = carat, y = price, colour = clarity) +\n    geom_point(shape = \"diamond\", size = 2L) +\n    scale_color_brewer(palette = \"YlOrRd\", direction = 1) +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        title = \"Cena diamentów od ich masy według szlifu\",\n        caption = \"Źródło danych: ggplot2\",\n        color = \"Przejrzystość\"\n    ) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\",\n        plot.title = element_text(face = \"bold\")\n    ) +\n    facet_wrap(vars(cut))\n\n\n\n\n\n\n\n\nPowyżej wykres, który stworzyłem w equisse. Warto jednak zwrócić uwagę na jego niedoskonałości, takie jak brak polskich tłumaczeń typów szlifu. Pewnym wyjaśnieniem może być dla nas nazwa francuskiego słowa equisse – szkic. Takie narzędzia jak equisse nie zwalniają nas całkowicie z umiejętności kodowania wykresów, ale pozwalają wygodnie tworzyć szkice naszego kodu. Ten szkic mogę pozmieniać, np. zamieniając theme_bw na theme_Publication i dodając polskie tłumaczenia. Nieco inaczej, niż wcześniej, bo facet_wrap nie jest estetyką, więc sposób dodawania do niego niestandardowych etykiet znalazłem w dokumentacji.\n\nggplot(df_diamonds) +\n    aes(x = carat, y = price, colour = clarity) +\n    geom_point(shape = \"diamond\", size = 2L) +\n    scale_color_brewer(palette = \"YlOrRd\", direction = 1) +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        title = \"Cena diamentów od ich masy według szlifu\",\n        caption = \"Źródło danych: ggplot2\",\n        color = \"Przejrzystość\"\n    ) +\n    theme_Publication() +\n    theme(\n        legend.position = \"bottom\",\n        plot.title = element_text(face = \"bold\")\n    ) +\n    facet_wrap(\n        vars(cut),\n        labeller = labeller(cut = c(\n            \"Fair\" = \"Zadowalający\",\n            \"Good\" = \"Dobry\",\n            \"Very Good\" = \"Bardzo dobry\",\n            \"Premium\" = \"Premium\",\n            \"Ideal\" = \"Idealny\"\n            )\n        )\n    )\n\n\n\n\n\n\n\n\nWykresy to olbrzymi temat, który tutaj tylko liznęliśmy z wierzchu. Sądzę jednak, że ta wiedza wystarczy, żeby – z pomocą dokumentacji i Google – być w stanie powoli rozbudowywać swoje umiejętności z zakresu ggplot2. Zwłaszcza, że tworzenie dobrych wykresów do trudna sztuka, co skutecznie udowadnia plebiscyt na najgorszy wykres roku organizowany przez dr. Przemysława Biecka na jego stronie. W tym miejscu mogę polecić jego książkę „Wykresy od kuchni”, dostępną za darmo i okraszoną kodem R, który posłużył do wygenerowania jego wykresów."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-broom",
    "href": "posts/podstawy_R.html#sec-broom",
    "title": "Podstawy programowania w R",
    "section": "8.1 Czyste wyniki, czyli pakiet broom",
    "text": "8.1 Czyste wyniki, czyli pakiet broom\nDlaczego nie jest to dobry format? Bo jest niejednolity i chaotyczny. Nie tylko w widocznej postaci, ale też pod maską, co przeszkadza nam, kiedy chcemy wejść głębiej z programowaniem albo pisać automatyczne raporty. Niestandardowe wykresy również mogą być wtedy problemem. Odpowiedzią na te problemy stał się pakiet broom. Należy on do świata tidyverse, ale nie jest ładowany z pakietem tidyverse, więc trzeba załadować go ręcznie. Rdzeniem tego pakietu jest specjalny słownik, standard tworzenia modeli, w którym kolumny zawierające określone informacje muszą nazywać się w określony sposób. Kolumny, bo wyczyszczony model ma zawsze postać tabeli. Dla przykładu kolumna z wartością \\(p\\) zawsze musi nazywać się p.value, a kolumna z wartością współczynnika – estimate. Wymusza to spójność i zapewnia przewidywalność obiektów zawierających modele. Wiele współczesnych pakietów do modelowania od razu dostosowuje swoje modele do wymogów czystych danych.\nbroom nie jest jednak tylko standardem – to także pakiet zawierający trzy funkcje czyszczące modele. Co więcej zawsze są to te same trzy funkcje, które zawsze przyjmują tak samo nazywające się argumenty. Ta spójność i zapewnienie spójności to najlepsze, co daje nam broom. Omówmy sobie teraz te trzy funkcje i zobaczmy je w akcji.\n\n8.1.1 glance\nPierwszą funkcją czyszczącą jest glance (nie mylić z glimpse). Służy ona do tworzenia czystych opisów samego modelu w pojedynczym wierszu. Będą to takie rzeczy jak statystyka \\(F\\) czy \\(R^2\\). Jak każda funkcja broom, jako pierwszy argument przyjmuje ona nasz model.\n\nlibrary(\"broom\")\n\nglance(lm_price)\n\n#&gt; # A tibble: 1 × 12\n#&gt;   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n#&gt;       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt; 1     0.868         0.866 0.646      482. 2.74e-65     2  -146.  300.  312.\n#&gt; # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\nWidzimy tutaj ogólne podsumowanie skuteczności naszego modelu z całą serią statystyk. Co ciekawe mamy też takie statystyki jak AIC i BIC, których summary nie wyświetla. W ten sposób broom nie tylko czyści nasz model, ale także dostarcza nam nowych informacji.\n\n\n8.1.2 tidy\nFunkcja tidy wyświetla nam najważniejsze elementy modelu. Dla wszelkich regresji będą to przede wszystkim współczynniki regresji, choć dokładna zawartość wynikowej tabeli naturalnie zależy od tego, jaki model czyściliśmy. Zobaczmy to.\n\ntidy(lm_price)\n\n#&gt; # A tibble: 3 × 5\n#&gt;   term         estimate std.error statistic  p.value\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt; 1 (Intercept)     -2.52    0.563      -4.48 1.48e- 5\n#&gt; 2 Sepal.Length     1.78    0.0644     27.6  5.85e-60\n#&gt; 3 Sepal.Width     -1.34    0.122     -10.9  9.43e-21\n\n\n\nWidzimy więc, że oba współczynniki (estimate) są istotne statystycznie, z czego długość działki kielicha ma współczynnik dodatni, a szerokość ujemny.\n\n\n8.1.3 augment\nOstania funkcja czyszcząca nie do końca cokolwiek czyści. Pozwala ona na dodanie do oryginalnej bazy danych informacji zaczerpniętych z modelu, m.in. jaką wartość przewiduje model dla tego przypadku, jaka jest wartość resztkowa (residual), zwykła i standaryzowana, jaki jest dystans Cooka i hat value itd. Dokładne możliwości funkcji augment zależą od rodzaju modelu. Są to informacje użyteczne chociażby kiedy chcemy zidentyfikować outliery. Wszystkie dodane kolumny mają nazwy zaczynające się kropką.\n\naugment(lm_price)\n\n#&gt; # A tibble: 150 × 9\n#&gt;    Petal.Length Sepal.Length Sepal.Width .fitted   .resid   .hat .sigma  .cooksd\n#&gt;           &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1          1.4          5.1         3.5    1.85 -0.446   0.0177  0.648  2.91e-3\n#&gt;  2          1.4          4.9         3      2.16 -0.760   0.0159  0.646  7.54e-3\n#&gt;  3          1.3          4.7         3.2    1.54 -0.237   0.0196  0.648  9.15e-4\n#&gt;  4          1.5          4.6         3.1    1.49  0.00677 0.0218  0.649  8.34e-7\n#&gt;  5          1.4          5           3.6    1.53 -0.134   0.0222  0.649  3.34e-4\n#&gt;  6          1.7          5.4         3.9    1.84 -0.143   0.0324  0.649  5.63e-4\n#&gt;  7          1.4          4.6         3.4    1.09  0.308   0.0243  0.648  1.94e-3\n#&gt;  8          1.5          5           3.4    1.80 -0.302   0.0167  0.648  1.25e-3\n#&gt;  9          1.4          4.4         2.9    1.41 -0.00584 0.0292  0.649  8.43e-7\n#&gt; 10          1.5          4.9         3.1    2.03 -0.526   0.0154  0.647  3.50e-3\n#&gt; # ℹ 140 more rows\n#&gt; # ℹ 1 more variable: .std.resid &lt;dbl&gt;"
  },
  {
    "objectID": "posts/podstawy_R.html#sec-rstatix",
    "href": "posts/podstawy_R.html#sec-rstatix",
    "title": "Podstawy programowania w R",
    "section": "8.2 Swiss Army Knife w R, czyli rstatix",
    "text": "8.2 Swiss Army Knife w R, czyli rstatix\nNiezwykłe możliwości i giętkość pakietu rstatix były już w tym tekście wychwalane i pokazywane. Tutaj jednak chciałbym powiedzieć o głównej zawartości rstatix czy funkcjach wykonujących testy statystyczne.\nR posiada dużą bibliotekę domyślnych funkcji do testów statystycznych, takich jak t.test czy cor.test. I one liczą dobrze, ale w wielu przypadkach są niewygodne. Nie ma się co dziwić, to są stare funkcje, swoją historią sięgające roku 2000, czyli początków R. Nie było wtedy dplyr, RStudio, potoków i całego dobrodziejstwa, które sprawia, że współczesny kod R średnio przypomina klasyczny kod, którego tu nie omawiam, ale który ciągle działa i buduje fundamenty tego języka. Dlatego właśnie wyniki tych testów są niewystandaryzowane, wymagają one niewygodnego wskazywania kolumn jako wektorów33 i mają argument data na drugim miejscu, przez co potoki zawsze wymagają używania ..\nrstatix jest odpowiedzią na te problemy. Poza tym, że dodaje mnóstwo przyjaznych funkcji od siebie, to uwspółcześnia stare funkcje, naprawiając w nich wszystko, co złe, m.in. poprawiając pracę z potokami. Dodatkowo wykorzystuje sprytną nomenklaturę, która ułatwia uczenie się – jeśli w klasycznym R funkcja nazywa się t.test, to w rstatix nazywa się t_test, cor.test zmienia się w cor_test i tak dalej. Jest to zgodne z ogólnym standarem tidyverse, żeby współczesne funkcje zawsze były pisane snake_case (więcej o formatowaniu kodu w podrozdziale 9). Większość funkcji pozwala też na wykonywanie wielu testów jednocześnie i radzi sobie ze zgrupowanymi danymi.\n\n# test korelacji Pearsona\ncor_test(diamonds, price, carat)\n\n#&gt; # A tibble: 1 × 8\n#&gt;   var1  var2    cor statistic     p conf.low conf.high method \n#&gt;   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;  \n#&gt; 1 price carat  0.92      551.     0    0.920     0.923 Pearson\n\n\n\n# test t-Studenta\nt_test(df_scream, screams ~ songwriter)\n\n#&gt; # A tibble: 1 × 8\n#&gt;   .y.     group1 group2     n1    n2 statistic    df      p\n#&gt; * &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n#&gt; 1 screams Andy   Malcolm    34    34      2.70  62.2 0.0089\n\n\n\n# test korelacji na zgrupowanych danych\niris %&gt;%\n    group_by(Species) %&gt;%\n    cor_test(Sepal.Length, Sepal.Width)\n\n#&gt; # A tibble: 3 × 9\n#&gt;   Species    var1       var2    cor statistic        p conf.low conf.high method\n#&gt;   &lt;fct&gt;      &lt;chr&gt;      &lt;chr&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt; \n#&gt; 1 setosa     Sepal.Len… Sepa…  0.74      7.68 6.71e-10    0.585     0.846 Pears…\n#&gt; 2 versicolor Sepal.Len… Sepa…  0.53      4.28 8.77e- 5    0.290     0.702 Pears…\n#&gt; 3 virginica  Sepal.Len… Sepa…  0.46      3.56 8.43e- 4    0.205     0.653 Pears…\n\n\n\n# ANOVA\nanova_test(diamonds, price ~ cut + color + clarity)\n\n#&gt; ANOVA Table (type II tests)\n#&gt; \n#&gt;    Effect DFn   DFd       F         p p&lt;.05   ges\n#&gt; 1     cut   4 53922  93.715  1.42e-79     * 0.007\n#&gt; 2   color   6 53922 298.044  0.00e+00     * 0.032\n#&gt; 3 clarity   7 53922 192.094 1.30e-282     * 0.024\n\n\n\n# testy post-hoc\ntukey_hsd(diamonds, price ~ cut + color + clarity)\n\n#&gt; # A tibble: 59 × 9\n#&gt;    term  group1    group2    null.value estimate conf.low conf.high    p.adj\n#&gt;  * &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n#&gt;  1 cut   Fair      Good               0   -430.    -732.     -128.  9.86e- 4\n#&gt;  2 cut   Fair      Very Good          0   -377.    -656.      -97.9 2.13e- 3\n#&gt;  3 cut   Fair      Premium            0    225.     -51.5     503.  1.72e- 1\n#&gt;  4 cut   Fair      Ideal              0   -901.   -1173.     -629.  6.68e-14\n#&gt;  5 cut   Good      Very Good          0     52.9   -125.      231.  9.28e- 1\n#&gt;  6 cut   Good      Premium            0    655.     481.      830.  4.22e-14\n#&gt;  7 cut   Good      Ideal              0   -471.    -638.     -305.  1.4 e-13\n#&gt;  8 cut   Very Good Premium            0    602.     471.      734.  0       \n#&gt;  9 cut   Very Good Ideal              0   -524.    -644.     -405.  0       \n#&gt; 10 cut   Premium   Ideal              0  -1127.   -1241.    -1012.  0       \n#&gt; # ℹ 49 more rows\n#&gt; # ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\n\n# wielkość efektu d Cohena\ncohens_d(df_scream, screams ~ songwriter)\n\n#&gt; # A tibble: 1 × 7\n#&gt;   .y.     group1 group2  effsize    n1    n2 magnitude\n#&gt; * &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n#&gt; 1 screams Andy   Malcolm   0.655    34    34 moderate\n\n\n\n# wielkość efektu eta^2\ndiamonds %&gt;%\n    aov(price ~ cut + color + clarity, .) %&gt;%\n    eta_squared()\n\n#&gt;        cut      color    clarity \n#&gt; 0.01286207 0.02971211 0.02329457\n\n\n\nDodatkowo rstatix dostarcza nam wygodą funkcję add_significance, która do obiektu z modelem zgodnym z broom dodaje kolumnę z gwiazdkami istotności wartości \\(p\\). Domyślne ustawienia są takie same jak dla funkcji cor_mark_significance34 i również można je modyfikować (por. 6.3).\n\nlm_price %&gt;%\n    tidy() %&gt;%\n    add_significance()\n\n#&gt; # A tibble: 3 × 6\n#&gt;   term         estimate std.error statistic  p.value p.value.signif\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;         \n#&gt; 1 (Intercept)     -2.52    0.563      -4.48 1.48e- 5 ****          \n#&gt; 2 Sepal.Length     1.78    0.0644     27.6  5.85e-60 ****          \n#&gt; 3 Sepal.Width     -1.34    0.122     -10.9  9.43e-21 ****\n\n\n\nNie są to wszystkie funkcje, które oferuje rstatix i zachęcam do zerknięcia do repozytorium rstatix na GitHubie, gdzie autor umieścił długą listę dostępnych funkcji ze streszczeniem ich działania."
  },
  {
    "objectID": "posts/podstawy_R.html#styler",
    "href": "posts/podstawy_R.html#styler",
    "title": "Podstawy programowania w R",
    "section": "9.1 styler",
    "text": "9.1 styler\nPierwszy z tych narzędzi to styler, który sam poprawia wiele aspektów kodu, w tym wiele oczywistych błędów. Instalujemy go jak każdy inny pakiet (install.packages(\"styler\")). Po zainstalowaniu menu Addins wzbogaca się o kilka opcji, z których najważniejsza to style active file35. Podczas pracy uruchamiam to co jakiś czas i zazwyczaj znajdują się jakieś przeoczenia.\n\n\n\n\n\n\nRysunek 4: Przykład wykorzystania styler"
  },
  {
    "objectID": "posts/podstawy_R.html#lintr",
    "href": "posts/podstawy_R.html#lintr",
    "title": "Podstawy programowania w R",
    "section": "9.2 lintr",
    "text": "9.2 lintr\nstyler nie poprawia wszystkich błędów, bo byłoby to zbyt destrukcyjne. Na szczęście lintr może nam większość z nich wypomnieć. Dla każdego naszego projektu powinniśmy raz uruchomić komendę lintr::use_lintr(type = \"tidyverse\"). Spowoduje to wytworzenie w folderze pliku o nazwie .lintr. Zawiera on konfigurację lintr dla danego projektu. Za jego pomocą będziemy mogli tę konfigurację również zmieniać.\n\n\n\n\n\n\nRysunek 5: Przykład wykorzystania lintr\n\n\n\nZmiana dokonuje się przez wpisanie do pliku .lintr36 odpowiednich opcji. Szczegóły omawia dokumentacja (vignette(\"lintr\")), ale na pewno chciałbym wskazać jedną, której zmiana jest wręcz konieczna. Domyślnie lintr uznaje za za długie linie powyżej 80 znaków. To jest strasznie mało! Ja zazwyczaj ustawiam ten limit to na 120-150 znaków. Zmodyfikujmy to. Domyślna zawartość pliku .lintr wygląda tak:\n\nlinters: linters_with_defaults()\nencoding: \"UTF-8\"\n\nBy zmienić limit znaków, należy przekształcić go w ten sposób:\n\nlinters: linters_with_defaults(\n    line_length_linter(120)\n)\nencoding: \"UTF-8\"\n\nGdzie 120 to nowy limit znaków. W podobny sposób możemy zmienić wymagany styl nazywania obiektów. Standardem tidyverse jest snake_case, czyli nazywanie małymi literami, oddzielając słowa podkreślnikiem. Poza tym wyróżniamy jeszcze camelCase, czyli każde słowo składowe wielką literą poza pierwszym, PascalCase, czyli każde słowo wielką literą (choć czasem Pascal case też nazywa się camel case lub upper camel case), kebab-case, czyli oddzielanie słów myślnikiem i dot.notation, czyli oddzielanie słów kropką, żeby wymienić te bardziej znane. Żaden ze stylów nie jest zły, jednal jak już się zdecydujemy na jeden, powinniśmy się go trzymać.\nOba te pakiety, lintr i styler, są bardzo użyteczne, jednak ostatecznie nie zastępują ludzkiej decyzji. Nie wystarczy więc tylko mieć te pakiety, trzeba też – choć pobieżnie – znać same standardy."
  },
  {
    "objectID": "posts/podstawy_R.html#sec-diag",
    "href": "posts/podstawy_R.html#sec-diag",
    "title": "Podstawy programowania w R",
    "section": "10.1 Diagnoza",
    "text": "10.1 Diagnoza\nMożemy roboczo podzielić bugi na dwa rodzaje – takie, które nie pozwalają na wykonanie kodu i takie, przy których kod się wykonuje, ale efekt nie jest taki, jak chcemy, żeby był. Pierwszy rodzaj błędu jest o tyle prosty w zauważeniu, że konsola zacznie na nas krzyczeć na czerwono informatycznym żargonem, jeśli taki błąd popełnimy. Do tej kategorii wpadną wszystkie literówki, błędy składniowe, niewłaściwy typ argumentu itd. Druga kategoria błędów jest bardziej podstępna, bo możemy je zawuażyć 100 linijek od miejsca, w którym rzeczywiście zrobiliśmy coś nie tak. A potem musimy szukać.\nProces debugowania zawsze zaczyna się od zauważenia błędu i dzieje się to zazwyczaj w konsoli. Na niektóre błędy (np. niektóre literówki) zostaną nam pokazane już przez edytor kodu, ale zazwyczaj dopiero wykonanie jakiegoś kodu pozwoli nam zauważyć, że jest on błędny. Zobaczmy sobie przykład, gdzie będziemy chcieli zainstalować pakiet ViewPipeSteps, który przyda nam się potem.\n\ninstall.packages(ViewPipeSteps)\n\n#&gt; Error in eval(expr, envir, enclos): nie znaleziono obiektu 'ViewPipeSteps'\n\n\n\nJeśli uruchominy ten kod, otrzymamy błąd. Komunikatów o błędach nie należy się bać, bo są bezcennym źródłem informacji na temat tego, co się dzieje. Konsola stara się poinformować nas gdzie popełniliśmy błąd i jakiego rodzaju jest to błąd. Tutaj otrzymaliśmy komunikat object 'ViewPipeSteps' not found. Widzimy więc, że komenda szuka zmiennej o nazwie ViewPipeSteps i jej nie znajduje. Wtedy możemy się zorientować, że my rzeczywiście nie mamy takiej zmiennej, bo to w ogóle nie ma być zmienna, tylko nazwa pakietu do zainstalowania. A jak nazwa, to dosłowny ciąg znaków. A więc powinniśmy napisać tak:\n\ninstall.packages(\"ViewPipeSteps\")\n\nAle wpadłem na to, bo już mi się to przydarzyło. To, swoją drogą, częsty błąd, branie w cudzysłów rzeczy, które nie powinny być w cudzysłowie albo odwrotnie, brak cudzysłowia, gdy powinien być on obecny. O tym, jak wpaść na rozwiązanie, piszę więcej niżej. Tutaj jednak warto zwrócić uwagę na sam komunikat błędu.\n\ndf %&gt;%\n    selcet(H_1:H_3)\n\n#&gt; Error in selcet(., H_1:H_3): nie udało się znaleźć funkcji 'selcet'\n\n\n\nPowyższa komenda zwraca błąd could not find function \"selcet\", co może zdarzyć się w dwóch sytuacjach – nie załadowaliśmy pakietu, w którym ta funkcja się znajduje, albo zrobiliśmy literówkę. Tutaj uważne przeczytanie wskazuje na tę drugą ewentualność, zamieniłem literki miejscami.\n\ndf %&gt;%\n    mutate(\n        H_suma = sum(pick(H_1:H_10)),\n        .after = id\n    )\n\n#&gt; # A tibble: 45 × 15\n#&gt;       id  wiek wyksztalcenie H_suma   H_1   H_2   H_3   H_4   H_5   H_6   H_7\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1    54    21 Wyższe          1932     5     5     5     5     2     5     2\n#&gt;  2    33    22 Wyższe          1932     5     5     4     5     2     5     2\n#&gt;  3    30    23 Wyższe          1932     6     5     5     5     2     5     3\n#&gt;  4    49    23 Wyższe          1932     6     6     6     6     1     6     1\n#&gt;  5    31    24 Wyższe          1932     3     4     4     4     3     4     4\n#&gt;  6    38    25 Wyższe          1932     5     5     5     5     2     5     2\n#&gt;  7    46    25 Wyższe          1932     5     5     1     5     2     1     3\n#&gt;  8    43    26 Wyższe          1932     6     6     6     6     1     6     1\n#&gt;  9    35    29 Wyższe          1932     6     6     6     6     1     6     1\n#&gt; 10     2    52 Wyższe          1932     5     5     5     5     2     5     2\n#&gt; # ℹ 35 more rows\n#&gt; # ℹ 4 more variables: H_8 &lt;dbl&gt;, H_9 &lt;dbl&gt;, H_10 &lt;dbl&gt;,\n#&gt; #   wyksztalcenie_grupa &lt;chr&gt;\n\n\n\nPowyższy przykład na nas nie krzyczy, wykonuje się, jednak nie działa tak, jak byśmy chcieli. Celem tego kodu było uzyskanie sumy kolumn od H1 do H10 dla każdej osoby badanej, jak robiliśmy w podrozdziale 5.4.1. Jednak kod nie działa tak, jak byśmy chcieli. W każdej komórce kolumny H_suma znajduje się ta sama wartość. Zaskakująco duża. Wynika to z tego, że R domyślnie myśli kolumnami, nie wierszami. Gdy więc każemy mu sumować kolumny od H1 do H10, to sumuje całe kolumny i w każdą komórkę wrzuca tę sumę. Jeśli chcemy policzyć sumę dla każdego wiersza z osobna, to musimy albo użyć funkcji rowSums zamiast zwykłego sum, albo wcześniej przepuścić przez funkcję rowwise(), czego nie robiliśmy wcześniej.\n\ndf %&gt;%\n    rowwise() %&gt;%\n    mutate(\n        H_suma = sum(pick(H_1:H_10)),\n        .after = id\n    )\n\n#&gt; # A tibble: 45 × 15\n#&gt; # Rowwise: \n#&gt;       id  wiek wyksztalcenie H_suma   H_1   H_2   H_3   H_4   H_5   H_6   H_7\n#&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1    54    21 Wyższe            44     5     5     5     5     2     5     2\n#&gt;  2    33    22 Wyższe            43     5     5     4     5     2     5     2\n#&gt;  3    30    23 Wyższe            45     6     5     5     5     2     5     3\n#&gt;  4    49    23 Wyższe            45     6     6     6     6     1     6     1\n#&gt;  5    31    24 Wyższe            39     3     4     4     4     3     4     4\n#&gt;  6    38    25 Wyższe            44     5     5     5     5     2     5     2\n#&gt;  7    46    25 Wyższe            33     5     5     1     5     2     1     3\n#&gt;  8    43    26 Wyższe            45     6     6     6     6     1     6     1\n#&gt;  9    35    29 Wyższe            50     6     6     6     6     1     6     1\n#&gt; 10     2    52 Wyższe            44     5     5     5     5     2     5     2\n#&gt; # ℹ 35 more rows\n#&gt; # ℹ 4 more variables: H_8 &lt;dbl&gt;, H_9 &lt;dbl&gt;, H_10 &lt;dbl&gt;,\n#&gt; #   wyksztalcenie_grupa &lt;chr&gt;\n\n\n\nOstatecznie orientujemy się, że gdzieś w naszym kodzie jest błąd i musimy go rozwiązać. Musimy jednak wiedzieć dokładnie jaka funkcja ten błąd powoduje. Jeśli jest to błąd uniemożliwiający wykonanie kodu, konsola postara się nas poinformować, gdzie ten błąd się znajduje. Gorzej w przypadku drugiej kategorii błędów. Tutaj musimy mocniej się postarać. Czytanie kodu to podstawowa metoda, ale mamy też narzędzia, które mogą nam w tym pomóc.\n\n10.1.1 ViewPipeSteps\nGdy piszemy całą serię funkcji połączonych potokami, możemy mieć problem ze zidentyfikowaniem, gdzie popełniliśmy błąd. Może się okazać, że w rzeczywistości błąd popełniliśmy kilka linijek wyżej, a nie w funkcji, która ten błąd zwraca. Spójrzmy na uproszczony przykład.\n\ndf %&gt;%\n    select(id, H_1:H_10) %&gt;%\n    filter(wiek &gt; 40)\n\n#&gt; Error in `filter()`:\n#&gt; ℹ In argument: `wiek &gt; 40`.\n#&gt; Caused by error:\n#&gt; ! nie znaleziono obiektu 'wiek'\n\n\n\nBłąd zwraca nam tutaj funkcja filter, jednak błąd popełniliśmy w funkcji select – nie możemy odfiltrować osób w wieku powyżej 40 roku życia, jeśli w naszej bazie nie ma już kolumny wiek. Tutaj możemy to szybko zauważyć, ale w dłuższych ciągach potoków może nie być to takie oczywiste. Zazwyczaj zacznaczamy wtedy kolejne kroki i patrzymy jak nasza baza się zmienia.\n\n# Najpierw uruchamiamy to\ndf\n\n# Potem to\ndf %&gt;%\n    select(id, H_1:H_10)\n\n# Na koniec to\ndf %&gt;%\n    select(id, H_1:H_10) %&gt;%\n    filter(wiek &gt; 40)\n\nZazwyczaj robimy to ręcznie, ale mamy też użyteczne narzędzie służące właśnie wyświetleniu kolejnych kroków ciągu potoków. Jest to dodatek do RStudio ViewPipeSteps. Po zainstalowaniu możemy wykorzystać to narzędzie zaznaczając swój ciąg i wybierając odpowiednią opcję z menu Addins. Otworzy to w kolejnych kartach (albo wydrukuje konsoli) kolejne kroki ciągu funkcji połączonych potokami. Często pozwala to na zidentyfikowanie kroku, w którym pojawia się błąd.\n\n\n10.1.2 Metoda gumowej kaczuszki\nTo zabrzmi jak żart, ale to nie jest żart. Naprawdę istnieje metoda debugowania znana jako metoda gumowej kaczuszki. Ma nawet swoją stronę na Wikipedii. W pierwszym kroku potrzebujemy gumowej kaczuszki. Przy braku kaczuszki możemy skorzystać z innych obiektów nieożywionych. Jeśli czujemy się wyjątkowo okrutni, możemy skorzystać z żywych osób38. Następnie tłumaczymy owej kaczuszce nasz kod, linijka po linijce. Na głos. Tłumaczenie tego na głos pozwoli zauważyć błędy logiczne, które zauważyć jest trudno przy zwykłym czytaniu kodu. Przypomina to metodę Feynmana, którą szerzej opisuję tutaj. Jest to podstawowa metoda debugowania, która pomaga również nabyć nawyki myślowe, dzięki którym będziemy z czasem popełniać mniej błędów."
  },
  {
    "objectID": "posts/podstawy_R.html#rozwiązywanie-problemu",
    "href": "posts/podstawy_R.html#rozwiązywanie-problemu",
    "title": "Podstawy programowania w R",
    "section": "10.2 Rozwiązywanie problemu",
    "text": "10.2 Rozwiązywanie problemu\nCzasami rozwiązanie problemu wprost wynika z diagnozy. Jeśli popełniliśmy błąd taki, że zamiast &lt; napisaliśmy &gt;, to rozwiązaniem tego problemu jest po prostu zmiana znaku. Jeśli usunęliśmy kolumnę z wiekiem, to musimy ją dodać. Rozwiązanie jest logiczną konsekwencją źródła problemu. Jednak czasem musimy nieco wiedzieć, żeby problem rozwiązać. Najważniejszym źródłem rozwiązań jest doświadczenie. Na początku drogi będziemy się często frustrować potykając się o różne głupotki. Jeśli mamy szczęście mieć nauczyciela, pomoże on znaleźć rozwiązanie i szybciej nabyć doświadczenia, pożyczając własne doświadczenie. Dlatego też to, co na początku będzie powodowało problemy, potem stanie się oczywistą częścią języka. Co jednak zrobić, gdy nie mamy jeszcze doświadczenia z danym problemem?\nPierwszym i podstawowym źródłem jest dokumentacja. Doświadczenie podpowiada mi, że być może install.packages(ViewPipeSteps) nie zadziałało, bo potrzeba gdzieś cudzysłowia, ale skąd mam to wiedzieć? Oczywiście wpisując w konsolę ?install.packages. Tam, przy opisie argumentów, znajdę informację, że pierwszy argument, pkgs, powinien mieć postać „character vector of the names of packages”. Oznacza to, że nazwy muszą być w cudzysłowie. Jeśli mamy problem z informatycznym żargonem (do którego przyzwyczaimy się z czasem), na samym dole dokumentacji zawsze znajdziemy przykłady. Dokumentacja jest więc pierwszą rzeczą, którą powinniśmy sprawdzić, pracując z nową funkcją albo funkcją powodującą problemy.\nDrugim istotnym źródłem informacji jest Internet, a konkretniej zazwyczaj StackOverflow. Jest fo swego rodzaju forum informatyczne, gdzie ludzie zadają pytania i czasem otrzymują odpowiedzi. Jeśli wyszukamy treść naszego błędu w Google (inteligentnie pomijając takie rzeczy jak nazwy naszych zmiennych) powinniśmy znaleźć pytanie kogoś, kto miał podobny problem. Jeśli nie (co czasem się zdarza), to sami możemy zadać na StackOverflow pytanie.\nW naszym pytaniu powinniśmy zawrzeć przykład kodu, jaki trzeba wykonać, żeby uzyskać nasz problem. Nazywa się to minimal reproducible example z naciskiem na minimal – nie wrzucamy dużych fragmentów naszej analizy, ale najlepiej piszemy nowy przykład, który pozwala uzyskać ten sam błąd. Praktyczna porada jest taka, że można wykorzystać jedną z kilku wbudowanych w R baz danych:\n\niris – długości płatków i działek kielicha trzech odmian irysów.\nmtcars – parametry techniczne 32 samochodów.\nToothGrowth – dane z eksperymentu o związku witaminy C z długością zębów kawii domowych (3 dawki, 2 sposoby podawania, dobre do ANOVY wieloczynnikowej).\nPlantGrowth – masa wysuszonych roślin z warunków kontrolnego i eksperymentalnego (typowa baza do testów t).\nUSArrests – dane statystyczne o zbrodniach popełnianych w każdym stanie USA w 1973 r.\n\nKażda z tych baz dancyh ma swoją stronę w dokumentacji wyjaśniającą szczegóły. Warto znać chciażby dwie pierwsze bazy. Wykorzystywanie ich w przykładach jest o tyle dobre, że każdy, kto ma R, automatycznie ma dostęp do tych baz. Więcej szczegółow na temat pisania pytań na StackOverflow można znaleźć na ich stronie.\nCzytanie StackOverflow jest zdobywaniem pewnego doświadczenia. Każdy rozwiązany problem to kolejny poznawczy egzemplarz do kolekcji. Im więcej takich mamy, tym trudniej nas zagiąć. Nie oznacza to, że bugi nie będą się pojawiać, ale to, że z pamięci będziemy wiedzieć – lub się domyślać – jak sprawnie je rozwiązać."
  },
  {
    "objectID": "posts/regex.html",
    "href": "posts/regex.html",
    "title": "Znajdź i zmień to narzędzie ostateczne",
    "section": "",
    "text": "Wyrażenia regularne to specjalny szyfr czy kod służący sprawnemu znajdowaniu fragmentów o określonej strukturze w istniejącym tekście. Jeśli chcesz przeczytać o generowaniu całkiem nowego tekstu, sprawdź ten wpis.\nJak wiele razy dawałem wyraz na tej stronie – nie znoszę mechanicznej pracy. To pożeracz czasu, który nic nie wnosi do naszego życia, a uszczupla je systematycznie. Zwłaszcza irytujące są te zadania, co do których czujemy, że polegają na dokładnie tym samym za każdym razem. Intuicyjnie nawet możemy czuć, że muszą istnieć sposoby, żeby tę pracę wykonać automatycznie.\n\n1 Możliwości\nZwlekałem z napisaniem tego tekstu, bo ciężko mi było znaleźć uniwersalny przykład zastosowania wyrażeń regularnych w praktyce. Wszystkie wydają się bardzo konkretne, wąskie. Bałem się, że osoba czytająca ten przykład może pomyśleć „nie zdarzyło mi się to”. I takie w istocie są wyrażenia regularne, ale trochę o to w nich chodzi – żeby zastosować je konkretnie do własnych potrzeb, które mogą być bardzo specyficzne. Jednak ogólne możliwości są bardzo duże, a paleta zastosowań nieskończona. Bardzo prostym przykładem jest sytuacja, gdy otrzymaliśmy listę elementów ponumerowanych tekstem (w sensie nie jest to lista w Wordzie, tylko numerki są rzeczywiście zapisane). Z różnych przyczyn możemy chcieć się tych numerków pozbyć. Czasem musiałem to robić jak składałem tekst. Załóżmy, że mamy taką listę:\n\n\nKeter\nChochma\nBina\nChesed\nGewura\nTiferet\nNecach\nHod\nJesod\nMalchut\n\n\nMożemy zaznaczyć każdy numerek i wykonać tę nudną, mechaniczną pracę. Ale im więcej elementów na liście, tym zmarnuje nam to więcej czasu. Gdybyśmy mogli powiedzieć komputerowi „na początku każdego wersu jest liczba, kropka i spacja – usuń je”, oszczędziłoby nam to czas. I tutaj właśnie przychodzą nam w sukurs wyrażenia regularne. Jest to zestaw kodów pozwalających nam zapisać takie rzeczy jak „początek wiersza” (^) i „jedna lub więcej dowolna liczba” (\\d+), dzięki czemu możemy jednym kliknięciem usunąć nasze liczby w całym tekście.\nJako drugi, bardziej złożony przykład wezmę swoją specyficzną potrzebę. Ostatnio poprawiałem linki do innych wpisów na tym blogu. Wcześniej używałem linków do stron, które wyglądały tak:\n\nWięcej można przeczytać w tekście o [podstawach R](https://nieobliczalne.pl/posts/podstawy_R.html).\n\nJak doczytałem potem, mogłem użyć lepszej składni, która idzie tak:\n\nWięcej można przeczytać w tekście o [podstawach R](./posts/podstawy_R.qmd).\n\nMa ona kilka zalet, m.in. to, że jakbym kiedykolwiek zmienił domenę, to nie muszę poprawiać wszystkich linków ręcznie. Zdecydowałem się więc poprawić starą konwencję na nową. Poprawianie linków we wszystkich wpisach to jednak dużo mechanicznej pracy, której nie lubię. Narzędziem, które mogłoby się tutaj przydać, jest zwykłe znajdź i zmień (find & replace). Możemy zwrócić uwagę, że wystarczy, abym https://nieobliczalne.pl zamienił na kropkę, a końcowe html na qmd. Nie mogę jednak tego zrobić na raty, bo wtedy zamienię też html z innych linków na qmd.\nWyrażenia regularne pozwalają mi zakodować swoje potrzeby. Potrzebuję wyszukać wszystkie słowa zapisane wielką literą? Nie ma problemu. Potrzebuję zamienić w długim kodzie wyrażenia typu {AUT.15} na {AUT_15}, gdzie liczby są różne? Spoko. Chcę masowo zmienić spacje po jednoliterowych myślnikach na twarde spacje? Jasne. Potrzebuję spacjami podzielić wszystkie numery telefonów w tekście na trójki? Da się zrobić. Chcę wyszukać dane słowo we wszystkich formach (paczka, paczki, paczce itd.)? Potrzymaj mi herbatę. O ile nasz tekst ma określoną strukturę, możemy go wyszukać i w razie potrzeby zmodyfikować.\n\n\n2 Gdzie korzystać?\nZanim dojdziemy do tego, jak stosować wyrażenia regularne, powiedzmy sobie, gdzie możemy je stosować. Po pierwsze ma je wiele edytorów tekstu (Notepad++, Kate i inne), ale niewiele pakietów biurowych. Możemy je znaleźć w Google Docs i LibreOffice. Microsoft Office ma własny, beznadziejny system „znaków wieloznacznych” (wildcards), tak jakby nie mogli zastosować ogólnoświatowego standardu. Każde IDE programistyczne, wliczając RStudio i VS Code, daje możliwość używania RegEx. W programowaniu przydają się one szczególnie.\nMamy też wiele wygodnych stron, na których możemy testować (i wykorzystywać) wyrażenia regularne, a które podpowiedzą nam różne popularne znaki lub zbiory i wyjaśnią, co robi to, co napisaliśmy. Kluczowe przykłady to RegEx101 i RegExr. Z nich polecam korzystać zwłaszcza na początku. Przykłady w tym wpisie to screeny z RegEx101.\nW ramach samego języka programowania również zazwyczaj możemy korzystać z wyrażeń regularnych. W R obsługuje je głównie pakiet stringr, zaś w Pythonie odpowiada za nie biblioteka re. Pozwolą one np. na masową modyfikację ciągów znaków w bazach danych.\nOryginalnie wyrażenia regularne związane są z programem grep i powiązanego z nim sed, a także języka awk i jego uwspółcześnionej wersji gawk. grep to wyszukiwarka tekstu w pliku, sed służy głównie do zamiany fragmentów tekstu, zaś awk to cały język stworzony specjalnie do manipulacji tekstem. Wszystkie one niezmiennie są dostępne w linuksie jako programy działające w konsoli.\n\n\n3 Zbiory znaków i |\nZałóżmy, że chcemy znaleźć różne formy słowa „statystyka”. Odmieniając to słowo przez przypadki otrzymujemy ten sam rdzeń statysty- i końcówki -ka, ki, -ce itd. Zostańmy przy liczbie pojedynczej. Za przykład wykorzystajmy sobie tekst:\n\nStatystyka to nauka, która zajmuje się zbieraniem, analizą i interpretacją danych liczbowych. Jej podstawowym celem jest odkrywanie regularności i wzorców w zjawiskach społecznych, ekonomicznych, naukowych i innych dziedzinach. W statystyce można zastosować wiele różnych metod, takich jak testy hipotez, analiza regresji, czy wykorzystanie rozkładów prawdopodobieństwa. Bez statystyki trudno byłoby dokonać rzetelnych prognoz czy wykazać zależności między zjawiskami. Dlatego statystykę można lubić lub nie, ale ciągle jest ona ważna. Statystyko! Dziękujemy ci!\n\nWyrażenia regularne pozwalają nam powiedzieć „wszystko, co ma rdzeń statystyk- i dane końcówki”. W tym wypadku wykorzystamy operator lub zapisywany znakiem separatora | (klawisz tuż pod backspace, klikany z shiftem) oraz nawiasy kwadratowe, które tworzą zbiory znaków (character sets).\n\n[Ss]tatysty(ka|ki|ce|kę|ką|ko)\n\n\n\n\n\nWszystkie przykłady wykorzystują RegEx101. Klikając w screen, przeniesiesz się do RegEx101, żeby przejrzeć dany przykład dokładniej.\n\n\n\n\n\n\n\nWszystkie przykłady wykorzystują RegEx101. Klikając w screen, przeniesiesz się do RegEx101, żeby przejrzeć dany przykład dokładniej.\n\n\n\nRozbijmy ten przykład na czynniki pierwsze. Po pierwsze mamy [Ss]. Nawiasy kwadratowe oznaczają, że wyrażenie ma wyszukiwać jeden ze znaków wewnątrz nich. W tym wypadku dopasuje zarówno duże, jak i małe S. Mogę tam też zapisywać przedziały typu [0-9] albo [A-Za-z]. Możemy też powiedzieć „wszystko poza znakami ze zbioru”, co zrobimy pisząc ^, np. [^0-9] dopasuje wszystko, co nie jest cyfrą.\nDalej mamy zwykłą część słowa „statystyka” i nawiasy. RegEx domyślnie ignoruje nawiasy (tj. nie wyszukuje ich). Służą one jako znaki specjalne do tworzenia grup, o których więcej później. Wewnątrz nawiasu wypisałem końcówki, które mają być dopasowane. Każdą końcówkę rozdzieliłem pionową kreską, co oznacza „albo to, albo to, albo to…“.\nJeśli chcielibyśmy wyszukać rzeczywiste nawiasy albo separator „|“, musimy poprzedzić je lewym ukośnikiem \\. Ta czynność nazywa się escaping i wymagają tego wszystkie znaki specjalne, na czele z nawiasami, kropką i samym lewym ukośnikiem, który wyszukujemy pisząc \\\\. Później będzie o tym więcej, ale chodzi o to, że jak chcemy wyszukać znak (, to w wyrażeniu regularnym zapiszemy \\(, bo samo ( oznacza początek grupy. Lewy ukośnik przełącza specjalne znaczenie na dosłowne.\nPoprzedni przykład mógłbym napisać nieco zwięźlej jako [Ss]tatysty[kc][aieęąo]. Technicznie wtedy dopasujemy też takie twory jak „statystycą” albo „statystyke”, ale to zazwyczaj nie jest problem, bo ich w tekście po prostu nie ma. Tutaj pojawia się jednak dość ważny wniosek – wyrażenia regularne dość łatwo się pisze i fatalnie się czyta.\nJako ciekawostkę mogę podać, że pisząc to wyrażenie regularne odmieniłem słowo „statystyka” przez przypadki w Kate, potem wyrażeniami regularnymi usunąłem „statysty-“, zastąpiłem znak nowej linii (\\n) separatorem (|) i na koniec ręcznie wziąłem to w nawiasy (patrz tutaj). Tak było mi łatwiej, niż pisać same te końcówki. Ale ja jestem uczulony na pracę mechaniczną bardziej niż reszta społeczeństwa i robiłem to już tyle razy, że nawet o tym nie myślę. Tak jest mi szybciej, ale nie każdy potrzebuje robić takie rzeczy automatycznie.\n\n\n4 Znaki specjalne i kotwice\nRegEx dysponuje całą serią znaków specjalnych typu „dowolna liczba” (\\d), „dowolna litera” (\\w), „dowolny znak odstępu” (\\s, głównie spacja, ale też np. tabulator albo twarde spacje) itd. Najszerszym znaczeniowo znakiem tego typu jest kropka (.). Oznacza ona po prostu „dowolny znak”. Dokładny spis tych znaków specjalnych może różnić się w zależności od wersji (flavour) RegEx, jaką dany program czy język implementuje. Są to wygodne skróty dla zapisów typu [0-9]. Każdy z nich ma też wersję zanegowaną w postaci wielkiej litery, np. \\D dopasuje wszystko, co nie jest liczbą. Do tego dochodzą nam znaki niewidoczne typu znak nowej linii (\\n, „enter”) albo tabulator (\\t).\nJeśli podajemy dane wyrażenie regularne jako argument w jakiejś funkcji w języku programowania, bardzo często będziemy musieli zapisać np. \\\\d czy \\\\n. Wynika to z faktu, że język programowania musi wiedzieć, że chodzi nam o lewy ukośnik w wyrażeniu regularnym, a nie, że escape’ujemy jakiś znak. Innymi słowy jeśli w Pythonie czy w R zapiszemy po prostu \"\\d\", to program zwariuje, bo nie będzie wiedział, jak escape’ować literę d. Dlatego zapiszemy \"\\\\d\" i wtedy program odczytując coś takiego, zrozumie to jako \\d. Jednak znowu – to się tyczy tylko języków programowania.\nRegEx implementuje również tak zwane kotwice, czyli oznaczenia pozycji. Mówiąc konkretnie, ^ oznacza początek linii (akapitu), a $ oznacza koniec linii. linia jest tutaj fragmentem tekstu między kolejnymi „enterami”, a nie tym, co akurat program wyświetla jako linię tekstu. Dlatego w tekście ciągłym cały akapit jest, technicznie, linią.\nWeźmy sobie za przykład tekst skopiowany z Worda, w którym mamy pozostałości po listach, które chcemy usunąć. Chodzi nam o sekwencje liczba, kropka i spacja obecne na początku linijki tekstu.\n\n\nWariancja – co to jest wariancja, jak się ją wyjaśnia i co to jest model.\nTesty statystyczne i wartość p – jak działają testy statystyczne, co to jest p i jak je interpretować.\nKombinatoryka – krótkie omówienie podstawowych terminów kombinatoryki, może być 1. wprowadzeniem do prawdopodobieństwa.\n\n\nOdpowiednie wyrażenie regularne mogłoby brzmieć ^\\d\\.. Po kropce jest tam spacja, którą silnik uporczywie mi usuwa, dlatego na stronie jej nie widać.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZwróćmy uwagę na kilka rzeczy. Po pierwsze, to wyrażenie nie dopasowało liczby „1.” w treści 3. punktu. Wynika to z tego, że dodaliśmy ^ na początku wyrażenia. Liczby, które nie są na początku linijki nie są przez to dopasowywane. Po drugie, musiałem przed kropką dodać ukośnik. Wynika to z faktu, że kropka to znak specjalny (oznacza dowolny znak) i jeśli chcemy wyszukać dosłowną kropkę, musimy wykorzystać lewy ukośnik.\n\n\n5 Powtarzanie\nNajbardziej (według mnie) użyteczne operatory, jakie oferuje RegEx, to operatory powtarzania. Jest to seria operatorów mogących powiedzieć „jedna lub więcej cyfr” albo „zero lub więcej liter”. Możemy wykorzystać je np. do znalezienia wszystkich tekstów wewnątrz nawiasów albo wszystkich słów zaczynających się wielką literą. Spróbujmy zrobić to drugie.\n\nTeoretycznie tyle wystarczy, ale żeby uprzyjemnić proces pisania, fajnie jest przygotować sobie jakieś IDE (program do programowania), np. Visual Studio Code albo chociaż porządny edytor tekstu w stylu Notepad++.\n\n\n[A-Z]\\w+\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZnak + oznacza tutaj „jeden lub więcej”, a więc \\w+ oznacza „jedna litera lub więcej”. Operatorów podobnych do + jest więcej i możemy je zobaczyć w tabelce.\n\n\n\nOperator\nZnaczenie\n\n\n\n\n?\nzero lub jedno wystąpienie\n\n\n*\nzero lub więcej wystąpień\n\n\n+\njedno lub więcej wystąpień\n\n\n{n}\ndokładnie n wystąpień\n\n\n{n,}\nn lub więcej wystąpień\n\n\n{n,m}\nmiędzy n a m wystąpień\n\n\n\nZ użyciem tych operatorów może się wiązać problem chciwości (greedyness). Załóżmy, że tym razem chcemy wyszukać wszystko, co zostało zapisane w nawiasach1.\n\nDlaczego jednak napisałem range(1, 11) a nie range(1, 10)? Python działa tutaj specyficznie. Wynika to z faktu, że w informatyce liczy się od 0, nie od 1. Jeśli do funkcji range() wrzucę tylko jedną liczbę, czyli na przykład range(10), to dostanę 10 elementów. Ponieważ jednak pierwszy element to 0, to będą to liczby od 0 do 9. Mogę podać dwie liczby, żeby powiedzieć funkcji range(), od czego ma zacząć, ale wtedy muszę mieć w głowie, że skoro range(0, 10) oznacza 10 liczb od 0 do 9, to liczby od 1 do 10 muszę zapisać jako range(1, 11). Innymi słowy koniec skali nie wlicza się do zakresu.\n\n\n\\(.+\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDziwne. Czemu takie długie wyszło? W naszym wyrażeniu regularnym wykorzystałem nawiasy (z lewym ukośnikiem, bo nawiasy to znak specjalny) oraz sekwencję .+ oznaczającą „jeden dowolny znak lub więcej”. Domyślnie jednak + jest chciwy (greedy) i wyszuka najdłuższe możliwe sekwencje. Oznacza to, że zaznaczył wszystko między pierwszym nawiasem otwierającym i ostatnim nawiasem zamykającym. W końcu nawiasy zamykające łapią się do kategorii dowolnych znaków. Żeby temu przeciwdziałać, zamiast .+ możemy użyć .+?.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJest już lepiej! Problem jednak powodują w tym wypadku puste nawiasy. Nasze wyrażenie regularne wymaga, by między nawiasami coś było. Takie właśnie sekwencje znajduje w przypadku zapisu (). Nawias zamykający jest po prosu kolejnym znakiem, czyli łapie się jako ., a dopasowanie kończy dopiero kolejny nawias zamykający.\nRozwiązania widzę tutaj dwa. Po pierwsze można wykorzystać lookarounds, żeby pustych nawiasów nie dopasowywać. Pozwolą one też dopasować tekst wewnątrz nawiasów bez samych nawiasów. Nie chcę jednak wchodzić tak głęboko, dlatego zainteresowanych odsyłam tutaj. Po drugie możemy umożliwić dopasowywanie też pustych nawiasów zastępując operator „jeden lub więcej” (+) operatorem „zero lub więcej” (*). Pamiętajmy jednak o chciwości tych operatorów i dodajmy jeszcze ?.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 Grupy i zastępowanie\nZazwyczaj po to wyszukujemy tekst, żeby zastąpić go innym. Jak to jednak zrobić, jeśli mamy takie fragmenty tekstu jak .+? Nie mogę wpisać w pole z tekstem do podstawienia .+, bo dostanę dosłowną kropkę i plus. Program musi dokładnie wiedzieć, co w dane miejsce wstawić. Załóżmy, że w poprzednim przykładzie chcemy zamienić nawiasy okrągłe na kwadratowe. Jak to zrobić? Za pomocą grup.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJak to zrobiłem? Najpierw w wyszukiwaniu zmieniłem sekwencję \\(.*?\\) w \\((.*?)\\). Innymi słowy otoczyłem .*? dodatkową parą nawiasów. Na screenie zaznaczone są na zielono. Znaki w nawiasach tworzą grupy. Grupy po to są nam potrzebne, żebyśmy mogli je potem wykorzystywać do zastępowania tekstu2. Zwróćmy uwagę na tekst w drugim polu, gdzie wpisałem [$1]. Oznacza to „tekst z pierwszej grupy ($1) w nawiasach kwadratowych”. Czyli bierzemy wnętrze naszego pierwotnego nawiasu (grupę 1.) i otaczamy je nawiasami kwadratowymi. Nawiasy okrągłe nie są częścią grupy, więc nie pojawiają się w tekście po zmianie.\nOdwołania do grup mogą wyglądać różnie w różnych wersjach RegEx. W tym wypadku użyłem $1, ale możemy się też spotkać z innymi oznaczeniami, przede wszystkim \\1. Zależy to od programu.\n\n\n7 Zamiana linków na ścieżki\nZ całą tą nową wiedzą możemy wreszcie rozwiązać problem z początku tego wpisu. Przypomnijmy – chcemy linki zamienić na ścieżki do plików .qmd.\n\nWięcej można przeczytać w tekście o [podstawach R](https://nieobliczalne.pl/posts/podstawy_R.html).\n\nRozwiązaniem, którego ja użyłem, było:\n\n]\\(.+(posts\\/.+)\\.html\\)\n\nTekstem zastępującym było:\n\n](./$1.qmd)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nZaczynam zamykającym nawiasem kwadratowym, bo hiperlinki w markdown są formatowane jako [tekst](link). Dalej otwieram dosłowny nawias okrągły i w nim dowolny tekst. Mogłem napisać dokładny link, bo to o ten i żaden inny tekst mi chodzi, ale nie chciało mi się pisać. Zabezpieczyłem się przed złymi dopasowaniami słowem „posts”. Po nim znajduje się prawy ukośnik i dowolne znaki, czyli wewnętrzna nazwa danego wpisu. posts/.+ jest też grupą, dlatego jest w nawiasach. Wyrażenie zakończone jest rozszerzeniem .html i nawiasem zamykającym. Całość zastępuję swoją grupą posts/nazwa_wpisu poprzedzoną tekstem ./3 i zakończoną rozszerzeniem .qmd. Całość otaczam odpowiednimi nawiasami.\nCzy mogłem zrobić to lepiej? Oczywiście! Tylko po co? Te sekwencje muszą być użytkowe. Nie musimy się starać, żeby były odporne na błędy i działały w każdym wypadku. To są rzeczy, które pisze się szybko, wykorzystuje i kasuje. Nie muszą być idealne, muszą być napisane sprawnie i działać tu i teraz, w tym konkretnym przypadku. W końcu po to uczymy się wyrażeń regularnych, żeby oszczędzić sobie pracy, a nie jej przysporzyć. Jeśli robimy coś często i chcemy to zachować na przyszłość, to może wtedy warto się bawić w szlifowanie. Podobnie jeśli wyrażenie regularne jest częścią skryptu. Mimo wszystko w większości przypadków piszemy wyrażenia regularne na jeden raz i warto o tym pamiętać.\n\n\n8 Ściągi\nInformacji jest dużo, ale na szczęście nie musimy tego wszystkiego znać na pamięć, żeby z tego korzystać. Możemy się wspierać różnymi dostępnymi w sieci materiałami, które przypomną nam, jak się coś robiło. Kluczowe jest, by wiedzieć, że coś da się zrobić. Chyba najlepszym materiałem, z którego korzystam praktycznie zawsze, jak chcę napisać wyrażenie regularne, jest cheat sheet do pakietu stringr, który możemy znaleźć tutaj. Mam go w szufladzie biurka położonego tak, że jest to pierwsze, co widzę, jak ją otworzę. Jeśli zapomnę, jak było „zero albo więcej” albo chcę zrobić coś bardziej skomplikowanego, to patrzę do szuflady i już pamiętam. Rzeczy, których używam często, zdążyłem zapamiętać, a resztę czytam ze ściągi.\n\n\n9 Bonus: szybkie usuwanie sierot\nZanim zacznie się kontrowersja, „sieroty” oznaczają tutaj wiszące jednoliterowe spójniki, które nie powinny znaleźć się na końcu linijki. Pozbywamy się ich za pomocą twardych spacji. Zazwyczaj scrollujemy dokument w Wordzie i wyszukujemy je wzrokiem, po czym dodajemy gdzie trzeba. Potem zmieniamy wielkość czcionki albo szerokość akapitu i wszystko się rozjeżdża. Wyrażeniami regularnymi możemy dodać spacje nierozdzielające wszędzie za jednym zamachem. Jak wspominałem, Word nie obsługuje wyrażeń regularnych, więc musimy to zrobić w Libre Office. Trzeba tylko w okienku Znajdź i zamień, w Innych opcjach zaznaczyć Wyrażenia regularne. Nie będę tutaj tłumaczył jak to działa, podam gotowy przepis.\nWyszukaj to:\n(?&lt;=\\s)([wuioaz])\\s\nZamień na to:\n$1 \nZwrócę tylko uwagę, że po $1 musi być twarda spacja. Możemy ją dodać klikają na pole tekstowe prawym przyciskiem myszy, wchodząc w znaki specjalne i wyszukując NO-BREAK SPACE. Ma ona kod 160 (U+A0).\n\n\n10 Podsumowanie\nNa koniec mogę polecić dwa źródła informacji o wyrażeniach regularnych. Pierwsza to strona RexEgg, która zawiera mnóstwo wpisów uszeregowanych w tutoriale od podstaw do czarnego pasa. Druga to książka Wyrażenia regularne autorstwa Jeffreya Friedla, wydana nakładem Wydawnictwa Helion. Oba źródła są pełne informacji, sztuczek i gotowych rozwiązań, za pomocą których różne czary można czynić w tekście. Na stronie RegexOne znajdziemy również interaktywne ćwiczenia.\n\nWyrażenia regularne (regular expressions, RegEx) pozwalają znajdować i modyfikować regularne struktury w tekście.\nNawiasy kwadratowe dopasowują dowolny znak zapisany w tych nawiasach, np. [tk] dopasuje statystyka. [^tk] dopasuje odwrotność, czyli statystyka.\n^ to początek linijki, $ to koniec linijki.\nIstnieje cała seria znaków wieloznacznych, z których najważniejsze to \\w (dowolna litera), \\d (dowolna cyfra), \\s (dowolny znak typu spacja), . (dowolny znak) i \\n (znak nowej linii, „enter”).\nOperatory ilości pozwalają powiedzieć, że dany znak pojawia się więcej niż raz – * (zero lub więcej), + (jeden lub więcej), ? (zero lub jeden). Domyślnie te operatory są chciwe (greedy), a żeby to zmienić, dodajemy do nich znak zapytania – *? i +?.\nGrupy bierzemy w nawiasy. Potem możemy wykorzystywać te grupy w zastępowaniu, pisząc $1 lub \\1 w zależności od systemu.\nWyrażenia regularne to coś, co piszemy na szybko, na już, żeby działało tu i teraz, a niekoniecznie, żeby działało zawsze.\nDobrą ściągą jest cheat sheet do pakietu stringr.\n\n\n\n\n\n\nPrzypisy\n\n\nW rzeczywistości RegEx ma do tego specjalne operatory, ale nie chcę wchodzić tak głęboko.↩︎\nEwentualnie możemy ich użyć do stosowania operatorów do całych grup, np. (foo)+ dopasuje jedno całe słowo „foo” lub ich więcej. Bez nawiasu + odnosiłby się tylko do litery „o”.↩︎\nWarto zauważyć, że w polu, w którym wpisujemy tekst, którym zastępujemy nasze dopasowania, nie musimy już escape’ować znaków specjalnych. Tam już wszystkie znaki są dosłowne.↩︎"
  },
  {
    "objectID": "posts/kombinatoryka.html",
    "href": "posts/kombinatoryka.html",
    "title": "Kombinatoryka",
    "section": "",
    "text": "Pokaż kod\nimport itertools\nimport pandas as pd\n\ndef variations(iterable, subset_length):\n    '''Kombinacje w wierszach, permutacje w kolumnach, wszystko razem to wariacje'''\n    df = pd.DataFrame([list(itertools.permutations(x)) for x in itertools.combinations(iterable, subset_length)])\n    df.index = range(1, len(df.index) + 1)\n    df.columns = range(1, len(df.columns) + 1)\n    return df\n\n\nKombinatoryka to część matematyki zajmująca się modyfikacjami zbiorów. Weźmy sobie zbiór 5 pierwszych liter alfabetu i nazwijmy go Z jak zbiór. Zacznę od skomplikowanie brzmiącego wstępu, a potem wyjaśnię to na przykładach.\n\nZ = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\nZ tym zbiorem mogę zrobić kilka rzeczy. Mogę go zacząć rozbijać na mniejsze zbiory. Mogę zacząć przestawiać w nim elementy. Mogę najpierw rozbić go na mniejsze zbiory, a potem przestawiać elementy w tych małych zbiorach. Każda z tych akcji ma swoją własną nazwę. Jeżeli mówię, że:\n\npermutuję – zmieniam kolejność elementów;\nkombinuję – rozbijam swój zbiór na mniejsze zbiory (combine – łączyć; łączę stare elementy na nowo).\n\nKiedy robię permutacje, z góry zakładam, że kolejność ma znaczenie. Jest wiele sytuacji, w których kolejność ma znaczenie, ale są też sytuacje, w których liczy się tylko to, jakie mam elementy, a nie w jakiej są kolejności. Dla przykładu nieważne, czy w losowaniu Lotto wyciągnięto 2, 5, 7 czy 7, 5, 2 – jeśli mamy te liczby na swoim kuponie, możemy dostać nagrodę. Jeśli kolejność ma znaczenie, mówimy o wariacjach, a jeśli znaczenia nie ma, mówimy o kombinacjach.\nPrzed chwilą mówiłem, że zmiana kolejności to permutacja, a potem nagle używam słowa wariacja. Istnieje pomiędzy nimi pewna różnica, polegająca na tym, czy zmieniam kolejność w całym naszym zbiorze, czy wcześniej rozbijam go na mniejsze zbiory. Słowem permutacja określamy zmiany kolejności w całym zbiorze, zaś o wariacjach mówimy wtedy, gdy przed zmianą kolejności rozbijamy nasz zbiór na mniejsze zbiory.\n\n1 Permutacje\nOmówmy to na przykładzie naszego zbioru liter od A do E. Permutacja tego zbioru będzie wyglądała tak:\n\npd.DataFrame(itertools.permutations(Z))\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\n0\nA\nB\nC\nD\nE\n\n\n1\nA\nB\nC\nE\nD\n\n\n2\nA\nB\nD\nC\nE\n\n\n3\nA\nB\nD\nE\nC\n\n\n4\nA\nB\nE\nC\nD\n\n\n...\n...\n...\n...\n...\n...\n\n\n115\nE\nD\nA\nC\nB\n\n\n116\nE\nD\nB\nA\nC\n\n\n117\nE\nD\nB\nC\nA\n\n\n118\nE\nD\nC\nA\nB\n\n\n119\nE\nD\nC\nB\nA\n\n\n\n\n120 rows × 5 columns\n\n\n\nZe zbioru 5 liter możemy zatem wytworzyć 120 zbiorów, każdy z inną kolejnością liter. Policzyć jest to dość łatwo. Mamy 5 miejsc i 5 liter, które możemy tam umieścić: \\(P_5 = \\_ \\times \\_ \\times \\_ \\times \\_ \\times \\_\\). Na pierwszym miejscu możemy umieścić 5 liter: \\(P_5 = 5 \\times \\_ \\times \\_ \\times \\_ \\times \\_\\). Ponieważ jedną literę już zużyliśmy, do drugiego miejsca możemy wsadzić tylko jedną z 4 pozostałych liter: \\(P_5 = 5 \\times 4 \\times \\_ \\times \\_ \\times \\_\\). Uzupełniając nasz schemacik dalej otrzymujemy równanie \\(P_5 = 5\\times 4 \\times 3 \\times 2 \\times 1 = 5! = 120\\). 5! (czyt. pięć silnia) to skrótowy zapis mnożenia liczb od 1 do 5. Powstaje nam z tego wzór na liczbę możliwych permutacji n elementów:\n\\[\nP_n = n!\n\\]\n\n\n2 Wariacje\nCiekawie zaczyna się robić, gdy przed zmianą kolejności chcemy jeszcze rozbić nasz zbiór na mniejsze zbiory. Dla przykładu możemy sobie wyobrazić, że chcemy z naszego zbioru 5 liter wybrać wszystkie możliwe zbiory po 2 litery, np. AB, AC, AD itd. Mamy do dyspozycji mniej miejsca, niż liter w zbiorze. Liczenie czegoś takiego jest analogiczne. Na pierwszym miejscu może pojawić się 1 z 5 liter, na drugim tylko 1 z 4: \\(V^2_5 = 5 \\times 4 = 20\\). Powinno więc istnieć 20 takich zbiorów. Wypiszmy je wszystkie.\n\nvariations(Z, 2)\n\n\n\n\n\n\n\n\n1\n2\n\n\n\n\n1\n(A, B)\n(B, A)\n\n\n2\n(A, C)\n(C, A)\n\n\n3\n(A, D)\n(D, A)\n\n\n4\n(A, E)\n(E, A)\n\n\n5\n(B, C)\n(C, B)\n\n\n6\n(B, D)\n(D, B)\n\n\n7\n(B, E)\n(E, B)\n\n\n8\n(C, D)\n(D, C)\n\n\n9\n(C, E)\n(E, C)\n\n\n10\n(D, E)\n(E, D)\n\n\n\n\n\n\n\nŻeby wyprowadzić wzór na takie wariacje, musimy zwrócić uwagę na fakt, że nasze obliczenie \\(5 \\times 4\\) wygląda jak kawałek silni. Brakuje tylko \\(3 \\times 2 \\times 1\\). Moglibyśmy więc zapisać to w taki sposób:\n\\[\nV^2_5 = 5 \\times 4 = \\frac{5 \\times 4 \\times 3 \\times 2 \\times 1}{3 \\times 2 \\times 1} = \\frac{5!}{3!}\n\\]\nW taki sposób \\(3 \\times 2 \\times 1\\) skróci się i zostanie tylko \\(5 \\times 4\\). Jeśli mielibyśmy 3 miejsca, chcielibyśmy uzyskać \\(5 \\times 4 \\times 3\\), a więc w mianowniku zapisalibyśmy tylko \\(2 \\times 1\\), czyli ostatecznie \\(\\frac{5!}{2!}\\). Powstaje nam z tego następujący wzór na liczbę wariacji n elementów po k elementów (czyli rozbicie w podzbiory po k elementów):\n\\[\nV^k_n = \\frac{n!}{(n-k)!}\n\\]\nSpróbujmy wypisać wariacje naszego zbioru po 3 elementy.\n\nvariations(Z, 3)\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n\n\n\n\n1\n(A, B, C)\n(A, C, B)\n(B, A, C)\n(B, C, A)\n(C, A, B)\n(C, B, A)\n\n\n2\n(A, B, D)\n(A, D, B)\n(B, A, D)\n(B, D, A)\n(D, A, B)\n(D, B, A)\n\n\n3\n(A, B, E)\n(A, E, B)\n(B, A, E)\n(B, E, A)\n(E, A, B)\n(E, B, A)\n\n\n4\n(A, C, D)\n(A, D, C)\n(C, A, D)\n(C, D, A)\n(D, A, C)\n(D, C, A)\n\n\n5\n(A, C, E)\n(A, E, C)\n(C, A, E)\n(C, E, A)\n(E, A, C)\n(E, C, A)\n\n\n6\n(A, D, E)\n(A, E, D)\n(D, A, E)\n(D, E, A)\n(E, A, D)\n(E, D, A)\n\n\n7\n(B, C, D)\n(B, D, C)\n(C, B, D)\n(C, D, B)\n(D, B, C)\n(D, C, B)\n\n\n8\n(B, C, E)\n(B, E, C)\n(C, B, E)\n(C, E, B)\n(E, B, C)\n(E, C, B)\n\n\n9\n(B, D, E)\n(B, E, D)\n(D, B, E)\n(D, E, B)\n(E, B, D)\n(E, D, B)\n\n\n10\n(C, D, E)\n(C, E, D)\n(D, C, E)\n(D, E, C)\n(E, C, D)\n(E, D, C)\n\n\n\n\n\n\n\nTabela jest bardziej rozbudowana, ale wszystko zgadza się z naszymi poprzednimi wnioskami:\n\\[\n\\displaylines{\nV^3_5 = \\frac{5!}{(5-3)!} = \\frac{5!}{2!} = \\\\\n= \\frac{5 \\times 4 \\times 3 \\times 2 \\times 1}{2 \\times 1} = \\\\\n= 5 \\times 4 \\times 3 = 20 \\times 3 = 60\n}\n\\]\nCzyli wariacji po 3 elementy jest w naszym przykładzie 3 razy więcej, niż wariacji po 2 elementy.\n\n\n3 Kombinacje\nMożemy zwrócić uwagę, że tabela powyżej ma ściśle określoną strukturę. W pierwszym wierszu wszystkie podzbiory składają się z literek A, B i C ułożonych na różne sposoby. Można więc powiedzieć, że podzbiory w każdym wierszu są dla siebie permutacjami, bo składają się z tych samych elementów, różnią się tylko kolejnością. Każda kolumna zawiera unikalne zestawy literek. Widzimy więc, że ze zbioru 5 literek możemy wybrać 10 różnych zestawów literek, a w każdym z tych zestawów można ułożyć literki na 6 sposobów, co daje łącznie 60 wariacji. Wariacje możemy więc uzyskać tak, że weźmiemy wszystkie unikalne mniejsze zestawy literek, a potem rozpiszemy permutacje każdego z tych zestawów. Takie unikalne zestawy literek, bez zwracania uwagi na ich kolejność, to kombinacje. W tabeli każdy wiersz to pełny zestaw kombinacji. Wynika nam z tego inny wzór na liczbę wariacji:\n\\[\nV^k_n = C^k_n \\times P_k\n\\]\nSą to w rzeczywistości wymiary naszej tabeli. Liczba kombinacji (tj. unikalnych zestawów) to liczba wierszy, a liczba permutacji to liczba kolumn. Tabela powyżej ma wymiary \\(10 \\times 6\\), bo mamy 10 unikalnych zestawów po 3 elementy i każdy taki zestaw da się ułożyć na 6 różnych sposobów, co ostatecznie daje 60 komórek.\nŻeby wyprowadzić wzór na liczbę kombinacji, możemy wykorzystać fakt, że wiemy, jak się liczy liczbę wariacji i permutacji. W powyższej tabeli mamy 60 wariacji, a każda kombinacja ma 6 możliwych permutacji. Żeby więc pozbyć się informacji o permutacjach, musimy podzielić 60 wariacji na 6. Podstawiając do wzoru:\n\\[\n\\displaylines{\nV^3_5 = C^3_5 \\times P_3 \\\\\n60 = C^3_5 \\times 6\\ |\\div 6 \\\\\nC^3_5 = \\frac{60}{6} = 10\n}\n\\]\nCzyli jeśli mamy tabelę z 6 kolumnami i ilomaś wierszami, która ma 60 komórek, to wierszy musi być 10.\nMożemy do naszego nowego wzoru podstawić wzory na liczbę wariacji i permutacji i w ten sposób uzyskać ogólny wzór na liczbę kombinacji:\n\\[\n\\displaylines{\nV^k_n = C^k_n \\times P_k \\\\\n\\frac{n!}{(n-k)!} = C^k_n \\times k! \\ |\\div k! \\\\\nC^k_n = \\frac{\\frac{n!}{(n-k)!}}{k!} = \\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n}\n\\]\nWzór ten doczekał się nawet własnego symbolu zwanego dwumianem Newtona \\(\\binom{n}{k}\\) (czyt. en nad ka). Dla przykładu liczba kombinacji 5 elementów po 3 elementy oznacza się jako 5 nad 3 i liczy tak:\n\\[\n\\displaylines{\n\\binom{5}{3} = \\frac{5!}{3!(5-3)!} = \\\\\n= \\frac{5!}{3!2!} = \\frac{5 \\times 4 \\times 3 \\times 2 \\times 1}{(3 \\times 2 \\times 1) \\times (2 \\times 1)} = \\\\\n= 10\n}\n\\]\n\n\n4 Powtórzenia\nDo tej pory omówiliśmy wariacje i kombinacje bez powtórzeń. Innymi słowy litera raz użyta nie mogła zostać użyta ponownie. Spotykaliśmy zbiory ABC, ale nie spotkaliśmy zbioru AAA. Wariacje i kombinacje mogą pozwalać na takie powtórzenia. Wariacje możemy policzyć jak zawsze kreskami. W zbiorze Z mamy 5 liter i chcemy zrobić z niego podzbiory po 2 elementy ze zwracaniem (czyli po wylosowaniu wraca do puli, czyli z powtórzeniami). Na pierwszym miejscu może być 5 liter, ale na drugim miejscu także może być 5 liter, bo litery się nie zużywają. Wychodzi nam więc takie działanie:\n\\[\n\\bar{V}^2_5 = 5 \\times 5 = 5^2 = 25\n\\]\nWychodzi nam z tego prosty wzór na liczbę wariacji n elementów po k elementów z powtórzeniami:\n\\[\n\\bar{V}^k_n = n^k\n\\]\nWzór na kombinacje z powtórzeniami podaję raczej pro forma, bo rzadko jest używany.\n\\[\n\\bar{C}^k_n = \\binom{k+n-1}{k} = \\frac{(k+n-1)!}{k!(n-1)!}\n\\]\n\n\n5 Podsumowanie\nPermutacje to zmiany kolejności, kombinacje to unikalne podzbiory. Jeśli zaczniemy robić permutacje unikalnych podzbiorów, wyjdą nam wariacje. Albo patrząc inaczej – permutacje to wariacje \\(V^n_n\\). Permutacje i wariacje możemy liczyć kreskami i silnią. Liczbę kombinacji uzyskamy dzieląc liczbę wariacji po k elementów przez liczbę permutacji k. Powstały wzór oznacza się symbolem Newtona \\(\\binom{n}{k}\\). Pomocny może okazać się poniższy schemat.\n\n\n\nKod\n```{mermaid}\n%%| fig-responsive: true\n%%| fig-width: 80%\n%%| code-fold: true\n%%| code-summary: \"Pokaż kod\"\nflowchart TD\n    START(START) --&gt; zbior\n    zbior[/Mam zbiór, z którym chcę coś zrobić/] --&gt;\n    kolejnosc{Czy kolejność ma znaczenie?}\n    kolejnosc --&gt;|Nie| C[kombinacja]\n        C --&gt; C_powtorzenia{Czy elementy mogą się powtarzać?}\n        C_powtorzenia --&gt;|Tak| C_powtorzenia_koniec(kombinacja z powtórzeniami)\n        C_powtorzenia --&gt;|Nie| C_koniec(kombinacja bez powrótrzeń)\n    kolejnosc --&gt;|Tak| V[wariacja]\n        V --&gt; V_calosc{Czy wykorzystuję cały zbiór?}\n        V_calosc --&gt;|Tak| P(permutacja)\n        V_calosc --&gt;|Nie| V_powtorzenia{Czy elementy mogą się powtarzać?}\n        V_powtorzenia --&gt;|Tak| V_powtorzenia_koniec(wariacja z powtórzeniami)\n        V_powtorzenia --&gt;|Nie| V_koniec(wariacja bez powrótrzeń)\n```\n\n\n\n\nflowchart TD\n    START(START) --&gt; zbior\n    zbior[/Mam zbiór, z którym chcę coś zrobić/] --&gt;\n    kolejnosc{Czy kolejność ma znaczenie?}\n    kolejnosc --&gt;|Nie| C[kombinacja]\n        C --&gt; C_powtorzenia{Czy elementy mogą się powtarzać?}\n        C_powtorzenia --&gt;|Tak| C_powtorzenia_koniec(kombinacja z powtórzeniami)\n        C_powtorzenia --&gt;|Nie| C_koniec(kombinacja bez powrótrzeń)\n    kolejnosc --&gt;|Tak| V[wariacja]\n        V --&gt; V_calosc{Czy wykorzystuję cały zbiór?}\n        V_calosc --&gt;|Tak| P(permutacja)\n        V_calosc --&gt;|Nie| V_powtorzenia{Czy elementy mogą się powtarzać?}\n        V_powtorzenia --&gt;|Tak| V_powtorzenia_koniec(wariacja z powtórzeniami)\n        V_powtorzenia --&gt;|Nie| V_koniec(wariacja bez powrótrzeń)\n\n\n\n\n\nTen utwór jest dostępny na licencji Creative Commons Uznanie autorstwa-Użycie niekomercyjne-Na tych samych warunkach 4.0 Międzynarodowe."
  },
  {
    "objectID": "posts/ai-research.html",
    "href": "posts/ai-research.html",
    "title": "Lepiej niż ChatGPT",
    "section": "",
    "text": "Jeśli jesteśmy studentami, naukowcami, popularyzatorami, musimy często przedzierać się przez morze publikacji naukowych. Zazwyczaj w tym celu korzystamy z Google Scholar, który szuka słów kluczowych w tytułach i abstraktach. Dobranie słów kluczowych tak, żeby uzyskać to i tylko to, czego potrzebujemy, to sztuka sama w sobie.\nZałóżmy, że zauważyliśmy któregoś ranka, że lepiej się czujemy, jeśli rano wystawimy się na jasne światło. Może nie dosłownie w chwili przebudzenia, ale że nasz nastrój jest lepszy, jak rano jest jasno. Być może tak jest? Może to ewolucyjne dostosowanie do tego, żeby nie wychodzić w deszcz, bo łatwo sobie coś zrobić? Jednak czy ktoś to sprawdził? Otwieramy więc podstawowe narzędzie naukowców, Google Scholar, i puszczamy nasze zapytanie. Ponieważ jest to zwykła wyszukiwarka, musimy przerobić nasze zapytanie na słowa kluczowe. To właśnie umiejętność dobrania skutecznego zapytania jest najtrudniejsza w opanowaniu.\nWyszukujemy więc coś w rodzaju „morning sunlight mood” I nagle się okazuje, że znalezione artykuły nie mówią o tym, czego chcemy się dowiedzieć. Jeśli w ogóle mówią o świetle słonecznym i nastroju, to w chorobie dwubiegunowej, którą większość ludzi nie jest dotknięta. Większość artykułów jest też bardzo stara. Musimy więc przeformułować nasze zapytanie, może zawęzić zakres dat i szukać dalej. W końcu znajdziemy interesujący nas artykuł i śledząc cytowania (w tej pracy, ale też artykuły cytujące znaleziony przez nas artykuł) możemy dotrzeć do bogactwa literatury na ten temat. Tak to się robiło do tej pory.\n\n1 ChatGPT\nNagle na scenę wchodzi ChatGPT. Mamy do swojej dyspozycji potężne narzędzie, które zna wszystkie artykuły naukowe XXI wieku. Może jak mu zadamy odpowiednie pytanie, to sam nam podsunie artykuły? Nie zaszkodzi spróbować! Jeśli to zadziała, nagle nasza praca może stać się po wielokroć prostsza.\nOkazuje się, że ChatGPT potrafi wskazać takie artykuły, ale korzystanie z niego jest opatrzone ryzykiem. Jeśli wskazane przez algorytm artykuły sprawdzimy, niechybnie okaże się, że co najmniej część z nich nie istnieje. Jest to zjawisko zwane halucynacjami sztucznej inteligencji. Ponieważ GPT to tylko model językowy, nie potrafi odróżnić istniejących artykułów od nieistniejących. Chce nam odpowiedzieć na pytanie, więc generuje odpowiedź. Z doświadczenia mogę powiedzieć, że szansa na podanie prawdziwych artykułów rośnie, jeśli poprosimy ChatGPT o dokładne cytowania. Jednak wyniki ciągle nie są zachwycające. Z jednego prostego powodu – ChatGPT nie służy do tego.\n\n\n2 Consensus\nNa fali popularności AI powstało mnóstwo narzędzi o różnym, szerszym lub węższym zastosowaniu. Na przykład istnieje rebbe.io, czyli internetowy rabin oparty o sztuczną inteligencję, któremu możemy zadawać pytania o Halachę, Torę czy ogólnie o judaizm1. Co prawda Żydów jest prawie dwa razy więcej niż naukowców, ale skoro Żydzi mają chatbota zaprojektowanego specjalnie do swoich celów, można podejrzewać, że naukowcy też mają coś bardziej specyficznego niż ChatGPT. Jednym z takich narzędzi jest Consensus.\n\nConsensus to darmowa aplikacja (albo raczej strona internetowa), która wykorzystuje modele językowe jak GPT do odnajdywania artykułów naukowych na zadany temat. Wykorzystanie AI pozwala na zadanie pytania w sposób naturalnym językiem, co pozwala uniknąć godzin szukania odpowiednich słów. Co jednak ciekawe, Consensus nie tylko znajduje artykuły, ale też konkretne fragmenty tych artykułów, które sztuczna inteligencja uznaje za potencjalnie użyteczne.\nDodatkowo jeśli zadajemy konkretne pytanie, Consensus podsumuje nam kilka najważniejszych artykułów. Jeśli zadaliśmy pytanie typu tak/nie, Consensus poda nam też (o ile będzie w stanie) jaka część znalezionych przez niego artykułów popiera naszą tezę, a jaka część jej nie popiera.\n\n\n3 Elicit\nKolejnym darmowym narzędziem AI dla naukowców jest Elicit. Ponownie możemy zadać pytanie naturalnym językiem i ponownie dostaniemy w odpowiedzi listę artykułów. Tym razem jednak na pierwszy plan nie będą wysuwane cytaty z prac, ale jednozdaniowe podsumowania abstraktów.\n\nNajwiększa moc Elicit pojawia się jednak w panelu po lewej stronie. AI generuje nam podsumowanie kilku najważniejszych arykułów wraz z cytowaniami, ale także pozwala nam wyciągać konkretne informacje z artykułów. Dla przykładu na obrazku widać, jak poprosiłem o wypisanie liczby osób badanych. Co ciekawe, poza parametrami z listy możemy też wpisywać własne parametry, które AI wyszuka dla nas w artykułach i pokaże w tabeli.\n\n\n4 scite_\nKolejne narzędzie ma tę wadę, że swoje kosztuje, co warto podkreślić w pierwszym zdaniu. Jednak nie cała aplikacja jest płatna i część jej funkcji jest dostępna za darmo. Pełną wersję możemy uzyskać za cenę ok. 380 zł rocznie2 (lub ok. 50 zł miesięcznie). Nie będę tutaj opisywał wszystkich funkcji scite_, bo tak nazywa się nasza kolejna aplikacja, ale skupię się na dwóch kluczowych.\nPo pierwsze, scite_ posiada dodatek do przeglądarki, który wzbogaca nasze wyszukiwanie w Google Scholar o tabelkę z dodatkowymi informacjami o cytowaniach. O ile liczba cytowań to nic wielkiego, już sam Scholar podaje nam łączną liczbę cytowań danego artykułu, o tyle scite_ dodatkowo podzieli nam te cytowania na popierające tezę danego artykułu oraz wchodzące z nią w polemikę. Dzięki temu możemy oszacować rzeczywisty wpływ danej pracy na środowisko naukowe, czego przecież miarą są cytowania.\n\nPo drugie od jakiegoś czasu mamy do naszej dyspozycji scite assistant, czyli właściwie to, o co nam chodziło od początku – ChatGPT do rozmowy o artykułach naukowych. Naturalnie scite assistant artykułów nie wymyśla, każda informacja ma przypis, a przypisy są zebrane we wspólną tabelę. W przeciwieństwie do poprzednich narzędzi, z asystentem możemy toczyć dialog, jak na prawdziwego chatbota przystało. Ważną jego cechą jest też to, że potrafi wskazać, że badań na jakiś temat nie ma (w jego bazie). Szczególnie użyteczne może być to dla naukowców, którzy w swoich badaniach chcą wytwarzać nową wiedzę.\n\n\n\n5 ResearchRabbit\nZgoła innym rodzajem narzędzia jest ResearchRabbit. Nie jest to wyszukiwarka, ale raczej organizer do bibliografii. Jeśli mamy już zgromadzoną bibliografię na dany temat, ResearchRabbit może nam zwizualizować połączenia między artykułami, wyciągnąć często przewijających się autorów, ale przede wszystkim podsunąć nam późniejsze, wcześniejsze lub podobne artykuły. Dzięki temu nasza baza wiedzy może rosnąć, powiększać się o coraz to nowe wątki, podobne do tego, co nas interesuje, ale także możemy dostrzec powiązania między naszą dziedziną, a innymi, może bardziej odległymi dziedzinami.\n\nNarzędzie jest darmowe i o tyle wygodne, że można je połączyć z Zotero, który jest menedżerem bibliografii z wyboru. Nie musimy się więc rozdrabniać i trzymać naszej bibliografii w kilku miejscach.\n\n\n6 ChatPDF\nWbrew obiegowej opinii, znalezione artykuły wypada przeczytać. Czasami jednak nie potrzebujemy dokładnie znać całości artykułu, a wystarczą nam konkretne informacje. Możemy ich szukać tradycyjnie, ale możemy też wykorzystać ChatPDF. Jest to narzędzie, do którego możemy wgrać plik .pdf z naszym artykułem i zacząć zadawać pytania. Dzienny darmowy limit to 3 artykuły.\n\n\n\n7 Trinka\nOstatnie narzędzie, które chciałbym omówić, nie tyle służy do znajdowania artykułów, ile do pisania własnych. Język artykułów naukowych to rządząca się specyficznymi regułami odmiana języka angielskiego3. Łatwo jest nie dotrzymać wszystkich form tego języka. Trinka to aplikacja, która może nam w tym pomóc. Sprawdza nam gotowy tekst, wskazuje na ewentualne błędy i sugeruje alternatywne formy. Darmowy plan pozwala na sprawdzenie 10 000 słów miesięcznie. Wersja premium to koszt $ 80 rocznie, w ramach których otrzymujemy przede wszystkim brak limitu słów oraz dodatek do Worda.\n\n\n8 Podziękowanie\nWpis opracowałem m.in. na podstawie tweetów dr. Mushtaqa Bilala.\n\n\n\n\n\nPrzypisy\n\n\nZapytany o przepisywanie AI Rabbi odpowiada:\n\nAccording to Torah sources, there is no direct mention of morning sunlight improving mood. However, there are teachings that emphasize the importance of starting the day with positive thoughts and actions. The Talmud states that one should wake up with a joyful heart and begin the day with gratitude and praise to God. Additionally, the Shulchan Aruch, a code of Jewish law, recommends reciting morning blessings upon waking up, which express gratitude for the gift of life and the ability to perform daily tasks.\nWhile there may not be a direct correlation between morning sunlight and mood, starting the day with positive thoughts and actions can certainly contribute to a more positive outlook and overall well-being. It is also important to note that exposure to natural light, including sunlight, has been shown to have a positive impact on mood and energy levels, so it is possible that morning sunlight could have a beneficial effect in this regard.\n\nTrzeba przyznać, że jest to sensowna odpowiedź.↩︎\nGdy to piszę, co jakiś czas strona proponuje 40% zniżki, co zmniejsza cenę za pierwszy rok do ok. 230 zł.↩︎\nDobry kurs specyfiki pisania w nauce od dr Kristin Sainani z Uniwersytetu Stanforda znajdziemy tutaj.↩︎"
  },
  {
    "objectID": "posts/funkcje.html",
    "href": "posts/funkcje.html",
    "title": "Funkcje matematyczne",
    "section": "",
    "text": "Czy masz czasem wrażenie, że rzeczy w matematyce nazywają się bez sensu? To jest bardzo słuszne wrażenie. Dziś na warsztat weźmiemy funkcje, które są zmorą uczniów liceów ze względu na nową, dziwną notację typu \\(f(x)\\), cały pozornie chaotyczny język, który odrywa funkcje od reszty matematyki, a także wrażenie, że nijak nie idzie tego spotkać w „prawdziwym życiu”, cokolwiek to jest. Ja właśnie spędzam wieczór z prawdziwego życia na pisaniu o funkcjach. Do czegoś mi się jednak przydały. Zaczynając jednak od nazwy – dlaczego „funkcja”? Bo Leibniz źle rozumiał to słowo. A że raz przyjęte konwencje trzymają się mocno, to do dzisiaj, właściwie przypadkowo, nazywamy tę rzeczy funkcjami."
  },
  {
    "objectID": "posts/funkcje.html#sec-programming",
    "href": "posts/funkcje.html#sec-programming",
    "title": "Funkcje matematyczne",
    "section": "3.1 R i Python",
    "text": "3.1 R i Python\nWszystkie maszyny, które zapisaliśmy w poprzednim zadaniu, moglibyśmy rzeczywiście stworzyć. Może nie w formie pudełka przerabiającego blaszki, ale możemy łatwo je zaprogramować. Nie jest to konieczne do zrozumienia funkcji, ale bardzo polecam, żeby chociaż spróbować. Wczesne zainteresowanie programowaniem mnie osobiście mocno pomogło w późniejszej nauce matematyki.\nKażdy język programowania pozwala nam stworzyć własne maszyny. Jeśli nie pozwala, to śmiem wątpić, czy w ogóle możemy mówić o języku programowania1. W naszym wypadku użyjemy jednego z dwóch języków programowania powszechnie używanych w nauce – R i Python. Oba możemy uruchomić w przeglądarce. Jeśli czytasz to zamierzając uczyć się statystyki, polecam wybrać R2, a jeśli nie, to Python może być nieco bardziej przejrzysty.\nJak więc rzeczywiście stworzyć naszą maszynę? W Pythonie wykorzystamy następującą składnię:\n\n\n\nPython\n\ndef nazwa_funkcji(to_co_wrzucamy):\n  return działanie funkcji\n\n\nW R składnia wygląda tak:\n\n\n\nR\n\nnazwa_funkcji &lt;- function(to_co_wrzucamy) {\n    działanie funkcji\n}\n\n\nMożemy wykorzystać oznaczenia, które wcześniej wprowadziliśmy i stworzyć naszą maszynę dodającą 3:\n\n\n\nPython\n\ndef m(b):\n  return b + 3\n\n\n\n\n\nR\n\nm &lt;- function(b) {\n  b + 3\n}\n\n\nGdzie to wpisać? Jeśli, za moją sugestią, korzystasz z Programiz, wpisz to w oknie po lewej stronie i kliknij guzik Run. Od teraz możesz po stronie prawej wywołać swoją funkcję. Jeśli wpiszesz np. m(5) i klikniesz enter, funkcja rzeczywiście wypluje z siebie wynik, czyli 8.\nMożesz w ten sposób stworzyć dowolną liczbę maszyn, ale jeśli piszesz w Pythonie, uważaj na wcięcia. Jeśli tworzysz jedną maszynę pod drugą, słowo def drugiej maszyny nie może być poprzedzone spacjami.\n\n\n\n\n\n\n\n\nZadanie\n\n\n\nSpróbuj stworzyć maszyny z poprzedniego zadania w wybranym języku programowania.\n\n\\(c(l) = l + 4\\)\n\\(t(l) = 3 l\\)\n\\(d(l) = 2 l + 1\\)\n\\(j(l) = (l + 1) \\times 2\\) lub po uproszczeniu \\(j(l) = 2 l + 2\\)\n\\(s(l) = \\frac{l}{7}\\)\n\\(p(l) = 5\\)\n\n\n\n\n\n\n\n\n\nOdpowiedź\n\n\n\n\n\n\n\\(c(l) = l + 4\\)\n\n\n\n\nPython\n\ndef c(l):\n  return l + 4\n\n\n\n\n\nR\n\ncztery &lt;- function(l) { # w R jest już funkcja c(), więc trzeba zmienić nazwę\n  l + 4\n}\n\n\n\n\\(t(l) = 3 l\\)\n\n\n\n\nPython\n\ndef t(l):\n  return 3 * l\n\n\n\n\n\nR\n\ntrzy &lt;- function(l) { # w R jest już funkcja t(), więc trzeba zmienić nazwę\n  3 * l\n}\n\n\n\n\\(d(l) = 2 l + 1\\)\n\n\n\n\nPython\n\ndef d(l):\n  return 2 * l + 1\n\n\n\n\n\nR\n\nd &lt;- function(l) {\n  2 * l + 1\n}\n\n\n\n\\(j(l) = (l + 1) \\times 2\\) lub po uproszczeniu \\(j(l) = 2 l + 2\\)\n\n\n\n\nPython\n\ndef j(l):\n  return (l + 1) * 2\n\n# lub po uproszczeniu\n\ndef j(l):\n  return 2 * l + 2\n\n\n\n\n\nR\n\nj &lt;- function(l) {\n  (l + 1) * 2\n}\n\n# lub po uproszczeniu\n\nj &lt;- function(l) {\n  2 * l + 2\n}\n\n\n\n\\(s(l) = \\frac{l}{7}\\)\n\n\n\n\nPython\n\ndef s(l):\n  return l / 7\n\n\n\n\n\nR\n\ns &lt;- function(l) {\n  l / 7\n}\n\n\n\n\\(p(l) = 5\\)\n\n\n\n\nPython\n\ndef p(l):\n  return 5\n\n\n\n\n\nR\n\np &lt;- function(l) {\n  5\n}"
  },
  {
    "objectID": "posts/funkcje.html#punkty",
    "href": "posts/funkcje.html#punkty",
    "title": "Funkcje matematyczne",
    "section": "7.1 Punkty",
    "text": "7.1 Punkty\nPunkt to miejsce na wykresie opisywane przez parę liczb – jego współrzędne. Punkty zaznaczamy tam, gdzie argument przecina się z wartością. Para liczb typu \\((2, 3)\\) oznacza więc, że nasz punkt jest dwa oczka w prawo i trzy oczka do góry w układzie współrzędnych. Jeżeli nasz punkt leży na linii wykresu, mówimy, że punkt należy do wykresu funkcji. To jest opis graficzny, możemy zobaczyć to na wykresie. Co to znaczy w języku algebraicznym, czyli w języku maszyn?\nMaszyny nie mają osi ani linii wykresu, one tylko przyjmują i wyrzucają liczby. Nie zapominajmy jednak, co oznaczają współrzędne. Pierwsza współrzędna to współrzędna pozioma (\\(x\\)), a druga to współrzędna pionowa (\\(y\\)). Współrzędne poziome oznaczają to, co wrzucam do funkcji, a pionowe to, co funkcja z siebie wypluwa. Stwierdzenie „Punkt \\((2, 3)\\) należy do wykresu funkcji.” oznacza ni mniej, ni więcej, jak „Jeżeli wrzucę do funkcji \\(2\\), to wypluje ona \\(3\\).” albo matematycznie \\(f(2) = 3\\).\nJeżeli spotkasz się z zadaniem typu „Sprawdź, czy punkt \\((5, 11)\\) należy do wykresu funkcji \\(y = 2x + 1\\)?“, to możesz to sobie przetłumaczyć z graficznego na algebraiczny – sprawdź, czy jak do funkcji \\(y = 2x + 1\\) wrzucisz \\(5\\), to wyjdzie ci \\(11\\).\n\n\n\n\n\n\nZadanie\n\n\n\n\nCzy punkt \\((21, 7)\\) należy do wykresu funkcji \\(y = \\frac{x}{3}\\)?\nPunkt \\(A\\) ma współrzędną \\(x\\) równą \\(7\\) i należy do wykresu funkcji \\(f(x) = 3x - 10\\). Znajdź drugą współrzędną punktu \\(A\\).3\nPunkt \\(B\\) ma współrzędną \\(y\\) równą 0 i należy do wykresu funkcji \\(y = -2x + 20\\). Znajdź pierwszą współrzędną punktu \\(B\\).\n\n\n\n\n\n\n\n\n\nOdpowiedź\n\n\n\n\n\n\nTak.\n\\(f(7) = 3 \\times 7 - 10 = 11\\), więc \\(A(7, 11)\\).\n\\(f(10) = -2 \\times 10 + 20 = 0\\), więc \\(B(10, 0)\\)."
  },
  {
    "objectID": "posts/funkcje.html#przecięcia",
    "href": "posts/funkcje.html#przecięcia",
    "title": "Funkcje matematyczne",
    "section": "7.2 Przecięcia",
    "text": "7.2 Przecięcia\nBardzo często w zadaniach jesteśmy proszeni o wyznaczenie punktów przecięcia wykresu funkcji z osią X, rzadziej z osią Y. Co to znaczy w języku algebraicznym?"
  },
  {
    "objectID": "posts/funkcje.html#przecięcie-z-osią-x",
    "href": "posts/funkcje.html#przecięcie-z-osią-x",
    "title": "Funkcje matematyczne",
    "section": "7.3 Przecięcie z osią X",
    "text": "7.3 Przecięcie z osią X\nSpójrz na ten wykres i powiedz, co wspólnego mają wszystkie te punkty?\n\nWszystkie leżą na osi X, to prawda. To jest opis w języku graficznym, gdzie istnieją punkty rozlokowane w przestrzeni. Teraz chcemy jednak przejść z opisu graficznego do algebraicznego, a punktem styku między jednym i drugim są współrzędne. Jaką współrzędną mają wspólną wszystkie te punkty? Po przeanalizowaniu wykresu dochodzimy do wniosku, że każdy z naszych punktów ma współrzędną \\(y = 0\\). Różnią się natomiast współrzędnymi \\(x\\).\nJeśli jesteśmy pytani o punkt przecięcia z osią X, innymi słowy jesteśmy pytani o takie miejsce, gdzie \\(y = 0\\). Takim miejscem dla narysowanej na wykresie maszyny jest punkt o współrzędnych \\((-1, 0)\\). Jeśli sięgniesz pamięcią do punktu 4., zorientujesz się, że mamy specjalną nazwę na punkty, gdzie \\(y\\) (czyli wartość funkcji) wynosi \\(0\\) – miejsce zerowe. Pytanie o punkt przecięcia z osią X jest więc tak naprawdę znanym nam już pytaniem o miejsce zerowe, gdzie ewentualnie odpowiedź będzie trzeba zapisać jako \\(A(-1, 0)\\) zamiast \\(f(-1) = 0\\)."
  },
  {
    "objectID": "posts/funkcje.html#przecięcie-z-osią-y",
    "href": "posts/funkcje.html#przecięcie-z-osią-y",
    "title": "Funkcje matematyczne",
    "section": "7.4 Przecięcie z osią Y",
    "text": "7.4 Przecięcie z osią Y\nPodobne pytanie o wspólną współrzędną możemy sobie zadać dla punktów na osi Y.\n\nTym razem pewnie szybciej możemy dojść do wniosku, że wszystkie zaznaczone punkty mają wspólną współrzędną \\(x\\), a różnią się współrzędnymi \\(y\\). Funkcja zaznaczona na rysunku ma punkt przecięcia z osią Y o współrzędnych \\((0, 1)\\). Pytanie o punkt przecięcia z osią Y jest więc niczym innym, jak pytaniem o wartość funkcji w zerze, czyli \\(f(0)\\)."
  },
  {
    "objectID": "posts/funkcje.html#wyraz-wolny",
    "href": "posts/funkcje.html#wyraz-wolny",
    "title": "Funkcje matematyczne",
    "section": "8.1 Wyraz wolny",
    "text": "8.1 Wyraz wolny\nZaczniemy od, być może, mniej istotnego i rewolucyjnego, ale który łatwiej jest zobaczyć. Narysujmy sobie 5 kolejnych funkcji: \\(f(x) = x + 2\\), \\(g(x) = x + 1\\), \\(h(x) = x\\), \\(i(x) = x - 1\\), \\(j(x) = x - 2\\).\n\n\n\nCzerwony – \\(f(x)\\); pomarańczowy – \\(g(x)\\); zielony – \\(h(x)\\); niebieski – \\(i(x)\\); fioletowy – \\(j(x)\\).\n\n\nJeśli teraz porównamy te funkcje, to możemy dojść do wniosku, że są to równoległe linie (tak samo nachylone), ale niektóre są wyżej, inne niżej. Dodawanie do \\(x\\) podnosi wykres, a odejmowanie go obniża. Najłatwiej skupić się tutaj na wartości w zerze, czyli na przecięciu z osią Y. Widzimy, że czerwona linia jest najwyżej, a każda kolejna o oczko niżej. Ten punkt, wartość w zerze, jest o tyle ciekawy, że jak spróbujemy podstawić 0 do funkcji, to \\(x\\) nam znika i zostaje tylko to, co dodaliśmy, lub odjęliśmy. \\(f(0) = 0 + 2 = 2\\).\nWbrew pozorom to bardzo ważny wniosek. Pozwala on bardzo szybko określić, co do \\(x\\) było dodane, albo od niego odjęte. Spróbuj to zrobić na podstawie poniższej ryciny.\n\nMożemy iść taką logiką – skoro \\(x\\) nie ma żadnego wpływu na wartość funkcji, jeśli wynosi 0 (bo zostaje tylko to, co dodajemy lub odejmujemy), to wystarczy spojrzeć na \\(f(0)\\). Tutaj wynosi ta wartość \\(1,5\\) i rzeczywiście, ta funkcja to \\(f(x) = x + 1,5\\). Nazywamy to wyraz wolny (ang. intercept) i najłatwiej go odczytać właśnie z przecięcia z osią Y. Wyraz wolny zawsze ma wartość równą \\(f(0)\\). Wyraz wolny decyduje o tym, czy wykres funkcji jest wyżej, czy niżej."
  },
  {
    "objectID": "posts/funkcje.html#nachylenie",
    "href": "posts/funkcje.html#nachylenie",
    "title": "Funkcje matematyczne",
    "section": "8.2 Nachylenie",
    "text": "8.2 Nachylenie\nDrugi współczynnik trudniej jest zrozumieć, dlatego warto się tu skupić mocniej. Naszkicujmy 5 funkcji – \\(f(x) = 2x\\), \\(g(x) = 1x\\), \\(h(x) = 0x\\), \\(i(x) = -1x\\), \\(j(x) = -2x\\).\n\n\n\nCzerwony – \\(f(x)\\); pomarańczowy – \\(g(x)\\); zielony – \\(h(x)\\); niebieski – \\(i(x)\\); fioletowy – \\(j(x)\\).\n\n\nŻeby się nie zgubić, porównajmy na początek funkcje zaznaczone na czerwono i pomarańczowo. Mam pytanie. Jeśli byłyby to schody, to po których wolał(a)byś wchodzić? Jeśli nie szukasz wszędzie okazji do fitnessu (to pisząc sięgam po kolejnego wafla z kremem orzechowym), to pewnie wybierzesz schody pomarańczowe. Są bowiem mniej strome. Innymi słowy mają mniejsze nachylenie.\nPorównajmy teraz obie te funkcje do funkcji zaznaczonej na zielono. Zielona funkcja jest płaska. Ma zerowe nachylenie. Nie ma się co dziwić, po uproszczeniu jej wzór to \\(h(x) = 0\\) – niezależnie, co wrzucimy, wyrzuci ona 0. Jej wartość jest stała. Następnie przechodzimy do funkcji niebieskiej i fioletowej – co do obu możemy zauważyć, że im bardziej przesuwamy palcem w prawo, tym niżej jesteśmy. Obie te funkcje są malejące, ale funkcja fioletowa maleje szybciej.\nZauważmy jednak, że wszystkie te funkcje przechodzą przez \\((0, 0)\\). Nachylenie nie ma więc związku z tym, czy wykres jest przesunięty w górę, czy w dół. To, jak już ustaliliśmy, kontroluje wyraz wolny. Rampa jest tak samo stroma na pokładzie samolotu i w piekle.\nNachylenie kontrolowane jest przez tzw. współczynnik kierunkowy, czyli liczbę, przez którą mnożę \\(x\\). Im ta liczba większa, tym bardziej stromy wykres. Jeśli ta liczba jest ujemna, funkcja jest malejąca."
  },
  {
    "objectID": "posts/funkcje.html#wzór-ogólny",
    "href": "posts/funkcje.html#wzór-ogólny",
    "title": "Funkcje matematyczne",
    "section": "8.3 Wzór ogólny",
    "text": "8.3 Wzór ogólny\nUstaliliśmy więc, że funkcje liniowe kontrolowane są przez dwie wartości:\n\nWspółczynnik kierunkowy, czyli liczbę, przez którą mnożę \\(x\\) – kontroluje nachylenie.\nWyraz wolny, czyli liczbę, którą dodaję do \\(x\\) – kontroluje położenie w pionie.\n\nJeśli porwiemy się na to szaleństwo i połączymy mnożenie z dodawaniem, to otrzymamy wzór ogólny funkcji liniowej:\n\\[\nf(x) = ax + b\n\\]\nOczywiście oznaczenia, jak zawsze, są umowne. Równie dobrze mogę napisać \\(m(l) = sl + w\\) albo nawet \\(m(l) = sl + m(0)\\), bo \\(m\\) to maszyna, \\(l\\) to liczba, \\(s\\) to stromizna, a \\(w\\) to wyraz wolny, który zawsze ma wartość równą wartości funkcji w zerze. Niezależnie, jakie literki sobie wybierzemy, funkcje, które da się przedstawić jako mnożenie \\(x\\) i dodawanie (lub odejmowanie4), będą miały kształt prostej linii. W standardowej notacji literką \\(a\\) oznacza się współczynnik kierunkowy, zaś literką \\(b\\) wyraz wolny.\nStatystyczny spoiler: linie regresji to proste linie, dlatego one również mają taką postać. Zazwyczaj zapisywane są upiornie wyglądającą notacją typu \\(\\beta_0 + \\beta_1 X\\), ale to cały czas to samo, tylko w odwrotnej kolejności i literkami greckimi. Nie dajcie się zwieść, to jest tylko po to, żeby Was przestraszyć."
  },
  {
    "objectID": "posts/rozklady-probkowania.html",
    "href": "posts/rozklady-probkowania.html",
    "title": "Rozkłady próbkowania i rozkłady \\(t\\)",
    "section": "",
    "text": "Sięgnijmy pamięcią do przykładu z tekstu o wartości \\(p\\). Na tym tekście będziemy budować, więc zachęcam, żeby z nim zapoznać się w pierwszej kolejności. Konieczne jest też rozumienie podstaw rozkładu normalnego. Jeśli nie potrafisz obliczyć, powiedzmy, jaka część populacji ma inteligencję między 85 a 115, zachęcam do nadrobienia tekstu o rozkładach normalnych. Nie jest to niezbędne do zrozumienia większości tego tekstu, ale bardzo pomoże.\nSprawdzaliśmy tam między innymi kontrowersyjną (i fałszywą) hipotezę, że kobiety są mniej inteligentne od mężczyzn. Zbadaliśmy 100 kobiet i 100 mężczyzn i wyszło nam, że tak istotnie jest – mężczyźni osiągnęli średnią 101, kobiety 99. Wiedzeni nieufnością powtórzyliśmy nasze badanie i – ku naszemu zdumieniu – tym razem kobiety osiągnęły średnią 101 a mężczyźni 98. Po głębszym zastanowieniu możemy stwierdzić, że nie ma w tym nic dziwnego. W końcu całkowicie losowo mogę trafić na bardziej lub mniej inteligentną próbkę mężczyzn. Jakie jednak mogę mieć zaufanie do swoich wyników? Jak to, że moi mężczyźni osiągnęli średnią 98 ma się do rzeczywistej średniej wszystkich mężczyzn? Czy muszę zbadać ich wszystkich, żeby mieć pewność? To zgłębimy tutaj.\nCały sens statystyki polega na tym, żeby być w stanie pobrać próbkę i na jej podstawie powiedzieć coś na temat populacji, z której ją pobraliśmy (Wackerly, Mendenhall, & Scheaffer, 2008). Nie muszę więc badać wszystkich mężczyzn świata, żeby coś o nich (jako o zbiorowości) powiedzieć. Statystyka jest jednak lepsza od chłopskiego rozumu nie tylko w tym, że daje bardziej obiektywne dane, niż chłopski rozum (wbrew temu, co mogliśmy słyszeć od wujków przy rodzinnym stole), ale też pozwala powiedzieć, jak bardzo pewni możemy być naszych wniosków. Kluczowe pytanie na ten moment będzie więc brzmiało – skoro w mojej próbce 100 mężczyzn wyszła średnia 98, to jak to ma się do średniej inteligencji wszystkich mężczyzn?"
  },
  {
    "objectID": "posts/rozklady-probkowania.html#normalność",
    "href": "posts/rozklady-probkowania.html#normalność",
    "title": "Rozkłady próbkowania i rozkłady \\(t\\)",
    "section": "4.1 Normalność",
    "text": "4.1 Normalność\nTwierdzenie to mówi nam o dwóch piekielnie ważnych rzeczach. Po pierwsze, jeśli próba jest odpowiednio duża1, to każdy rozkład próbkowania jest normalny. Odpowiednio duża oznacza tutaj według większości autorów minimum 15 osób, chociaż co bardziej konserwatywni autorzy mówią 20 albo nawet 30 osób. Podkreślam tutaj słowo każdy. Oznacza to, że nieważne, jak wygląda rozkład zmiennej, czy jest normalny, czy nie. Rozkład próbkowania będzie normalny. Wyobraźmy sobie, że rozkład poniżej to rozkład zarobków w pewnym państwie. Większość ludzi zarabia niedużo, ale jest też niewielka ilość ludzi zarabiająca bardzo dużo.\n\n\nKod\nrexp(10000, 0.0007) %&gt;%\n    tibble(income = .) %&gt;%\n    ggplot(aes(income)) +\n    geom_histogram(fill = \"white\", colour = \"gray60\") +\n    labs(x = \"Dochód\", y = \"Liczba przypadków\") +\n    theme_main()\n\n\n\n\n\n\n\n\n\nZ pobieram z tego rozkładu losową próbkę 100 osób. Wychodzi mi średni dochód 1185. Spoko. Pobieram kolejne 100 osób i tym razem dostaję średni dochód 1448. Robię tak jeszcze bardzo dużo razy, więc ostatecznie mam zapisane w Excelu bardzo dużo średnich. Gdy z tych średnich, nie z pojedynczych obserwacji, zrobię histogram, dostanę coś takiego.\n\n\nKod\nincome &lt;- tibble(income = map_vec(1:1000, \\(x) mean(rexp(100, 0.0007))))\n\nggplot(income, aes(income)) +\n    geom_histogram(fill = \"white\", colour = \"gray60\") +\n    geom_vline(xintercept = mean(income$income)) +\n    geom_vline(xintercept = c(mean(income$income) - sd(income$income), mean(income$income) + sd(income$income)), linetype = \"dashed\") +\n    labs(x = \"Średni dochód\", y = \"Liczba przypadków\") +\n    theme_main()\n\n\n\n\n\n\n\n\n\nTen rozkład wygląda zupełnie inaczej, niż rozkład wyjściowy. Dlaczego? Bo rozkład wyjściowy to rozkład zmiennej, a to na obrazku to rozkład średnich, jakie wychodzą w różnych próbkach, czyli rozkład próbkowania. Wielkość pojedynczej próbki wynosi 100, czyli więcej niż 15, a więc rozkład próbkowania wyszedł normalny. To jest pierwsze, co mówi nam CLT.\nJest to założenie o tyle ważne, że idzie na przekór popularnym w naukowym świecie nieporozumieniom. W części testów (np. w teście \\(t\\)) istnieje założenie o normalności. Ta normalność dotyczy jednak nie rozkładu zmiennej, ale rozkładu próbkowania. Dane wcale nie muszą rozkładać się normalnie! Jeśli mamy te minimum 15-30 osób, to rozkład próbkowania i tak jest normalny na mocy centralnego twierdzenia granicznego. Owszem, możemy sprawdzić, czy dane rozkładają się normalnie, bo jeśli tak, to rozkład próbkowania też jest normalny, ale nawet najbardziej szalone rozkłady zmiennej mają normalne rozkłady próbkowania, o ile liczność próbki jest wystarczająca.\nJeśli czytałeś(-aś) tekst o rozkładach normalnych, możesz się już domyślać, dlaczego to jest ważne. Jeśli wiemy, że rozkład jest normalny, to moglibyśmy powiedzieć, np. „OK, w mojej próbce dzieci karmionych kalarepą średnia inteligencja wynosi wynosi 110. Rozkład próbkowania ma średnią 100 i odchylenie standardowe 5, więc szansa na to, że taka lub wyższa średnia wylosuje się z rozkładu normalnego przez przypadek wynosi \\(\\int_{110}^{\\infty} \\mathcal N(x;\\ 100,\\ 5^2) = 1 - pnorm(110,\\ mean = 100,\\ sd = 5) \\approx 2,3\\%\\), a więc raczej nie jest to przypadek”. Skąd jednak mam wiedzieć, jaką średnią i odchylenie standardowe ma rozkład próbkowania, jak mam tylko jedną próbkę?\n\n\n\n\n\n\nZadanie\n\n\n\nSzybka powtórka z rozkładów normalnych i rozgrzewka w myśleniu o rozkładach próbkowania.\n\nTwoja próbka 40 osób ma średnią 121. Rozkład próbkowania ma średnią 100 i odchylenie standardowe 7. Jaka jest szansa, że próbka 40 osób będzie miała średnią 121 lub wyższą?\nTwoja próbka 120 osób ma średnią -0,78. Rozkład próbkowania ma średnią 0 i odchylenie standardowe 0,5. Jaka jest szansa, że próbka 120 osób będzie miała średnią taką lub niższą?\nTwoja próbka 300 osób ma średnią 5,7. Rozkład próbkowania ma średnią 5,5 i odchylenie standardowe 0,65. Różnica między Twoją średnią a średnią rozkładu próbkowania wynosi 0,2. Jaka jest szansa wylosowania próbki o takim lub większym odchyleniu (czyli mniej niż 5,3 lub więcej niż 5,7)?\n\n\n\n\n\n\n\n\n\nOdpowiedź\n\n\n\n\n\n\n\\(1 - pnorm(121,\\ mean = 100,\\ sd = 7) \\approx 0,13\\%\\)\n\\(pnorm(-0.78,\\ 0,\\ 0.5) \\approx 5,9\\%\\)\n\\(1 - pnorm(5.7,\\ 5.5,\\ 0.65) + pnorm(5.3,\\ 5.5,\\ 0.65) \\approx 75,8\\%\\)"
  },
  {
    "objectID": "posts/rozklady-probkowania.html#błąd-standardowy",
    "href": "posts/rozklady-probkowania.html#błąd-standardowy",
    "title": "Rozkłady próbkowania i rozkłady \\(t\\)",
    "section": "4.2 Błąd standardowy",
    "text": "4.2 Błąd standardowy\nDruga rzecz, na którą pozwala nam CLT to dokładnie rozwiązanie naszego problemu. Mówi nam ono, że możemy oszacować odchylenie standardowe rozkładu próbkowania dzieląc odchylenie standardowe naszej próbki przez pierwiastek z liczby osób badanych.\n\\[\nSE = \\frac{SD}{\\sqrt{N}}\n\\]\nRozszyfrujmy to sobie i podajmy przykład. Skrót \\(SE\\) oznacza błąd standardowy. Jest to specjalna nazwa na odchylenie standardowe rozkładu próbkowania. Pozwala ona odróżnić łatwo odchylenie standardowe rozkładu zmiennej, od odchylenia standardowego rozkładu próbkowania. \\(N\\) to liczba osób w próbce. \\(SD\\) w tym wzorze odnosi się do odchylenia standardowego zmiennej. Jeśli znamy prawdziwe odchylenie standardowe w populacji, to możemy je wykorzystać, ale jeśli nie (czyli zazwyczaj), to możemy je zastąpić odchyleniem standardowym naszej próbki. Jeśli sięgniesz pamięcią do wzoru na odchylenie standardowe, to możesz sobie przypomnieć, że sumę odchyleń podniesionych do kwadratu dzieliliśmy przez \\(N - 1\\) zamiast przez \\(N\\). Jest to takie zabezpieczenie wbudowane we wzór na odchylenie standardowe, dzięki któremu odchylenie standardowe z próby bardziej przypomina to z populacji. Dzięki temu, że dzieliliśmy przez \\(N - 1\\) zamiast przez \\(N\\), teraz możemy podstawiać do wzoru na błąd standardowy odchylenie standardowe z próby.\nZaczęliśmy od wylosowania próbki 100 mężczyzn i zmierzeniu ich inteligencji. Wiemy, że rozkład inteligencji ma średnią 100, a więc rozkład próbkowania również ma średnią 100. Odchylenie standardowe wynosi 15, co również wiemy dlatego, że wiemy odrobinę o inteligencji. Możemy więc oszacować błąd standardowy, czyli odchylenie standardowe rozkładu próbkowania.\n\\[\nSE = \\frac{SD}{\\sqrt{N}} = \\frac{15}{\\sqrt{100}} = 1,5\n\\]\nPrzypomnijmy, że wcześniej zrobiliśmy symulację rozkładu próbkowania, losując 100 próbek po 100 osób i licząc średnie. Tak uzyskany rozkład próbkowania miał odchylenie standardowe 1.45. Jak widzimy, było to w miarę rozsądne oszacowanie, ale jeśli zamiast 100 próbek pobralibyśmy 1000, 10 000 czy 100 000, to nasz symulowany rozkład miałby odchylenie standardowe coraz bliższe 1,5. Poniżej przedstawiam 3 wykresy gęstości, które pokazują, że im więcej próbek włączymy do symulacji, tym rozkład staje się bardziej normalny, a jego odchylenie standardowe bliższe prawdziwemu. Oznacza to, że symulacja symulacją, ale pod spodem czai się jakiś prawdziwy rozkład próbkowania, który ma średnią 100 (równą prawdziwej średniej z populacji) i odchylenie standardowe 1,5.\nW naszym drugim przykładzie z dochodami nie znamy prawdziwego odchylenia standardowego, ale możemy je oszacować. Załóżmy, że wylosowałem 100 osób, zapytałem o dochody i wyszło mi odchylenie standardowe 1753,93. Możemy oszacować błąd standardowy:\n\\[\nSE = \\frac{1753,93}{\\sqrt{100}} \\approx 175\n\\]\nJeśli wiedziałbym, że prawdziwa średnia wynosi, powiedzmy 1600 (ze średnią poradzimy sobie potem), to próbki ze średnią między 1425 i 1775 mogę uznać za zupełnie typowe (ok. 67% przypadków według prawa trzech sigm), zaś próbki ze średnią między 1250 a 1950 za zupełnie typowe (ok. 95% przypadków według prawa trzech sigm). Jeśli dzieci karmione kalarepą osiągałyby średni dochód na poziomie 2000, to miałbym mniej niż 5% szans, że ten wynik jest przypadkowy.\nZ tego wzoru wynika też ważny wniosek – im większa próbka, tym mniejszy błąd standardowy. Możemy to sobie przedstawić na wykresach. Poniżej nałożyłem na siebie 3 histogramy. Każdy histogram to\n95% próbek 100 osób będzie miało średnią inteligencję między 97 a 103 (\\(100 \\pm 2 \\times 1,5\\)), co policzyliśmy wyżej. Różnica między 100 a 101 będzie więc typowa, bo bardzo łatwo uzyskać ją przez przypadek. Jeśli jednak nasza próbka wyniesie nie 100, a 1000 osób, to błąd standardowy spadnie do \\(SE = \\frac{15}{\\sqrt{1000}} \\approx 0,47\\), a więc 95% próbek będzie miało średnią między 99,06 a 100,94. Tym samym średnia 101 staje się bardziej podejrzana. Ba! Staje się istotnie różna od 100, bo szansa na uzyskanie jej przez przypadek w próbie 1000 osób jest mniejsza niż 5%2. Takie widełki nazywamy 95% przedziałem ufności. Jeśli chcemy uzyskać dokładnie 95% pewności, będziemy mnożyć błąd standardowy nie przez 2, jak zrobiłem to wyżej, a przez 1,96 (patrz odwrotna dystrybuanta rozkładu normalnego).\n\n\n\n\n\n\nZadanie\n\n\n\nPewna zmienna ma rozkład o średniej 5,5. 25 dzieci karmionych kalarepą uzyskuje średnią 5,8 z odchyleniem standardowym 0,5. Jaka jest szansa, że wynik jest przypadkowy? Załóż hipotezę prawostronną (tj. że średnia u dzieci jest 5,8 albo wyższa). Oblicz 95% przedział ufności dla średniej 5,5.\n\n\n\n\n\n\n\n\nOdpowiedź\n\n\n\n\n\n\n4.2.1 Błąd standardowy\n\\[\nSE = \\frac{0,5}{\\sqrt{25}} = \\frac{0,5}{5} = 0,1\n\\]\n\n\n4.2.2 Wartość \\(p\\)\n\\[\n1 - pnorm(5.8,\\ 5.5,\\ 0.1) = 0,13\\%\n\\]\n\n\n4.2.3 Przedział ufności\n\\[\n5,5 \\pm 1,96 \\times 0,1 = 5,5 \\pm 0,196 = 5,304 \\text{ i } 5,696\n\\]"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jakub Jędrusiak",
    "section": "",
    "text": "Student psychologii Uniwersytetu Wrocławskiego. Zainteresowany psychologią procesów poznawczych i wnioskowaniem statystycznym, w tym wykorzystaniem R i pythona w nauce. Zwolennik Open Science.\n\nPublikacje\n\n\nBurzyński, E. (2020). Czarny Zeszyt. Codzienność (J. Jędrusiak & D. Wójcik, Red.). Oleśnica: Stowarzyszenie Evviva l’arte. Pobrano z https://s-el.pl/wp-content/uploads/2020/10/Czarny-zeszyt.pdf\n\n\nJędrusiak, J. (2021). Twórczość i perspektywy na aktywność twórczą w medycynie. W M. M. Nowak (Red.), Sztuka w medycynie (s. 11–22). Poznań: Wydawnictwo Kontekst. Pobrano z https://www.researchgate.net/publication/357701421_Sztuka_w_medycynie\n\n\n\n\nFonty\nFonty użyte na tej stronie:\nTekst główny – Open Sans\nNagłówki – Raleway\nKod – Cascadia Code"
  },
  {
    "objectID": "newsletter.html",
    "href": "newsletter.html",
    "title": "Nieobliczalne",
    "section": "",
    "text": "Bądź na bieżąco\n                Otrzymuj maile z nowymi wpisami\n              \n            \n\n            \n              \n                \n\n                  \n                  \n                    \n\n                      \n\n\n                      \n                      \n                      \n\n                      \n                      \n                      \n\n                      \n                      \n                      \n\n                      \n            \n            \n\n                      \n                      \n                      \n\n                      \n                      \n                      \n\n\n\n\n\n                    \n                  \n                \n              \n\n              \n\n              \n              \n              \n\n              \n\n              \n\n              \n\n\n\n\n                \n  \n  \n\n\n\n              \n              \n\n              \n                \n                  Subskrybuj\n                \n                \n                  \n                  Loading...\n                \n              \n\n              \n              \n            \n          \n\n          \n\n            \n              \n                Sprawdź maila\n                Link z potwierdzeniem zapisu powinien być na Twoim mailu. Jeśli go tam nie ma, sprawdź spam."
  },
  {
    "objectID": "posts/p-value.html",
    "href": "posts/p-value.html",
    "title": "Testy statystyczne i wartość p",
    "section": "",
    "text": "Dla przykładu – wychodzę z mieszkania i zamykam drzwi na klucz. Zrobiłem to odruchowo, nieuważnie, więc po chwili nie wiem już, czy zamknąłem te drzwi, czy nie. Wzbiera we mnie lęk, więc cofam się i sprawdzam. Uff, zamknąłem. W tej chwili uzyskuję chwilową pewność1, że drzwi są zamknięte i spokojnie wychodzę z budynku. Wspomnienie zaciera się nieco, więc moja pewność spada ze 100% do 96%. Jednak taki poziom pewności w zupełności wystarczy, żebym na pytanie „Zamknąłeś drzwi?” odpowiedział „Tak”.\nJeśli jednak cierpiałbym na zaburzenie obsesyjno-kompulsywne (OCD), 96% pewności mogłoby się okazać stanowczo za dużym ryzykiem. Mógłbym wracać do tych drzwi kilkanaście razy, za każdym razem sprawdzając, czy na pewno są zamknięte. Każda pewność niższa niż 100% byłaby dla mnie trudna do zaakceptowania. Nawet 0,5% szans, że stanie się coś złego, mógłbym uważać za bardzo, bardzo dużo. Jednak takie zachowania i zniekształcenia poznawcze uznajemy za objaw psychopatologii (Butcher & Hooley, 2018). W świecie rzeczywistym musimy akceptować jakąś niepewność, jakąś szansę na pomyłkę. W przypadku zamykania drzwi robimy to na oko. W nauce, gdzie prawdopodobieństwo określamy liczbowo, możemy przyjąć konkretną granicę."
  },
  {
    "objectID": "posts/p-value.html#problemy-z-próbkowaniem",
    "href": "posts/p-value.html#problemy-z-próbkowaniem",
    "title": "Testy statystyczne i wartość p",
    "section": "1 Problemy z próbkowaniem",
    "text": "1 Problemy z próbkowaniem\nWyobraźmy sobie, że słyszymy w telewizji wypowiedź pewnego polityka, który twierdzi, że kobiety są mniej inteligentne od mężczyzn. Do tego wyobraźmy sobie, że jesteśmy psychologami, więc mamy uprawnienia do badania inteligencji. Sceptyczni, postanawiamy naukowo sprawdzić, czy chłopski rozum naszego polityka może mieć trochę racji. Docieramy do 100 kobiet i 100 mężczyzn, każdej z osób badanych przeprowadzamy rzetelny i trafny test inteligencji WAIS-R, liczymy średnie i jesteśmy w szoku. Na skali IQ, średnia inteligencja kobiet wyniosła 99, a średnia inteligencja mężczyzn 101. Ogarnia nas zdziwienie. Czy polityk miał rację? Czy musimy się teraz pokajać i w geście hołdu zaśpiewać razem Yellow Submarine? Niekoniecznie.\nNie ufamy swoim wnioskom, więc postanawiamy powtórzyć badanie. Ponownie pobieramy próbkę 100 kobiet i 100 mężczyzn, ponownie wykonujemy WAIS-R i ponownie liczymy średnie. Ciekawe. Tym razem średnia dla kobiet wyniosła 101, a dla mężczyzn… 98. Nie dość, że różnica wzrosła, to jeszcze się odwróciła. Ale jak to? Losowaliśmy z tej samej populacji. Dlaczego nagle średnia inteligencja w naszych próbkach jest inna?\nTen przykład stał się nierealny, gdy od niechcenia samodzielnie przeprowadziliśmy WAIS-R 200 osobom. Popuśćmy wodze fantazji jeszcze luźniej. Wyobraźmy sobie, że jesteśmy szalonym dyktatorem dużego państwa i mamy nieograniczone zasoby. Zaskoczeni naszymi wynikami, chcemy sprawdzić, czy kolejne średnie też będą się od siebie różniły. Badamy więc 1000 próbek po 100 osób każda. Mamy na swoim komputerze 1000 średnich, które przeglądamy. Kilka spośród nich pokazałem w tabeli 1.\n\n\n\n\nTabela 1: Średnie wyniki IQ są zawsze trochę inne, ale rzadko są dalekie od 100.\n\n\n\n\n\n\nlp\nIQ\n\n\n\n\n1\n98.77\n\n\n2\n99.78\n\n\n3\n97.06\n\n\n4\n103.18\n\n\n5\n101.92\n\n\n6\n99.63\n\n\n7\n98.33\n\n\n8\n100.49\n\n\n9\n99.89\n\n\n10\n98.66\n\n\n\n\n\n\n\n\n\n\nZaczynamy zauważać, że wyniki za każdym razem były trochę inne, ale zawsze lądowały w pobliżu 100. Innymi słowy średnie bliskie 100 pojawiały się często, a średnie dalekie od 100 pojawiały się rzadko. Nie mamy na przykład ani jednego wyniku mniejszego niż 95, a w zakresie 99-101 mieści się 47% wszystkich uzyskanych średnich. Możemy zrobić nawet wykres, jak często pojawiały się jakie średnie.\n\n\n\n\n\n\n\n\nRysunek 1: Im wyższy słupek, tym średnia z tego zakresu pojawiała się częściej w naszym zbiorze 1000 średnich. Jak widać, średnie koncentrują się dookoła wyniku 100 IQ.\n\n\n\n\n\nWpadamy więc na pomysł – skoro średnie koncentrują się dookoła wyniku 100, to może prawdziwa średnia inteligencja w populacji wynosi właśnie 100? To jest bardzo rozsądne założenie. Zwłaszcza, że jeśli policzymy jedną, zbiorczą średnią dla 100 000 naszych badanych, to rzeczywiście wychodzi 100.\nDobieranie próby, wielkości tej próby i generalizowanie wniosków z prób na całe populacje to temat na całą inną rozmowę. Nie będziemy się teraz w to zagłębiać. Dla nas ważny jest taki problem – skąd mamy wiedzieć, czy różnica w IQ, która wychodzi nam w naszym badaniu, rzeczywiście istnieje? Może wyszła nam tylko przez przypadek? Czy cała nauka to kłamstwo, bo nigdy nie możemy mieć pewności? Czy nie powinniśmy odrzucić akademickich dywagacji i podążać za chłopskim rozumem? Wstrzymajmy panikę. To da się rozwiązać."
  },
  {
    "objectID": "posts/p-value.html#jak-łatwo-wychodzą-przypadkowe-różnice",
    "href": "posts/p-value.html#jak-łatwo-wychodzą-przypadkowe-różnice",
    "title": "Testy statystyczne i wartość p",
    "section": "2 Jak łatwo wychodzą przypadkowe różnice?",
    "text": "2 Jak łatwo wychodzą przypadkowe różnice?\nI teraz robi się ciekawie. To jest skomplikowane, warto tutaj zwolnić. Jeśli to się zrozumie, to zrozumie się całą wielką logikę testów statystycznych. Spróbujmy więc nieco się zagłębić. Jesteśmy w końcu w pięknej, abstrakcyjnej sytuacji, gdzie mamy do swojej dyspozycji 1000 średnich z różnych próbek. Na rysunku 1. widzimy, że większość naszych średnich mieści się mniej więcej w granicach 98-102. Uzyskać przez przypadek średnią 105 byłoby bardzo trudno. Uzyskać średnią 115 jest praktycznie niemożliwe. Ale już średnią 101 przez przypadek uzyskać jest bardzo łatwo.\nMożemy to ugryźć od innej strony. Jestem w końcu szalonym, bogatym dyktatorem dużego państwa, więc znowu robię olbrzymie badania i powtarzam nasz pierwotny eksperyment 1000 razy. Czyli losuję 1000 próbek po 100 mężczyzn, 1000 próbek po 100 kobiet, mierzę inteligencję i liczę różnicę. Efektem jest zbiór danych, którego fragment znajdziemy w tabeli 2.\n\n\n\n\nTabela 2: Możemy wiele razy powtórzyć nasz eksperyment, a różnice za każdym razem będą inne.\n\n\n\n\n\n\nlp\nIQ kobiet\nIQ mężczyzn\nróżnica\n\n\n\n\n1\n100.49\n98.83\n1.66\n\n\n2\n97.89\n99.14\n-1.25\n\n\n3\n99.89\n101.26\n-1.37\n\n\n4\n99.87\n100.14\n-0.27\n\n\n5\n102.91\n98.65\n4.26\n\n\n6\n101.43\n101.82\n-0.39\n\n\n7\n103.46\n101.24\n2.22\n\n\n8\n100.30\n101.45\n-1.15\n\n\n9\n99.98\n99.17\n0.81\n\n\n10\n99.00\n100.54\n-1.54\n\n\n\n\n\n\n\n\n\n\nPodstawą do różnicy była tu średnia inteligencja kobiet, więc ujemna różnica oznacza, że mężczyźni okazali się mniej inteligentni, zaś dodatnia, że bardziej. Tak jak w przypadku tabeli 1., niektóre różnice są większe, niektóre mniejsze, ale ich rozkład nie jest przypadkowy. Wrzućmy nasze różnice na wykres.\n\n\n\n\n\n\n\n\nRysunek 2: Różnice w średnich bywały większe lub mniejsze, ale oscylowały wokół zera.\n\n\n\n\n\nJak widzimy, różnice oscylują wokół zera. Skoro w większości eksperymentów różnica wyniosła zero albo prawie zero, to najprawdopodobniej to jest właśnie odpowiedź – prawdziwa różnica wynosi właśnie zero! Ale pomimo tego łatwo jest losowo uzyskać różnicę rzędu 1 punktu. Trudniej jest uzyskać różnicę rzędu 5 punktów. Przypadkowa różnica rzędu 10 punktów jest prawie niemożliwa do uzyskania.\nTo jest absolutnie kluczowe. Gdy nie ma różnic w populacjach, różnice między poszczególnymi próbkami i tak się zdarzają. Małe różnice w próbkach są łatwe do uzyskania, a duże różnice są mniej prawdopodobne. Możemy określić dokładnie, jak łatwo uzyskać jest jaką różnicę. Na przykład – jak często pojawiła się w naszym zbiorze różnica 5 lub więcej punktów (na plusie albo na minusie)? Mogę to policzyć – w 1000 próbkach różnica 5 albo większa pojawiła się 19 razy, co daje 1,9% szans. A jak często pojawiła się różnica co najmniej 2 punktów? W 343 przypadkach na 1000, czyli 34,3% szans."
  },
  {
    "objectID": "posts/p-value.html#istotność-statystyczna",
    "href": "posts/p-value.html#istotność-statystyczna",
    "title": "Testy statystyczne i wartość p",
    "section": "3 Istotność statystyczna",
    "text": "3 Istotność statystyczna\nPowtarzając nasze badanie 1000 razy, byliśmy w stanie ustalić, że różnicy w inteligencji tak naprawdę nie ma. Zyskaliśmy też możliwość policzenia, jak łatwo uzyskać daną różnicę przez przypadek, mimo że tak naprawdę różnic nie ma. Ale czy naprawdę musimy powtarzać nasz eksperyment 1000 razy, żeby zyskać taką pewność? Ronald Fisher wymyślił sposób, który ułatwia nam całą sprawę i pozwala dochodzić do podobnych wniosków na podstawie pojedynczych próbek.\nFisher daje nam taką propozycję – najpierw udajmy, że różnicy tak naprawdę nie ma. Przyjmijmy za fakt to, co trochę wyżej wywołało u nas kryzys wiary w naukę, czyli że jakakolwiek różnica, która między próbkami powstała, powstała w wyniku przypadku. Ot, bo średnie z próbek nie oddają dokładnie średniej z całej populacji i majtają się losowo dookoła prawdziwej średniej2. Taką hipotezę nazywamy hipotezą zerową. Hipoteza zerowa zawsze mówi, że tak naprawdę różnic nie ma, a jeśli jakieś wykryliśmy, to powstały one przez przypadek3.\nWyobrażamy sobie więc, że różnic między populacjami kobiet i mężczyzn w zakresie inteligencji nie ma. Przypomnijmy, że w pierwszym rzucie naszych badań kobiety miały średnią inteligencję równą 99, a mężczyźni 101. Różnica wyniosła więc 2 punkty. Z Badań Szalonego Dyktatora™ wiemy, że uzyskanie tak małej różnicy przez przypadek jest całkiem prawdopodobne, ale zakładamy, że dostępu do tamtych danych nie mamy.\nWiemy, że kiedy różnic tak naprawdę nie ma, to łatwo uzyskuje się małe różnice, ale uzyskanie dużej różnicy staje się naprawdę trudne. Możemy pójść więc taką logiką – jeśli 2 punkty to duża różnica, to to nie mógł być przypadek, że ją uzyskaliśmy. Z tego, że jest duża, możemy wnioskować, że istnieje naprawdę, a nie wynika z losowego majtania się średnich w próbkach dookoła prawdziwej średniej. Jeśli zaś różnica 2 punktów jest mała, no to równie dobrze mogła nam się trafić przez przypadek.\nLecz czy da się sprawdzić, czy dana różnica jest duża, czy mała? Bo przecież to nie może być po prostu umowne. Duże różnice to takie, które przez przypadek zdarzają się rzadko, a małe różnice to takie, które zdarzają się często. Ale czy bez pomocy państwowego aparatu opresji możemy zrobić to, co robiliśmy na koniec poprzedniego podrozdziału, czyli policzyć dokładne szanse na uzyskanie danej różnicy przez przypadek? Możesz pewnie teraz powtórzyć za Tadeuszem Sznukiem „nie wiem, ale się domyślam”.\nOczywiście sposób na to istnieje. Ten sposób nazywamy testem t-Studenta4. Należy on do szerokiej grupy podobnych sposobów na różne problemy, nazywających się ogólnie testami statystycznymi. Co najlepsze, nie będziemy wgłębiać się teraz w to, jak konkretnie ten test działa z punktu widzenia matematyki. Nie jest nam to potrzebne do zrozumienia jego logiki. A logika jest następująca:\n\nZałóż, że różnic tak naprawdę nie ma (hipoteza zerowa jest prawdziwa).\nPolicz, jak prawdopodobne jest uzyskanie takiej różnicy, jaka wyszła5.\n\nPonieważ założyliśmy, że różnic tak naprawdę nie ma, to różnica, która wyszła, musiała nam wyjść przez przypadek. Innymi słowy my chwilowo zakładamy, że to rzeczywiście był przypadek. Wtedy, korzystając ze wzoru testu t-Studenta, sprawdzamy, jaka była szansa, żeby ten przypadek miał miejsce. Jeśli szansa na to była bardzo mała, dochodzimy do wniosku, że to może jednak nie stało się przypadkiem. Mówiąc językiem naukowym – odrzucamy hipotezę zerową. Zerowa mówiła, że to wszystko wyszło przez przypadek, Ty jej odpowiadasz, że takie różnice nie dzieją się przez przypadek, że prawdopodobieństwo jest za niskie. Odrzucona hipoteza zerowa zaczyna płakać i upokorzona ucieka. Gorzej, jeśli szansa na przypadek jest duża. Wtedy nie możemy hipotezy zerowej odrzucić. Ciężko jest ją też przyjąć, ale o tym później.\nNie wgłębiając się za bardzo w to, jak nasz test to robi, wrzucamy do niego nasze dane. Program statystyczny wypluwa nam wtedy coś w tym rodzaju:\n\n\n\n    Welch Two Sample t-test\n\ndata:  women and men\nt = -0.94536, df = 195.91, p-value = 0.3456\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -6.172271  2.172271\nsample estimates:\nmean of x mean of y \n       99       101 \n\n\nNajważniejsza dla nas wartość określona jest literką \\(p\\) (p-value). Ta wartość znaczy to, nad czym tak bardzo się rozwodziłem we wcześniejszych akapitach – jak łatwo byłoby uzyskać otrzymać różnicę 2 punktów przez przypadek, gdyby różnic tak naprawdę nie było? W naszym wypadku odpowiedzią jest, że taką różnicę uzyskamy w 34,6% losowych próbek. Badania Szalonego Dyktatora™ pokazały wartość 34,3%, także widzimy, że test t-Studenta ma naprawdę dobrą skuteczność. Z jednego powtórzenia wyczytał to, do czego szalony dyktator potrzebował tysiąca powtórzeń. Teraz pojawia się pytanie – czy to wystarczająco mała szansa, żeby stwierdzić, że to nie przypadek? Żeby odrzucić hipotezę zerową?\nNa oko moglibyśmy powiedzieć, że nie. Jeśli naprawdę nie byłoby różnic, to różnicę rzędu 2 punktów albo więcej i tak dostawalibyśmy w mniej więcej co trzeciej próbce. Tak po prostu losowo. Ale jeśli \\(p\\) wyszłoby nam 0,000001%, to raczej nie mielibyśmy wątpliwości, że to nie mógł być przypadek i różnica istnieje naprawdę. Ale co gdy \\(p = 8\\%\\)? Albo \\(p = 4\\%\\)? Być może dobrze by było, gdybyśmy zaczęli się zastanawiać, jak duże ryzyko błędu możemy ponieść i podejmowali tę decyzję osobno w każdym przypadku. A może powodowałoby to tylko chaos i trudności w dojściu do porozumienia, bo dla każdego naukowca duża i mała szansa znaczyłyby co innego.\nTak czy inaczej musimy ustalić gdzieś punkt odcięcia. Jak małe prawdopodobieństwo jest wystarczająco małe, żeby uznać, że to nie mógł być przypadek? Taką wartość odcięcia oznaczamy \\(\\alpha\\) i w większości nauk przyjęty zwyczaj mówi, że \\(\\alpha = 0,05\\). Jeśli szansa na przypadek wynosi mniej niż 5%, to uznajemy, że to nie był przypadek. Warto zaznaczyć, że taka granica jest całkiem arbitralna. Jak każda granica, wywołuje pytania w stylu „Czyli jak szansa na przypadek wynosi 4,9%, to nie wierzę, że to przypadek, ale jak wynosi już 5,1%, to nie mogę być taki pewny?“. Odpowiedź brzmi „tak”. Mimo że na zdrowy rozum to nie ma sensu, to bycie blisko granicy rzeczywiście wywołuje duże emocje i część naukowców skłania do podejrzanych praktyk, żeby tylko zbić \\(p\\) poniżej magicznego 0,056.\nGdy prawdopodobieństwo uzyskania danej różnicy w sytuacji braku rzeczywistych różnic jest mniejsze niż 5%7, mówimy o takiej różnicy, że jest istotna statystycznie i odrzucamy hipotezę zerową (czyt. to nie mógł być przypadek). W naszym przykładzie \\(p = 0,346\\), a więc nie możemy odrzucić hipotezy zerowej (\\(0,346 &gt; 0,005\\)). Prawdopodobieństwo przypadku jest stanowczo za wysokie.\nCo jest jednak ważne, wartość \\(\\alpha = 0,05\\) ciągle dopuszcza 5% szans na błąd. Jeśli zrobilibyśmy 100 badań, to ok. 5 z nich dałoby nam złudzenie, że różnica jest prawdziwa, mimo że tak naprawdę nie jest. W Badaniach Szalonego Dyktatora™ w 1000 powtórzeń uzyskaliśmy dużą różnicę 47 razy (właśnie ok. 5%). Jakieś ryzyko błędu zawsze jest, pytanie brzmi tylko, jak duże ryzyko jesteśmy w stanie zaakceptować. Daryl Bem (2011) opublikował kiedyś podsumowanie swoich badań, 9 eksperymentów z których 8 udowadniało, że ludzie potrafią przewidywać przyszłość. Jak się potem okazało, w ciągu 10 lat swoich badań wykonał on znacznie więcej niż te 9 eksperymentów, ale opublikował tylko te, które wyszły (Engber, 2017). Można więc zapytać, dlaczego wyszły? A odpowiedź może brzmieć – przypadkiem. Bo wartość \\(\\alpha\\) zawsze jest trochę większa, niż zero.\nPytaniem filozoficznym jest, czy w takim razie mamy hipotezę zerową przyjąć? To nie jest takie proste, bo brakuje nam jednego jeszcze czynnika – wielkości próby. Zanim jednak omówimy wielkość próby, weźmiemy to, co powiedzieliśmy o teście t-Studenta i zobaczymy, jak to ma się do całej reszty testów statystycznych."
  },
  {
    "objectID": "posts/p-value.html#logika-testów-statystycznych",
    "href": "posts/p-value.html#logika-testów-statystycznych",
    "title": "Testy statystyczne i wartość p",
    "section": "4 Logika testów statystycznych",
    "text": "4 Logika testów statystycznych\nTestów statystycznych jest od groma. Chciałbym móc powiedzieć, że każdy sprawdza co innego, ale niestety często wiele testów służy do dokładnie tego samego, a ich autorzy przekrzykują się w tym, który test jest mocniejszy, lepszy, bardziej odporny, lepiej pachnie itd. Zdobycie pewnej orientacji w tym gąszczu wymaga czasu i doświadczenia. Dzisiaj wiem, że różnice w średnich sprawdzam testem t-Studenta, a normalność rozkładu np. testem Shapiro-Wilka, ale nie wiedziałem tego, jak zaczynałem dopiero zaczynałem się uczyć.\nMimo całego bogactwa testów statystycznych, każdy z nich idzie bardzo podobną logiką, opierającą się na dwóch hipotezach – zerowej (\\(H_0\\)) i alternatywnej (\\(H_1\\)). Zerową już poznaliśmy (i skrzywdziliśmy). Hipoteza alternatywna to jej dokładne przeciwieństwo. Jeśli zerowa mówi, że nie ma różnic, to alternatywna na przekór twierdzi, że różnice są. Każdy test statystyczny ma takie dwie hipotezy. Wiedzieć, co robi dany test, to znaczy znać jego hipotezy. Przykładowo dla testu t-Studenta hipotezy brzmią tak:\n\n\\(H_0\\): Prawdziwa różnica między średnimi wynosi 0.\n\\(H_1\\): Prawdziwa różnica między średnimi jest różna od 0.\n\nDla testu Shapiro-Wilka hipotezy brzmią tak:\n\n\\(H_0\\): Rozkład zmiennej nie różni się od normalnego8.\n\\(H_1\\): Rozkład zmiennej różni się od normalnego.\n\nHipoteza alternatywna może być niekierunkowa (np. kobiety i mężczyźni różnią się inteligencją) lub kierunkowa (np. kobiety są inteligentniejsze od mężczyzn). W tym miejscu tylko to sygnalizuję, ale to ma potem znaczenie np. przy testowaniu, czy średnie się różnią (tzw. testy jedno- lub dwustronne).\nKażdy test statystyczny wyrzuca nam na koniec wartość \\(p\\). Ta wartość zawsze znaczy to samo – jeśli hipoteza zerowa byłaby prawdziwa, to jak trudno by było uzyskać dane, które uzyskaliśmy? Jeśli byłoby trudno, jeśli szansa na to wynosi mniej niż 5%, to odrzucamy hipotezę zerową. Przy teście t-Studenta oznacza to, że stwierdzamy, że różnica w średnich istnieje naprawdę, a nie jest tylko artefaktem w naszych próbkach. W teście Shapiro-Wilka odrzucenie hipotezy zerowej oznacza, że stwierdzamy, że rozkład różni się od normalnego. Na przykład jest paranormalny i straszy.\nCiekawe jest to, że o ile przy większości testów chcemy odrzucić hipotezę zerową, tak nie zawsze jest to prawdą. Nie chodzi mi o sytuację, w której po prostu chcemy, żeby się okazało, że kobiety i mężczyźni nie różnią się inteligencją. Czasem po prostu potrzebujemy, żeby hipoteza zerowa była prawdziwa. Dla przykładu z pewnych matematycznych przyczyn lubimy, jak dane nam się rozkładają normalnie. Dlatego jeśli test Shapiro-Wilka daje istotny statystycznie wynik, to zaklinamy pod nosem, bo to znaczy, że nasz rozkład istotnie różni się od normalnego9. Logika jest ta sama – \\(p &lt; 0,05\\) oznacza odrzucenie hipotezy zerowej, tylko w przypadku tego testu, odrzucanie hipotezy zerowej jest nam nie na rękę.\nJeśli rozumie się tę ogólną logikę testów statystycznych, zna się hipotezę zerową danego testu, potrafi się wykorzystać wartość \\(p\\) do jej odrzucenia bądź nieodrzucania oraz potrafi się zinterpretować, co to odrzucenie znaczy w konkretnym teście, to spokojnie można iść w świat. Najtrudniejsze już za nami. Pierwszy kamień milowy właściwie mamy osiągnięty. Zanim jednak otworzymy szampana, warto się jeszcze zorientować, jak na \\(p\\) można wpływać, kiedy badanie nam nie wychodzi i jak ma się istotność statystyczna do wielkości efektu."
  },
  {
    "objectID": "posts/p-value.html#wielkość-próbki",
    "href": "posts/p-value.html#wielkość-próbki",
    "title": "Testy statystyczne i wartość p",
    "section": "5 Wielkość próbki",
    "text": "5 Wielkość próbki\nZ może nieco przydługich rozważań powyżej wyłonił nam się jasny obraz – duże różnice możemy wykryć, a małe nie, bo jak różnica jest mała, to zawsze jest ryzyko, że powstała przypadkiem. Ale na wielkość różnicy nie możemy wpływać. Ona jest jaka jest. A co jeśli się uprę i będę chciał wykryć małą różnicę? Czy jest coś, na co mogę wpłynąć, a co pozwoli mi wykrywać nawet te mniejsze różnice tak, żeby były istotne statystycznie? Owszem. Spoilerem był tytuł podrozdziału, ale można wykrywać mniejsze różnice, gdy zbierze się większą próbę.\nZobaczmy to na przykładzie. Pokazywałem wcześniej (na rysunku 1) wyniki symulacji, gdzie braliśmy 100 osób, liczyliśmy ich średnią inteligencję. Powtórzyliśmy to 1000 razy i patrzyliśmy, jak często pojawiają się dane średnie. Teraz zrobimy to samo, ale każda pojedyncza próbka będzie składała się z 1000 osób.\n\n\n\n\n\n\n\n\nRysunek 3: Jak często pojawiały się dane średnie, gdy w jednym rzucie badaliśmy 100 osób (mała próba) albo 1000 osób (duża próba).\n\n\n\n\n\nNa rysunku 3 widzimy, co się dzieje, kiedy liczebność próby zwiększa się. Kiedy próba była mała, wyniki koncentrowały się wokół prawdziwej średniej, czyli wartości 100, ale miały całkiem spory rozstrzał – ok. 95-105. Większość wyników plasowała się wtedy w zakresie10 od 98,5 do 101,5. Gdy próbka była duża, wyniki ciągle koncentrowały się wokół 100, ale były znacznie mniej rozproszone. Całkowity rozstęp to ok. 98-102, a większość wyników mieści się w zakresie 99,5-100,5.\nMa to sens, jak się dłużej zastanowić. Na czuja da się stwierdzić, że średnia z 1000 osób będzie bardziej adekwatna, niż średnia ze 100 osób. A „bardziej adekwatna” oznacza w tym wypadku „bliższa prawdziwej średniej całej populacji” czyli „bliżej 100”. Dlatego właśnie ten rozkład jest bardziej zbity – bo każda pojedyncza średnia, która go buduje, jest bardziej adekwatna.\nMożna powiedzieć – no tak, ale co to zmienia? Zmienia to tyle, że teraz co innego jest małą, a co innego dużą różnicą. Przy małych próbach wyniki oddalone o 1 lub więcej punktów od prawdziwej średniej zdarzały się dość często, bo w 53,3% przypadków. W takim razie można powiedzieć, że 1 punkt oddalenia to dość mało, często się zdarza przez przypadek. Ale jak próba jest duża, to takie samo oddalenie możemy już uznać za duże, bo pojawiło się tylko w 3,4% przypadków.\nPrzekładając to na logikę wartości \\(p\\) – różnice, które bardzo łatwo uzyskać w małych próbach, nagle stają się trudne do uzyskania, gdy próba jest duża. Jeśli zbadaliśmy po 100 kobiet i mężczyzn i wyszła nam różnica 2 punktów IQ, to nie zrobi to na nas większego wrażenia, bo taka różnica może być przypadkowa. Ale jeśli różnica 2 punkty wychodzi przy próbce po 1000 osób, to wtedy mamy już więcej zaufania, że ta różnica istnieje naprawdę.\nJako ciekawostkę mogę jeszcze dorzucić, że jeśli spodziewamy się, jak duży będzie nasz efekt (np. dlatego, że przeczytaliśmy dużo na ten temat i mamy już mniej więcej wyczucie, czego się spodziewać), to możemy oszacować, ile osób musimy zbadać, żeby efekt o takiej wielkości wykryć. Im mniejszy jest efekt do wykrycia, tym więcej ludzi trzeba zbadać, żeby okazał się istotny statystycznie. Nazywamy to analizą mocy i możemy wykonać w darmowym programie G*Power lub za pomocą pakietu pwr w R.\n\n5.1 Dlaczego większe próbki pozwalają wykryć mniejsze efekty?\nTa część jest zupełnie nieobowiązkowa. Spokojnie da się dobrze rozumieć testy statystyczne po prostu wiedząc, że większe próbki pozwalają wykryć mniejsze efekty, bez wchodzenia w szczegóły dlaczego tak się dzieje. Ale poświęcę chwilę, żeby to drugie też wyjaśnić. Kilka kolejnych akapitów będzie też bardziej techniczne i można spokojnie je pominąć i przejść do podrozdziału o wielkości efektu.\nPod rysunkiem 3. pisałem o tym, że większość wyników mieści się w jakimśtam zakresie dookoła średniej. Ten zakres, w którym zawiera się większość wyników, nazywamy odchyleniem standardowym i więcej na jego temat piszę w tym wpisie.\nBardzo szczególne jest odchylenie standardowe rozkładów takich, jak te z rysunku 3. Przypomnijmy, co to są za rozkłady. Pokazują one, jak często możemy wylosować jaką średnią. Środkiem tego rozkładu jest prawdziwa średnia całej populacji, a średnie dookoła to losowe odchylenia od tej prawdziwej średniej. Takie rozkłady nazywamy rozkładami próbkowania i niosą one bardzo ważną informację – jakie różnice są duże, a jakie są małe. Odchylenia standardowe takich rozkładów nazywamy błędem standardowym. Błąd standardowy mówi nam, jakie średnie będą nam często wychodzić przez przypadek, a jakie rzadko.\nJeśli wartości są w obrębie ok. 2 błędów standardowych, czyli tam, gdzie 95% wszystkich wyników, to uznajemy, że różnice są w sumie małe, nieistotne statystycznie. To, że to są 2 odchylenia standardowe11 to konkret, którego nie mówiłem wcześniej, ale który wynika z faktu, że przyjmujemy \\(\\alpha = 0,05\\) (por. podrozdział Istotność statystyczna). Innymi słowy wyniki oddalone o mniej niż 2 odchylenia standardowe uzyskać jest łatwo (więcej niż 5% szans), a wyniki oddalone o więcej niż 2 odchylenia standardowe uzyskać jest trudno (mniej niż 5% szans).\nŻeby przerobić to na konkretnym przykładzie, zerknijmy jeszcze raz na rozkłady z rysunku 3. Odchylenie standardowe rozkładu z małymi próbkami wynosi12 ok. 1,5, zaś średnia wynosi 100. Oznacza to, że często (w ok. 95% przypadków) będą nam się losowały średnie z zakresu 97-103. Jeśli jakaś grupa, np. członkowie Mensy, będzie miała średnią spoza tego zakresu, to da nam dowód na to, że średnia inteligencja tej grupy naprawdę jest inna niż średnia ogółu społeczeństwa. Odchylenie standardowe rozkładu z większymi próbami jest mniejsze, bo wynosi zaledwie ok. 0,5 (widać na rysunku, że rozkład jest węższy, więc trzeba węższych widełek, żeby objąć większość wyników). Oznacza to, że spodziewamy się wyników w zakresie 99-101 punktów IQ, a wyniki spoza tego zakresu uznajemy za nieprzypadkowe. Średni wynik 102 łatwo jest uzyskać, kiedy zbadamy tylko 100 osób, ale trudno, kiedy zbadamy ich 1000.\nWidzimy więc, że im większa próbka, tym błąd standardowy mniejszy. Przypomnijmy, że mniejszy błąd standardowy oznacza, że łatwiej nam wykryć mniejsze efekty, bo zakres wyników podejrzanych o bycie przypadkowymi (średnia ± 2 błędy standardowe) jest węższy. Możemy to sobie wyobrazić tak, że gdy policja robi dokładniejsze sprawdzenie podejrzanych, to winę części z nich mogą wykluczyć. Grono podejrzanych zmniejsza się i łatwiej jest odróżnić prawego obywatela od przestępcy.\nCzy potrzebujemy jednak Badań Szalonego Dyktatora™, żeby znać błąd standardowy? W końcu musimy wiedzieć, jaka jest prawdziwa średnia i zobaczyć, jak często pojawiają się jakie średnie próbek, żeby sprawdzić odchylenie standardowe tego rozkładu. Na szczęście, tak jak wcześniej odkryliśmy dla testów t-Studenta, istnieje zaskakująco prosty sposób na oszacowanie wielkości błędu standardowego na podstawie jednej próbki, bez pobierania 1000 próbek po 100 osób. Wzór ten wynika z czegoś, co nazywa się centralnym twierdzeniem granicznym i wygląda następująco:\n\\[\nSE=\\frac{SD}{\\sqrt{n}}\n\\]\nBierzemy odchylenie standardowe naszej próbki i dzielimy je przez pierwiastek z liczby obserwacji (w naszym wypadku liczby osób badanych). Z Badań Szalonego Dyktatora™ wyszło, że błąd standardowy dla próbki wielkości 100 osób wynosi 1,52. Jeśli weźmiemy naszą próbkę 100 mężczyzn z samego początku i podstawimy dane do wzoru, wyjdzie nam \\(SE = \\frac{14,17}{\\sqrt{100}} = 1,42\\). Jak widzimy, skuteczność tego twierdzenia jest niezła. Biorąc pod uwagę, jak dużo środków i czasu oszczędza (nie trzeba powtarzać badania 1000 razy), jest wręcz fenomenalna.\nNajważniejsze jest jednak to, że w mianowniku tego twierdzenia pojawia się pierwiastek z liczby osób badanych. Oznacza to, że im więcej osób badanych, tym przez większą liczbę dzielimy. Jak dzielimy przez większe liczby, rzeczy maleją. Czyli im więcej osób badanych, tym mniejszy błąd standardowy. A wiemy, że im mniejszy błąd standardowy, tym łatwiej wykrywać małe efekty. I właśnie dlatego, że w mianowniku tego twierdzenia jest liczba obserwacji, większe próbki pozwalają wykryć mniejsze efekty."
  },
  {
    "objectID": "posts/p-value.html#wielkość-efektu-a-istotność-statystyczna",
    "href": "posts/p-value.html#wielkość-efektu-a-istotność-statystyczna",
    "title": "Testy statystyczne i wartość p",
    "section": "6 Wielkość efektu a istotność statystyczna",
    "text": "6 Wielkość efektu a istotność statystyczna\nKiedy wiemy już, co znaczy istotność statystyczna, dobrze byłoby powiedzieć, czego nie oznacza. Ludzie często uważają, że jest ona czymś innym, niż jest w rzeczywistości, być może dzięki temu, że ma tak chwytliwą nazwę. Z jednej strony mamy w statystyce rzeczy, które nazywają się „błąd I rodzaju” i „błąd II rodzaju”, które spokojnie dało się nazwać lepiej13. Na przykład żeby nazwa sugerowała nam który jest który14. „Istotność statystyczna” to termin znacznie doskonalszy, bo nie dość, że przypomina, co oznacza, to do tego sugeruje, że znaczy znacznie, znacznie więcej, niż znaczy w rzeczywistości. Coś jak nazwa „apartament” użyta na pokój motelowy z łóżkiem, oknem i niewielkim stołem.\nKiedy mówimy, że różnice są nieistotne statystycznie, sugeruje to, że nie mają żadnego znaczenia. Kiedy mówimy, że odkryliśmy istotne różnice, sugeruje to, że odnaleźliśmy coś fundamentalnego. „Nieistotne” rozumiemy jako „małe”, „zaniedbywalne”, zaś „istotne” jako „duże”, „ważne”. I to jest błąd.\nWyobraźmy sobie (przykład kompletnie zmyślony), że badaliśmy skuteczność maści z sadła goryla, którą naciera się bananowce, żeby rodziły więcej owoców. A co tam, czemu nie. Jako szalony dyktator jesteśmy potentatami światowego rynku bananowego, mamy do swojej dyspozycji miliony drzew bananowych, możemy testować. Planujemy nasz eksperyment poprawnie metodologicznie, z grupą kontrolną i standardowymi procedurami, kupujemy tony rzeczonej maści i wydajemy polecenie smarowania oznaczonych drzew. Banany rodzone w eksperymentalnych hodowlach skrzętnie ważymy. Gdy dane spływają, dokonujemy analizy statystycznej. I cieszymy się niezmiernie, bo oto wychodzi nam istotny statystycznie efekt! Odrzucamy hipotezę zerową, to nie może być przypadek, bo \\(p &lt; 0,001\\). Czy to znaczy, że czas zacząć masowo smarować wszystkie nasze drzewa? Nie tak szybko.\nWartość \\(p\\) mówi nam, że różnica między drzewami smarowanymi a niesmarowanymi istnieje naprawdę, że nie jest tylko losowym artefaktem próbkowania. Wiemy więc, że drzewa traktowane maścią rodzą więcej bananów. Ale zupełnie innym pytaniem jest „o ile więcej”. W końcu zbadaliśmy bardzo, bardzo dużo drzew. Oznacza to, że możemy skutecznie wykrywać bardzo małe różnice. Pytanie zaś jest o wielkość efektu. Sprawdzamy więc, za jakie zwiększenie produkcji goryle oddały swoje sadło. Okazuje się, że maść powoduje zwiększenie miesięcznej produkcji bananów z jednej hodowli o ok. 2 kilogramy. Na oko 2 kiście. W chwili, kiedy to piszę, przekłada się to na zwiększenie miesięcznego przychodu o ok. 11,6 zł. A maść z sadła goryla jest znacznie, znacznie droższa. Zwłaszcza biorąc pod uwagę straty wizerunkowe.\nTakże kiedy mówimy, że „efekt jest istotny statystycznie” mamy na myśli „mamy rozsądne powody przypuszczać, że istnieje naprawdę”, a kiedy mówimy, że „efekt okazał się nieistotny statystycznie” mamy na myśli „nie możemy być pewni, czy w ogóle istnieje”. A przynajmniej to powinniśmy mieć na myśli. Z tym, co ludzie rzeczywiście mają na myśli mówiąc o statystycznej istotności, cóż, różnie bywa.\nZ naszym bananowym przykładem wiąże się jeszcze ciekawy problem dotyczący wyników nieistotnych. Nasz potentat bananowy, przed rozpoczęciem eksperymentu, mógłby sprawdzić, jak mocno maść z sadła goryla musiałaby zwiększyć produkcję, żeby wprowadzanie jej w ogóle było opłacalne. Wtedy mógłby wykonać analizę mocy, o której wspominałem wcześniej, żeby sprawdzić, ile drzew musi smarować, żeby wykryć efekt o pożądanej wielkości. Mniejszych efektów wykrywać po prostu nie potrzebuje. Badając za dużo drzew dał sobie możliwość wykrycia bardzo małych różnic, ale też wydał mnóstwo pieniędzy na maść i stracił więcej czasu swoich pracownikóę. Dla naszego potentata nie było ważne, czy maść w ogóle ma szansę zwiększyć liczbę owoców (co mogłoby ciekawić biologa), ale czy może zwiększyć tę produkcję w opłacalny sposób.\nTo rodzi pytanie dotyczące udowadniania nieistnienia. Wydaje się, że nie da się udowodnić nieistnienia czegoś. Jeśli powiem, że jednorożców nie ma, ktoś może mi odpowiedzieć, że źle szukałem. I technicznie będzie miał rację, może po prostu wszyscy źle szukamy. Czasem mam taką nadzieję. W statystyce również nie możemy być do końca pewni, czy efekt nie istnieje. Dla przykładu – jeśli w naszych badaniach maści z sadła goryla nie zaobserwowalibyśmy żadnych efektów (czyli najbardziej prawdopodobny scenariusz), mogłoby to być dlatego, że żadnego efektu tak naprawdę nie ma (czyli najbardziej prawdopodobny scenariusz). Ale być może efekt jest, tylko nasza próbka jest zbyt mała. Może efekt zwiększa masy hodowanych bananów o tysięczne części grama i po prostu na świecie jest za mało bananowców, żeby to wykryć. A może nasze badanie ma błędy metodologiczne. Może efekt jest, ale użyliśmy za mało maści albo smarowaliśmy gałęzie, podczas gdy powinniśmy byli smarować liście. Zawsze istnieje pewna doza niepewności w określaniu, że czegoś nie ma. Z drugiej strony, jeśli zrobiliśmy wszystko, co było w naszej mocy i według wszystkiego, co wiemy, powinniśmy coś zaobserwować, a nie widzimy nic, to może rzeczywiście tego nie ma? Może brak dowodu tam, gdzie się go spodziewamy, może być dowodem nieistnienia? W niektórych przypadkach, przynajmniej. W końcu z praktycznego punktu widzenia, co za różnica, czy efektu nie ma wcale, czy jest tak mały, że niemożliwy do zaobserwowania?15"
  },
  {
    "objectID": "posts/p-value.html#podsumowanie",
    "href": "posts/p-value.html#podsumowanie",
    "title": "Testy statystyczne i wartość p",
    "section": "7 Podsumowanie",
    "text": "7 Podsumowanie\n\nJeżeli obserwujemy jakiś efekt w naszych danych to może on albo istnieć naprawdę, albo być losowym artefaktem, błędem, wynikającym z tego, że próbka nie jest doskonałym odzwierciedleniem populacji, z której pochodzi.\nWartość \\(p\\) mówi nam, jaka jest szansa uzyskania danego efektu (np. różnicy w średnich) przez przypadek, czyli gdyby ten efekt tak naprawdę nie istniał. Jeśli ta szansa jest mniejsza niż 5%, to uznajemy, że nie może być mowy o przypadku.\nWartość \\(p\\) dają nam testy statystyczne, które są wymyślane w konkretnym celu. Rdzeń testu statystycznego stanowi jego hipoteza zerowa, którą – na podstawie wartości \\(p\\) – odrzucimy lub nie.\nWartość \\(p\\) zależy od dwóch rzeczy – wielkości efektu i wielkości próby. Na wielkość efektu nie mamy wpływu, ale możemy wykrywać mniejsze efekty zwiększając liczebność próby.\nTo, że coś jest istotne statystycznie znaczy tylko tyle, że prawdopodobnie istnieje naprawdę. Wartość \\(p\\) nie mówi nam, czy efekt jest duży, czy mały, ani czy ma praktyczne znaczenie.\n\nTen utwór jest dostępny na licencji Creative Commons Uznanie autorstwa-Użycie niekomercyjne-Na tych samych warunkach 4.0 Międzynarodowe."
  },
  {
    "objectID": "posts/procent-wariancji.html",
    "href": "posts/procent-wariancji.html",
    "title": "Wyjaśnianie wariancji i test F",
    "section": "",
    "text": "W psychologii można spotkać się z przytaczaniem wartości odziedziczalności. Możemy, dla przykładu, przeczytać, że wzrost jest odziedziczalny w 90% (Plomin, DeFries, McClearn, & McGuffin, 2001). Ale co to znaczy? Że z moich 170 cm wzrostu 153 cm zawdzięczam genom, a pozostałe 17 cm środowisku? To jest bez sensu. Czy odziedziczalność powie mi, czy byłem skazany na bycie niskim od początku albo czy dało się tego uniknąć? W Genetyce zachowania możemy przeczytać, że odziedziczalność to:\nW innym miejscu spotykamy uszczegółowienie:\nPo którym następuje groźnie brzmiące ostrzeżenie:\nŁatwo jest zrozumieć, że istnieje jakieś nieporozumienie co do tego, czym jest odziedziczalność. Widzimy wyraźnie, że odziedziczalność nie ma oznaczać ograniczeń, jakie biologia narzuca na jednostkę, ale że jest to miara, która w jakiś sposób dotyczy całej populacji. Pojawia się tajemnicza wariancja, co powoduje mroczne skojarzenia z matematyką. Ale ciągle możemy mieć wątpliwości co do tego, co ma znaczyć, że wzrost jest odziedziczalny w 90%? Bez tła statystycznego powyższe wyjaśnienia właściwie nic nie wyjaśniają. Ale na ich podstawie możemy wyrazić myśl o wzroście tak: czynniki genetyczne wyjaśniają 90% wariancji w zakresie wzrostu w populacji. I tak dochodzimy do clou tego tekstu – co to znaczy, że coś wyjaśnia 90% wariancji?"
  },
  {
    "objectID": "posts/procent-wariancji.html#średnia-jako-model",
    "href": "posts/procent-wariancji.html#średnia-jako-model",
    "title": "Wyjaśnianie wariancji i test F",
    "section": "1 Średnia jako model",
    "text": "1 Średnia jako model\nBy zrozumieć sens odziedziczalności (i podobnych tworów) w kontekście wyjaśniania wariancji, musimy najpierw powiedzieć sobie coś o tym, czym jest owa tajemnicza wariancja. Wykorzystamy do tego spreparowany zbiór danych nt. związku wielkości pająków z niepokojem odczuwanym przez obserwatorów(na podstawie: Field, Miles, & Field, 2012). Wyobraźmy sobie, że wielkość pająka wyrażono na Międzynarodowej Skali Wielkości Pająków, zaś niepokój mierzono kwestionariuszowo. Dane zebrano w tabeli poniżej.\n\n\n\n\nTabela 1: Wielkość pająka i niepokój, jaki wzbudza.\n\n\n\n\n\n\nwielkość pająka\nniepokój\n\n\n\n\n1\n2\n\n\n2\n8\n\n\n3\n5\n\n\n4\n8\n\n\n5\n12\n\n\n6\n11\n\n\n7\n18\n\n\n8\n16\n\n\n\n\n\n\n\n\n\n\nDla takich danych możemy policzyć całkiem sporo rzeczy. Łatwiej jest jednak zrozumieć dane, jak się je rzeczywiście widzi. Spróbujmy więc stworzyć wykres. Chcielibyśmy wiedzieć, jaki jest związek wielkości pająka z niepokojem, jaki wywołuje1.\n\n\n\n\n\n\n\n\nRysunek 1: Wielkość pająka i niepokój, jaki wzbudza, tyle że na wykresie.\n\n\n\n\n\nWidzimy więc wyraźnie, że im większy pająk, tym większy lęk wywołuje. Ale z tym wnioskiem wybiegliśmy sporo w przyszłość. Zazwyczaj w pierwszym odruchu robimy coś znacznie prostszego – liczymy średnią. Średni niepokój wyniósł tutaj dokładnie 10. Szybkie spojrzenie w tabelę pozwala nam stwierdzić, że żaden badany takiego wyniku niepokoju nie uzyskał i niezbyt nas to dziwi. W końcu to tylko średnia, żaden prawdziwy wynik nie musi mieć dokładnie średniej wartości. Jak w znienawidzonym przez statystyków kawale o średniej liczbie nóg na spacerze z psem. Rozumiemy więc, że średnia tylko w pewnym przybliżeniu oddaje cały zbiór danych, a konkretne wartości mogą się od średniej mniej lub bardziej odchylać. Te odchylenia możemy nanieść na wykres.\n\n\n\n\n\n\n\n\nRysunek 2: Średnia to rodzaj modelu, czyli narzędzia do przewidywania danych. Żaden z naszych pająków nie wywoływał średniego niepokoju, każdy punkt ma większe lub mniejsze odchylenie od średniej.\n\n\n\n\n\nPrzerywane linie pokazują nam odchylenie każdego punktu od średniej (residual). Dla przykładu pierwszy punkt ma wartość niepokoju 2, więc jego odchylenie to \\(2 - 10 = -8\\). Dlaczego na minusie? Bo pierwszy punkt jest poniżej średniej, więc żeby przejść od średniej do wartości tego konkretnego punktu musimy wykonać odejmowanie. Inny przykład – ostatni punkt ma wartość niepokoju 15, więc jego odchylenie od średniej wynosi \\(15 - 10 = 5\\). Tutaj odchylenie jest już dodatnie, co ma sens, bo ostatni punkt jest powyżej średniej, więc żeby przesunąć się od wartości średniej do wartości tego punktu musimy dodawać. Jak widać, żeby policzyć odchylenie, wystarczy od wartości punktu odjąć średnią. Można zapisać to matematycznie:\n\\[\nS_i = x_i - \\bar x\n\\]\ngdzie \\(S_i\\) to odchylenie, \\(x_i\\) to wartość konkretnego punktu, a \\(\\bar x\\) to średnia. Pozioma kreska nad czymś zawsze oznacza średnią. Dla każdego punktu moglibyśmy policzyć takie odchylenie i dopisać sobie je do tabeli.\n\n\n\n\nTabela 2: Każdy punkt ma swoje odchylenie od średniej, które możemy policzyć. Dodatnie odchylenia oznaczają, że wartość jest wyższa od średniej, a ujemne, że jest niższa.\n\n\n\n\n\n\nwielkość pająka\nniepokój\nSi\n\n\n\n\n1\n2\n2 - 10 = -8\n\n\n2\n8\n8 - 10 = -2\n\n\n3\n5\n5 - 10 = -5\n\n\n4\n8\n8 - 10 = -2\n\n\n5\n12\n12 - 10 = 2\n\n\n6\n11\n11 - 10 = 1\n\n\n7\n18\n18 - 10 = 8\n\n\n8\n16\n16 - 10 = 6\n\n\n\n\n\n\n\n\n\n\nŚrednia jest więc pewnym modelem, czyli służy nam jako uproszczony opis danych. Ponieważ modele są uproszczone, zawsze zawierają w sobie jakiś błąd. tym wypadku błędem jest po prostu odchylenie, które właśnie policzyliśmy.\nJakby ktoś zapytał mnie, jak duży niepokój wywołują pająki i uparł się, żeby podać mu jedną liczbę, to podałbym właśnie średnią. Dlaczego? Średnie mają tę niezwykłą właściwość, że starają się być tak bardzo na środku, tak blisko każdego punktu, jak to możliwe. Innymi słowy zwykła średnia daje nam mniejsze odchylenia niż jakakolwiek inna pojedyncza liczba. Dla przykładu, jakbyśmy stwierdzili sobie, że nie chcemy brać za model takiej brzydkiej liczby jak 10, a chcemy wziąć, powiedzmy, 15, to nagle okazałoby się, że odchylenia nam wzrosły. Obrazowo mówiąc, przerywane kreski na wykresie zrobiłyby się dłuższe.\nŚrednia gwarantuje mi, że pomylę się tak mało, jak to tylko możliwe, czyli że sumarycznie przerywane kreski na wykresie będą tak krótkie, jak się da. Średnia jest więc modelem niedoskonałym (bo ma błąd), ale lepszym niż jakakolwiek inna pojedyncza liczba (bo ma najmniejszy błąd).\n\n\n\n\n\n\n\n\nRysunek 3: Modelem mogłaby być dowolna liczba, jeśli się uprzeć, ale żadna pojedyncza liczba nie będzie miała tak małych odchyleń, jak średnia."
  },
  {
    "objectID": "posts/procent-wariancji.html#wariancja",
    "href": "posts/procent-wariancji.html#wariancja",
    "title": "Wyjaśnianie wariancji i test F",
    "section": "2 Wariancja",
    "text": "2 Wariancja\nPotrafimy policzyć odchylenie dowolnego punktu od średniej i wiemy, jak wyglądają te odchylenia na wykresie. Powiedziałem też, że sumarycznie średnia daje mniejsze odchylenia niż jakakolwiek inna liczba. Kusi więc, żeby odchylenia te rzeczywiście zsumować. W końcu w ten sposób wiedzielibyśmy, ile mamy błędu w modelu, czy jest go dużo, czy mało. Co więcej, poszczególne odchylenia różnią się między sobą. Fajnie więc by było policzyć średnią odchyleń. Takie średnie odchylenie powiedziałoby nam, czy ogólnie patrząc odchylenia są duże, czy niewielkie, a więc czy sama średnia jest blisko danych (dobrze je przybliża), czy może jest kompletnie od czapy (nie oddaje dobrze danych)2.\nSkuszeni matematyczną perspektywą bierzemy kalkulator w dłoń, dodajemy do siebie wartości odchyleń (co ładnie, matematycznie możemy zapisać jako \\(\\sum S_i\\), bo wielka litera sigma znaczy po prostu dodawanie)3 i bardzo się dziwimy, kiedy wychodzi nam 0. Ale jak 0? W końcu sumaryczne odchylenie nie może nam wyjść 0! Chcieliśmy jakiejś dużej liczby, która da nam ogólne pojęcie o tym, jak nasze dane odchylają się od średniej, a wychodzi nam 0, tak jakby w ogóle nie było żadnych odchyleń. Po chwili jednak orientujemy się dlaczego – część odchyleń jest dodatnich, a część ujemnych, to jak się je doda, to się zerują. No tak. A średnia (jak sama nazwa wskazuje) jest dokładnie pośrodku tych danych.\nTo w takim razie, jak chcemy dostać to nasze ogólne odchylenie, to musimy pozbyć się minusów. Moglibyśmy je po prostu zignorować (tzn. dodawać wartości bezwzględne odchyleń \\(|S_i|\\)), ale to by przecież by łoza proste. Statystycy, chcąc utrudnić wszystkim życie4, wybrali inny sposób na pozbywanie się minusów, czyli podnoszenie do potęgi drugiej. Minus razy minus daje plus, głosi szkolna formułka. Opanowujemy więc flashbacki z liceum, zaciskamy zęby, podnosimy odchylenia do kwadratu i znowu je do siebie dodajemy. Matematycznie wyrazilibyśmy to tak:\n\\[\nSS_T = \\sum_{i=1}^N S_i^2 = \\sum_{i=1}^N (x_i - \\bar x)^2\n\\]\nZa tym przerażającym zapisem kryje się prosta idea – weź wszystkie odchylenia (a mamy ich N, bo tak się oznacza liczbę zebranych obserwacji) i dodaj je wszystkie po kolei, zaczynając od 1. i na N-tym (w naszym przypadku 8.) kończąc. Po znaku równa się przypominam, co właściwie oznaczaliśmy przez \\(S_i\\), czyli obserwacja odjąć średnia do kwadratu. Jak to rzeczywiście zrobimy, wyjdzie nam 190,875. Niezbyt ładna liczba, ale daje więcej satysfakcji niż 0. Tę liczbę we wzorze nazwałem \\(SS_T\\), co jest skrótem od total sum of squares, czyli całkowita suma kwadratów. Jak się zastanowić, to jest to niezła nazwa, bo od razu jest praktycznie wzorem.\nAle z całkowitą sumą kwadratów jest pewien problem. Siłą rzeczy, im będzie więcej obserwacji, tym ta liczba będzie większa, nawet jeśli odchylenia będą mniejsze. 100 ciężarówek z 50 paletami każda dadzą w sumie więcej towaru, niż 1 ciężarówka z 200 paletami. Suma ocen wzrasta z każdą oceną, mimo że średnia może nawet nie drgnąć. Ale możemy sobie z tym poradzić dość łatwo – po prostu zróbmy z tego średnią. Średnia to suma dzielona przez liczbę przypadków. \\(SS_T\\) jest sumą kwadratów odchyleń (w tym miejscu tekstu należy się zatrzymać i upewnić się, że rozumie się wyrażenie „suma kwadratów odchyleń”), więc jak podzielimy ją przez liczbę obserwacji, to wyjdzie nam średnia z kwadratów odchyleń. Prawda? No prawie, bo to znowu byłoby za proste.\nNie rozwodząc się za bardzo, bo to też jest szeroki temat, zamiast dzielić \\(SS_T\\) przez 8, musimy podzielić ją przez 7. Mówiąc w dużym skrócie, nas w badaniach obchodzi cała populacja. Badamy tylko (losową) próbę z tej populacji, ale tak naprawdę to o samej populacji chcemy wnioskować. Nie wystawiłem 8 osób na kontakt z dużymi i małymi pająkami, bo ciekawiły mnie te konkretne osoby, tylko dlatego, że chcę powiedzieć coś ogólnie o ludziach, o Polakach, o nastolatkach czy jakiejkolwiek innej interesującej mnie populacji. Jeśli podzielilibyśmy przez 8, to wyszłaby nam średnia, która jest prawdziwa dla tej konkretnej próby, ale prawie na pewno niższa, niż ta prawdziwa, populacyjna. Ale jak podzielimy przez 7, to będziemy znacznie, znacznie bliżej prawdziwym wartościom. To da się matematycznie udowodnić, ale bez przesady, nie wszystko naraz. Ta skorygowana liczba nazywa się stopniami swobody i dla średniej zawsze jest ich \\(N-1\\). To będzie ogólnie problem w statystyce, że od czasu do czasu będziemy musieli dzielić przez stopnie swobody zamiast normalnie przez wielkość próby, żeby dostać bardziej wiarygodne liczby. Jeśli uczy się tego pierwszy raz, to można to \\(N-1\\) przyjąć na wiarę, bo sama wariancja jest wystarczająco skomplikowana. Nie trzeba wszystkiego w pełni zrozumieć od razu.\nTakże jeśli chcemy uzyskać średnią kwadratów odchyleń, to bierzemy naszą całkowitą sumę kwadratów i dzielimy ją przez \\(N-1\\). Tak jak średnią ocen liczymy dzieląc sumę ocen przez ich liczbę. Możemy to zapisać matematycznie na kilka sposobów:\n\\[\n\\sigma^2 = \\frac{SS_T}{N-1} = \\frac{\\sum^N_{i = 1} S_i^2}{N-1} = \\frac{\\sum^N_{i = 1} (x_i - \\bar x)^2}{N-1}\n\\]\nSpokojnie. To są tylko wzory, nie trzeba się ich bać. Każdy z tych trzech wzorów znaczy to samo i opiera się na tym, co już wiemy z poprzednich części tekstu. Tak naprawdę to ciągle wzór na \\(SS_T\\), tylko teraz podzielony na \\(N-1\\). Po pokonaniu pierwszego szoku można zacząć świętować! W ten sposób udało nam się wreszcie policzyć średnią kwadratów odchyleń (ponownie – trzeba się zatrzymać, upewnić się, że się rozumie, ewentualnie trochę się cofnąć i dopiero można iść dalej).\nTrzeba przyznać, że „średni kwadrat odchylenia” albo „średnia z odchyleń podniesionych do potęgi drugiej” to nie są zbyt chwytliwe nazwy. Dlatego właśnie nazwano to wariancją i oznaczono przez \\(\\sigma^2\\). To jest mała grecka litera sigma. A dlaczego \\(\\sigma^2\\) a nie po prostu \\(\\sigma\\)? Bo to średnia z kwadratów odchyleń. Jeśli byśmy chcieli wycofać się teraz z tej gmatwającej wszystko decyzji o podnoszeniu czegokolwiek do kwadratu i uzyskać upragnione średnie odchylenie, to możemy teraz wyciągnąć pierwiastek ze wzoru na wariancję, jaki nam przed chwilą wyszedł i który na pierwszy rzut oka może przytłaczać.\n\\[\n\\sigma = \\sqrt{\\frac{SS_T}{N-1}} = \\sqrt{\\frac{\\sum^N_{i = 1} S_i^2}{N-1}} = \\sqrt{\\frac{\\sum^N_{i = 1} (x_i - \\bar x)^2}{N-1}}\n\\]\nKażdy z tych wzorów znaczy ostatecznie to samo, więc można używać któregokolwiek. Ważne jest to, że wychodzi nam z tego średnie odchylenie, które nazywamy odchyleniem standardowym i które oznaczamy literą \\(\\sigma\\) lub skrótem SD (standard deviation). Odchylenie standardowe, jak już wyżej wspomniałem, mówi nam, czy średnia dobrze reprezentuje dane. Jeśli jest wysokie, to znaczy, że dane są mocno rozproszone (odchylenia są ogólnie duże) i sama średnia jest mało wiarygodna. Jeśli SD jest niewielkie, to średniej zasadniczo można zaufać. To jakie SD jest duże, a jakie małe, zależy od tego, co mierzymy. Dla średniej pensji SD = 10 uznalibyśmy za raczej niewielkie, ale takie samo odchylenie standardowe dla średniej liczby nóg takie małe już się nie wydaje. Także może i mamy średnio po 3 nogi jak wychodzimy z psem na spacer, ale ta średnia ma całą nogę odchylenia standardowego!\nPodsumowując ten fragment:\n\nZauważyliśmy, że każdy punkt ma swoje większe lub mniejsze odchylenie od średniej.\nPróbowaliśmy sumować te odchylenia, ale przeszkadzały nam minusy, więc przed sumowaniem podnosiliśmy nasze odchylenia do kwadratu, uzyskując w ten sposób całkowitą sumę kwadratów (\\(SS_T\\)).\nWykorzystaliśmy całkowitą sumę kwadratów do policzenia wariancji (średniej z kwadratów odchyleń) w taki sposób, że podzieliliśmy \\(SS_T\\) przez \\(N-1\\) (stopnie swobody). Nie mogliśmy podzielić \\(SS_T\\) przez \\(N\\), bo to zaniżyłoby prawdziwą wariancję w populacji.\nWyciągnęliśmy pierwiastek z wariacji, żeby wreszcie dostać średnie odchylenie, które nazywamy odchyleniem standardowym."
  },
  {
    "objectID": "posts/procent-wariancji.html#średnia-to-nie-wszystko",
    "href": "posts/procent-wariancji.html#średnia-to-nie-wszystko",
    "title": "Wyjaśnianie wariancji i test F",
    "section": "3 Średnia to nie wszystko",
    "text": "3 Średnia to nie wszystko\nWiemy już bardzo dokładnie czym jest wariancja i jaki ma związek ze średnią. Ale nie oszukujmy się, jesteśmy w stanie wymyślić lepsze modele niż średnia. Od samego początku korci nas, żeby na wykresie niepokoju od wielkości pająka narysować piękną, skośną, rosnącą linię, która będzie zależała nie tylko od wartości niepokoju, ale również od wielkości pająka. Przecież od razu widać, że pasowałaby do naszych danych znacznie bardziej.\n\n\n\n\n\n\n\n\nRysunek 4: Linia rosnąca sugeruje nam, że im większy pająk, tym większy niepokój wywołuje. Na pierwszy rzut oka widać, że odchylenia są mniejsze, niż przy średniej.\n\n\n\n\n\nOd razu lepiej. Od początku chcieliśmy móc powiedzieć, że im większy pająk, tym większy niepokój, a teraz mamy tego dowód. W końcu ta linia ewidentnie pasuje do danych lepiej – na pierwszy rzut oka widać, że odchylenia są mniejsze. No właśnie! Dzięki łamaniu sobie głowy nad odchyleniami od średniej zyskaliśmy sposób na stwierdzenie, czy jakiś model jest lepszy od średniej, czy nie – wystarczy sprawdzić, czy odchylenia zrobiły się istotnie mniejsze.\nSpróbujmy więc zrobić dla tych nowych odchyleń to samo, co zrobiliśmy dla odchyleń od średniej. Ponieważ samo wyznaczanie wartości tych odchyleń wymaga bardziej złożonych obliczeń ze wzoru na prostą, po prostu je podam (bo policzył je za mnie program statystyczny). Powiem tylko, że procedura jest identyczna, jak w przypadku liczenia \\(SS_T\\), ale licząc odchylenia zamiast wartości średniej (która poprzednio była naszym modelem) odejmujemy wartość, którą przewiduje nasz nowy, lepszy model, a którą w tabeli oznaczyłem jako \\(x_M\\). Podnosimy więc wartości odchyleń (oznaczone w tabeli \\(S_R\\)) do kwadratu, sumujemy i wychodzi nam w zaokrągleniu \\(SS_R = 29,98\\). \\(SS_R\\) oznacza residual sum of squares i w gruncie rzeczy mówi nam o tych odchyleniach, które ciągle są, które nie zniknęły, mimo że nowy model jest lepszy.\n\n\n\n\nTabela 3: Nowy model magicznie przewiduje niepokój z wielkości pająka (xM), ale nie robi tego doskonale. Tak jak dla średniej, możemy policzyć, jak wartość prawdziwa odchyla się od tej z modelu.\n\n\n\n\n\n\nwielkość pająka\nniepokój\nxM\nSR\n\n\n\n\n1\n2\n2,92\n-0,92\n\n\n2\n8\n4,94\n3,06\n\n\n3\n5\n6,96\n-1,96\n\n\n4\n8\n8,99\n-0,99\n\n\n5\n12\n11,01\n0,99\n\n\n6\n11\n13,04\n-2,04\n\n\n7\n18\n15,06\n2,94\n\n\n8\n16\n17,08\n-1,08\n\n\n\n\n\n\n\n\n\n\nMożemy w pewnym sensie mówić o wariancji wokół średniej i wariancji wokół modelu5. Wariancja, niezależnie wokół czego, to suma kwadratów podzielona przez \\(N-1\\), więc naturalnie musielibyśmy teraz \\(SS_T\\) i \\(SS_R\\) podzielić przez \\(N-1\\). Jednak to, co chcę pokazać dalej, będzie łatwiejsze do zrozumienia, jeśli będziemy się posługiwać surowymi sumami kwadratów. Możemy więc zignorować dzielenie przez \\(N-1\\). Jeśli ktoś mi nie ufa, to może robić to, co za moment zrobię, ale wcześniej podzielić nasze sumy kwadratów przez \\(N-1\\) i gwarantuję, że wyjdzie mu to samo6.\nMożemy więc spojrzeć na to w ten sposób – na początku mieliśmy 190,875 jednostek wariancji, a teraz, po dopasowaniu nowego modelu, mamy ich tylko 29,98. Dla wygody zaokrąglijmy te liczby do całości. Także cała różnica, \\(191 - 30 = 161\\), gdzieś nam wyparowała. Ta wariancja była, a teraz nagle jej nie ma. O takiej wariancji możemy powiedzieć, że została wyjaśniona i możemy ją oznaczyć \\(SS_M\\) od model sum of squares. Czyli stworzyliśmy nowy model, który pozwala nam przewidywać niepokój z wielkości pająka i w ten sposób wyjaśniliśmy jakąś część wariancji. Jaką? Coż, wystarczy to przeliczyć na procenty. Cała wariancja7 wynosiła 191 jednostek, a wariacja wyjaśniona wyniosła 161 jednostek. Odsetek wariancji, który udało nam się wyjaśnić, oznaczamy \\(R^2\\) i możemy go wyrazić w procentach8.\n\\[\nR^2 = \\frac{161}{191}\\times 100\\% = 84,3\\%\n\\]\nMożemy więc powiedzieć, że wielkość pająka (bo nowy model przewiduje niepokój na podstawie wielkości pająka) wyjaśnia 84,3% wariancji niepokoju. Ludzie ciągle różnią się niepokojem w reagowaniu na pająki, ale odchylenia od nowego modelu są znacznie mniejsze. Ogólny wzór na \\(R^2\\) wygląda więc tak9:\n\\[\nR^2 = \\frac{SS_M}{SS_T} = \\frac{SS_T - SS_R}{SS_T}\n\\]\nMoglibyśmy się pytać, co odpowiada za pozostałe 17,3% wariancji. Mogą to być dodatkowe czynniki, takie jak doświadczenia z pająkami w dzieciństwie albo uszkodzenia mózgu. To wymaga dalszych badań.\nMożemy wyobrazić sobie, że pojawia nam się nowa osoba badana, która pająków nie boi się w ogóle, co pokazałem na wykresie. Jak to wpływa na nasz model?\n\n\n\n\n\n\n\n\nRysunek 5: Nowa osoba badana, która nie boi się pająków wcale, nie przystaje do naszego modelu. Może to sugerować, że nasz model jest adekwatny tylko dla pewnej grupy osób (czyli możemy stworzyć bardziej złożony model, który weźmie to pod uwagę) albo że odważny badany kłamie (i powinien zostać wykluczony z bazy).\n\n\n\n\n\nJak widzimy model się trochę pozmieniał. \\(R^2\\) spadło dramatycznie, bo w tym drugim modelu wyniosło zaledwie 23,4%. Ale pojawienie się tego nowego, odważnego badanego nie zmieniło wyników pozostałych.\nTak samo jest z odziedziczalnością. Odziedziczalność mówi nam, jaką część różnic w populacji da się wyjaśnić czynnikami genetycznymi, ale nawet bardzo wysoka odziedziczalność nie mówi nam, jakie są biologiczne, genetyczne granice ludzkich możliwości rozwoju. Jeśli pojawi się zdeterminowana jednostka, to może ona wstrząsnąć naszym modelem albo, co ciekawsze, nie zrobić żadnej różnicy w modelu (jeśli próba byłaby odpowiednio duża)10. W takim wypadku model ciągle byłby bardzo adekwatny, wyjaśniałby bardzo dużo wariancji, a jednocześnie nie wykluczał istnienia jednostek, które całkowicie do niego nie przystają.\nDlatego właśnie odziedziczalność tej samej cechy może być różna w różnych populacjach albo zmieniać się w czasie – odziedziczalność, tak jak każdy model statystyczny, opisuje różnice tu i teraz, na poziomie populacji. Niestety (albo na szczęście) sama odziedziczalność nie może rozstrzygnąć, czy ja, konkretna jednostka, od urodzenia byłem skazany na bycie tak niskim. Może mi jednak powiedzieć, że czynniki genetyczne wyjaśniają 90% wariancji wzrostu w mojej populacji. Tylko teraz już wiem, co to znaczy."
  },
  {
    "objectID": "posts/procent-wariancji.html#test-f",
    "href": "posts/procent-wariancji.html#test-f",
    "title": "Wyjaśnianie wariancji i test F",
    "section": "4 Test F",
    "text": "4 Test F\nMoglibyśmy w tym momencie skończyć, ale skoro mamy wszystkie skróty powtórzone na świeżo, dobrze byłoby jeszcze jedną sprawę omówić. Wiedzę o tym, czym jest \\(SS_M\\) i \\(SS_R\\) (tzn. cała wariancja wyjaśniona przez model i cała wariancja niewyjaśniona przez model) możemy wykorzystać, do zrozumienia potężnej, uniwersalnej statystyki diagnostycznej – \\(F\\). Najpierw wyjaśnię, jak to się liczy, a potem jak się to interpretuje.\nStatystyka \\(F\\) to stosunek (czyli wynik dzielenia) wariancji wyjaśnionej do wariancji niewyjaśnionej. Matematycznie możemy zapisać to tak:\n\\[\nF = \\frac{MS_M}{MS_R}\n\\]\nMoment. Dopiero akapit wyżej pisałem, że wariancja wyjaśniona to \\(SS_M\\), a niewyjaśniona to \\(SS_R\\), a teraz nagle piszę \\(MS_M\\) i \\(MS_R\\). Czy to podważa moją wiarygodność? Czy należy rzucić komputerem w proteście przeciw umowności matematyki? Nie. Bez przesady. \\(M\\) znaczy średnia.\nPrzypominam – \\(SS_M\\) i \\(SS_R\\) to sumy kwadratów, tak jakbyśmy tylko zsumowali swoje oceny bez liczenia średniej. Do policzenia \\(F\\) potrzebujemy nie sum kwadratów, a średniego kwadratu. Ponownie, średnią ocen liczymy dzieląc sumę ocen przez ich liczbę. Mamy już policzone sumy kwadratów \\(SS_M\\) i \\(SS_R\\), więc dzielimy je przez liczbę obserwacji. Uważnemu czytelnikowi zapaliła się właśnie w głowie lampka ostrzegawcza, bo pisałem przecież, że liczba przypadków jest zakłamana i nie można jej ufać. Nie pożyczamy jej pieniędzy. Ufać można stopniom swobody i przez nie powinniśmy podzielić. Już raz to zrobiliśmy, podzieliliśmy \\(SS_T\\) przez stopnie swobody i nazwaliśmy to wariancją. Teraz tę samą logikę chcemy zastosować do \\(SS_R\\) i \\(SS_M\\).\nTylko pojawia się trudne pytanie – ile stopni swobody mają nasze \\(SS_M\\) i \\(SS_R\\)? Już odpowiadam. Liczba stopni swobody dla \\(SS_M\\) to liczba zmiennych niezależnych (w przykładzie z pająkami to 1, bo lęk przewidujemy tylko na podstawie wielkości pająka). Liczba stopni swobody dla \\(SS_R\\) to liczba obserwacji – liczba zmiennych niezależnych – 1 (w naszym przypadku \\(8-1-1 = 6\\)). I to trzeba po prostu wziąć na wiarę.\nTakże bierzemy nasze sumy kwadratów i robimy z nich średnie. \\(MS_M = \\frac{160,895}{1} = 160,895\\) oraz \\(MS_R = \\frac{29,98}{6} = 5\\). Mając średnie możemy policzyć wartość naszej statystyki \\(F = \\frac{160,895}{5} \\approx 32\\). Czad. Tylko co z tego?\nTa liczba, 32, informuje nas, że ilość wariancji wyjaśnionej przez model jest 32 razy większa niż ilość wariancji niewyjaśnionej. Czyli że model więcej wyjaśnia niż nie wyjaśnia, bo wyjaśnia średnio 160,895 jednostek wariancji, a nie wyjaśnia średnio 5 jednostek wariacji. Dobry model ma co najmniej \\(F &gt; 1\\), bo – jak się zastanowić – wtedy właśnie więcej wyjaśnia, bo nie wyjaśnia11. Licznik jest większy niż mianownik. \\(SS_M &gt; SS_R\\). Jakby wyszło nam, że \\(F = 0,25\\), to znaczyłoby to, że model jest fatalny, bo ma więcej (4 razy więcej!) błędu niż racji. W skrócie mówiąc – im większe \\(F\\), tym lepiej, a \\(F &gt; 1\\) to absolutne minimum."
  },
  {
    "objectID": "posts/procent-wariancji.html#postscriptum-jak-szybciej-liczyć-ssm",
    "href": "posts/procent-wariancji.html#postscriptum-jak-szybciej-liczyć-ssm",
    "title": "Wyjaśnianie wariancji i test F",
    "section": "5 Postscriptum – jak szybciej liczyć SSM",
    "text": "5 Postscriptum – jak szybciej liczyć SSM\nMożna zauważyć ciekawą rzecz związaną z obliczaniem \\(SS_M\\). Jak się okazuje, \\(SS_M\\) da się policzyć bez liczenia \\(SS_T\\) i \\(SS_R\\). \\(SS_M\\) uzyskujemy, jeśli zignorujemy obserwacje i zsumujemy kwadraty różnic między średnią a nowym modelem. Zaznaczyłem te różnice na poniższym wykresie przerywanymi liniami.\n\n\n\n\n\n\n\n\nRysunek 6: \\(SS_M\\) można szybciej policzyć za pomocą odchyleń modelu od średniej, czyli kresek między średnią wartościami przewidywanymi przez nasz nowy model.\n\n\n\n\n\nMożna to przetestować za pomocą danych z tabeli niżej. \\(x_M\\) to wartość przewidywana przez model, a \\(S_M\\) to różnica między modelem a średnim niepokojem. Podniesienie wartości \\(S_M\\) do kwadratu i zsumowanie ich daje w wyniku nasze \\(SS_M\\).\n\n\n\n\nTabela 4: Podstawą do policzenia SSM mogą być odchylenia modelu od średniej SM. Trzeba je policzyć dla każdego punktu, podnieść do kwadratu i dodać.\n\n\n\n\n\n\nwielkość pająka\nniepokój\nxM\nSM\n\n\n\n\n1\n2\n2,92\n2,92 - 10 = -7,08\n\n\n2\n8\n4,94\n4,94 - 10 = -5,06\n\n\n3\n5\n6,96\n6,96 - 10 = -3,04\n\n\n4\n8\n8,99\n8,99 - 10 = -1,01\n\n\n5\n12\n11,01\n11,01 - 10 = 1,01\n\n\n6\n11\n13,04\n13,04 - 10 = 3,04\n\n\n7\n18\n15,06\n15,06 - 10 = 5,06\n\n\n8\n16\n17,08\n17,08 - 10 = 7,08\n\n\n\n\n\n\n\n\n\n\nJest to jeszcze jeden sposób myślenia o \\(SS_M\\), który może pojawić się w niektórych opracowaniach. Zgłębienie tego może dać nam nowy sposób myślenia o wariancji wyjaśnianej, ale nie jest konieczne, bo ostatecznie liczy nam to komputer. Rozumienie tego, jak działa \\(SS_M\\) według poprzedniego sposobu (wariancja, która zniknęła, jak zmieniliśmy model ze średniej na jakiś inny) to i tak dużo i powinno wystarczyć.\nTen utwór jest dostępny na licencji Creative Commons Uznanie autorstwa-Użycie niekomercyjne-Na tych samych warunkach 4.0 Międzynarodowe."
  },
  {
    "objectID": "posts/uczenie-sie.html",
    "href": "posts/uczenie-sie.html",
    "title": "Pamięć w służbie ucznia",
    "section": "",
    "text": "Ten tekst oryginalnie miał być częścią publikacji pokonferencyjnej, która nigdy się nie ukazała, dlatego jest napisany bardziej formalnie niż inne teksty tutaj zebrane."
  },
  {
    "objectID": "posts/uczenie-sie.html#czym-są-techniki-uczenia-się",
    "href": "posts/uczenie-sie.html#czym-są-techniki-uczenia-się",
    "title": "Pamięć w służbie ucznia",
    "section": "1.1 Czym są techniki uczenia się?",
    "text": "1.1 Czym są techniki uczenia się?\nW swojej definicji technik uczenia się nawiązuję do koncepcji taksonomii Blooma (1956), czyli klasyfikacji celów nauczania. Pojęcie „technik uczenia się” często utożsamiane jest z pojęciem „mnemotechnik”. Nie mniej mnemotechniki odnoszą się wyłącznie do pierwszego elementu taksonomii, tj. zapamiętywania. Rzeczywiście, takie metody jak pałac pamięci nie mają na celu zwiększenia zrozumienia, zdolności wykorzystywania, analizy, syntezy czy ewaluacji informacji, jednak moja analiza wskazuje, że istnieją techniki, jak omówiona niżej metoda Feynmana, które największy nacisk kładą na cele inne niż zapamiętywanie. Dlatego też proponuję następującą definicję:\nTechniki uczenia się – usystematyzowane działania dążące do zwiększenia poziomu zapamiętywania, rozumienia, zdolności wykorzystywania, analizy, syntezy i/lub ewaluacji informacji, oparte na wiedzy na temat procesów poznawczych człowieka.\nChcielibyśmy zwrócić uwagę na trzy elementy tej definicji.\nTechniki uczenia się muszą być usystematyzowane. Tym samym nie włączam w zakres tego pojęcia „luźnych porad” w rodzaju „przed nauką zjedz lekki posiłek”, gdyż (jakkolwiek może być to korzystne) nie wpływa bezpośrednio na realizację celów postawionych przez Blooma. Tym samym, każda technika uczenia się musi dać się ująć w postaci konkretnej instrukcji.\nDrugi element to omówione wyżej odwołanie do taksonomii Blooma, i w konsekwencji rozszerzenie pojęcia technik uczenia się w stosunku do mnemotechnik. W tym kontekście, mnemotechniki można zdefiniować jako te techniki uczenia się, których głównym celem jest zwiększenie poziomu zapamiętywania.\nTrzecim elementem jest odniesienie pojęcia technik uczenia się do wiedzy empirycznej. Jest to o tyle ważne, że (jak zostało wspomniane) funkcjonuje wiele mitów, pseudonaukowych tez bez potwierdzenia w empirii, które wielokrotnie powielane, zostały powszechnie przyjęte nawet w szkołach. Przykładem takiego mitu jest, że uczyć należy się zawsze w tym samym miejscu, podczas gdy badania (np. Smith, Glenberg, & Bjork, 1978) sugerują, że to właśnie zmiana miejsca nauki może być korzystna dla przyswajania informacji."
  },
  {
    "objectID": "posts/uczenie-sie.html#sec-skutecznosc",
    "href": "posts/uczenie-sie.html#sec-skutecznosc",
    "title": "Pamięć w służbie ucznia",
    "section": "1.2 Skuteczność technik uczenia się w kontekście systemu egzaminacyjnego",
    "text": "1.2 Skuteczność technik uczenia się w kontekście systemu egzaminacyjnego\nJak wskazują Soderstrom i Bjork (2015), inne techniki uczenia się są skuteczne w perspektywie krótko- i długoterminowej. W tym kontekście przeanalizujemy teraz potencjalną wartość stosowana usystematyzowanych technik uczenia się w porównaniu do metod stosowanych przez uczniów. Jest to o tyle istotne, że nauczanie w szkołach publicznych opiera się w dużej mierze na dwóch typach metod sprawdzania wiedzy – sprawdzianach i egzaminach.\nSprawdziany to formy obejmujące mniejsze partie materiału. Co do zasady, uczeń jest w stanie powtórzyć całość wymaganego materiału „zarywając nockę” bezpośrednio przed sprawdzianem (a często jest nawet w stanie nauczyć się go od podstaw). Ze względu na rozpowszechnienie tej praktyki (w którym swój udział ma także wspomniane wyżej przeciążenie ucznia), uczeń będzie stosował i uważał za skuteczne metody raczej krótkoterminowe. Egzaminy (ósmoklasisty i matura) obejmują znacznie większą partię materiału – uczeń nie jest w stanie powtórzyć wszystkiego, czego nauczył się w przeciągu całej swojej dotychczasowej edukacji w jedną noc, tydzień, czy nawet miesiąc.\nAnaliza Willinghama (2014) wskazuje na istnienie pewnej konkretnej techniki uczenia się, która dominuje pośród uczniów. Składa się na nią (1) uważne czytanie literatury, (2) podkreślanie lub zaznaczanie co ważniejszych fragmentów tekstu, (3) nierobienie nic aż do egzaminu i (4) powtórne czytanie literatury, tym razem przykładając szczególną uwagę do tekstu uprzednio zaznaczonego. Metoda prosta i intuicyjna, cechuje się wysoką skutecznością krótkoterminową [74%; Dunlosky & Rawson (2015)], ale niską skutecznością długoterminową (29%; ibidem). Wynik na granicy progu zdawalności matury (który wynosi 30%) zwykle nie jest dla ucznia satysfakcjonujący.\nWyżej przywołane badania związane są z działaniami podejmowanymi przez ucznia i nie uwzględniają wpływu nauczyciela na proces uczenia się."
  },
  {
    "objectID": "posts/uczenie-sie.html#co-zapamiętujemy-najlepiej",
    "href": "posts/uczenie-sie.html#co-zapamiętujemy-najlepiej",
    "title": "Pamięć w służbie ucznia",
    "section": "1.3 Co zapamiętujemy najlepiej?",
    "text": "1.3 Co zapamiętujemy najlepiej?\nPodam teraz kilka ogólnych prawidłowości skutecznego zapamiętywania a także rolę powtórek i analogii w procesie uczenia się. Następnie poddam krytyce częstą praktykę uczenia się za pomocą bezrefleksyjnego przepisywania z tablicy.\nZimbardo, Johnson i McCann (2017, s. 155) stwierdzają, że „tworzymy najpełniejsze i najwierniejsze zapisy pamięciowe dla:\n\nInformacji, na której skupiliśmy uwagę, jak w przypadku słów wypowiadanych przez przyjaciela na tle rozmowy innych osób.\nInformacji, którą jesteśmy zainteresowani, jak akcja ulubionego filmu.\nInformacji, która pobudza nas emocjonalnie, jak szczególnie radosne lub ciężkie doświadczenia (…).\nInformacji, która wiąże się z wcześniejszym doświadczeniem, jak w przypadku owych danych na temat dzieciństwa muzyka, na którego koncercie byliśmy w zeszłym tygodniu.\nInformacji, którą powtarzamy, jak materiał przeglądany przed egzaminem.”\n\nPonadto, wymieniają oni dwa rodzaje powtarzania – utrzymujące i opracowujące [elaboracja; Zimbardo i in. (2017)]. Zasadniczą różnicą pomiędzy tymi dwoma jest dokonywanie zmian w utrzymywanych w pamięci informacjach. Przy powtarzaniu utrzymującym, informacje nie ulegają żadnym zmianom, podczas gdy powtarzanie opracowujące zakłada poznawcze zniekształcanie informacji. Głębokość przetworzenia przekłada się na skuteczność zapamiętywania – im dana informacja zostanie bardziej przetworzona, tym skuteczniejsze będzie zapamiętywanie (np. Craik & Lockhart, 1972).\nDobrym sposobem opracowywania informacji jest tworzenie analogii (Halpern, Hansen, & Riefer, 1990). Łączą to, co już uczeń wie lub potrafi sobie wyobrazić z informacjami nowymi i abstrakcyjnymi. Za przykład możemy przywołać porównanie elektronów na powłokach elektronowych do piłeczki na schodach. Tak jak piłeczka może wylądować na pierwszym, drugim, trzecim schodku, tak nigdy nie zatrzyma się pomiędzy schodkami. Tak samo elektron może mieć różną energię, ale zawsze będzie na jakiejś powłoce, nigdy pomiędzy (na podstawie: Campbell i in., 2016). Tworzenie analogii pomaga także rozwijać zdolności twórczego myślenia (Szmidt, 2019).\n\n\n\n\n\n\nRysunek 1: Analogia – przykład opracowywania informacji (© OpenStax na licencji CC BY 4.0)"
  },
  {
    "objectID": "posts/uczenie-sie.html#przepisywanie-jako-metoda-prowadzenia-notatek",
    "href": "posts/uczenie-sie.html#przepisywanie-jako-metoda-prowadzenia-notatek",
    "title": "Pamięć w służbie ucznia",
    "section": "1.4 Przepisywanie jako metoda prowadzenia notatek",
    "text": "1.4 Przepisywanie jako metoda prowadzenia notatek\nKonsekwencją konieczności opracowywania materiałów jest niski poziom trwałego zapamiętywania informacji przepisywanych w niezmienionej formie. Mimo to, bezrefleksyje przepisywanie z tablicy lub prezentacji jest w szkołach częstą praktyką. Jak podaje Piolat (2001; Piolat, Roussey, & Barbier, 2003; za: Piolat, Olive, & Kellogg, 2005) intensywne przepisywanie może skutkować przesunięciem zasobów pamięci operacyjnej z poznawczego przetwarzania tekstu (lub słów wypowiadanych przez nauczyciela) na sam akt pisania. Dzięki temu uczeń da radę przepisać nawet całą prezentację, ale nie będzie wiedział, co właściwie przepisał.\nProblemu tego łatwo jest uniknąć poprzez udostępnianie uczniom prezentacji lub innych materiałów dydaktycznych. Nie sugeruję, że zawsze jest to najlepsze rozwiązanie – celem prezentacji multimedialnej często nie jest przekazanie jakichkolwiek informacji, których nie byłoby w literaturze (często prezentacje multimedialne składają się w dużej mierze z cytatów i rycin zaczerpniętych właśnie z podręcznika), a jedynie merytoryczne wsparcie prowadzącego lekcję. Biorąc jednak pod uwagę ograniczoną pojemność pamięci operacyjnej, stoję na stanowisku, że jeśli uczniowie znaczącą część zajęć spędzają na przepisywaniu już opracowanego tekstu, należy albo udostępnić im materiały z lekcji, albo napomnieć, że mają już do nich dostęp w literaturze."
  },
  {
    "objectID": "posts/uczenie-sie.html#skuteczność",
    "href": "posts/uczenie-sie.html#skuteczność",
    "title": "Pamięć w służbie ucznia",
    "section": "2.1 Skuteczność",
    "text": "2.1 Skuteczność\nSkuteczność metody miejsc została wielokrotnie udowodniona eksperymentalnie (np. Roediger, 1980), również w zapamiętywaniu skomplikowanych koncepcji (Qureshi, Rizvi, Syed, Shahid, & Manzoor, 2014). Pośrednio o skuteczności metody świadczy jej wiek, bowiem stosowana jest od co najmniej dwóch tysięcy lat (Cyceron, 2001). Jako metoda intrapsychiczna, stanowi ona pewien wysiłek – gdyby „zwykłe powtarzanie” było skuteczniejsze i jednocześnie wymagało mniejszych nakładów energii, pałac pamięci prawdopodobnie zostałby przez ludzkość porzucony, gdyż nie byłby warty energii, jaką trzeba na jego budowę poświęcić. Inny pośredni dowód wynika z badania Grossa i współpracowników (2014), zgodnie z którym 25% dorosłych badanych nauczonych metody pałacu pamięci po 5 latach od kursu deklarowała, że korzysta z niej w dalszym ciągu.\nPodczas prowadzonych pilotażowo przez Stowarzyszenie Evviva l’arte w I Liceum Ogólnokształcącym im. Juliusza Słowackiego w Oleśnicy zajęć z technik uczenia się, zachęcałem uczniów do przetestowania metody na sobie, proponując podjęcie próby zapamiętania nazw dwunastu nerwów czaszkowych. Informacje zwrotne, jakie otrzymałem, świadczą, że uczniowie zaskoczeni są skutecznością, z jaką udało im się przypomnieć listę po zaledwie kilku minutach myślenia. Zauważają także, że przypominanie sobie przy zastosowaniu metody miejsc jest nie tylko skuteczniejsze, ale i łatwiejsze. Jedna z uczestniczek zajęć, 16-letnia uczennica klasy pierwszej (po gimnazjum) stwierdziła „Nie musiałam się wysilać, żeby sobie przypomnieć. Po prostu leciało.” Nadmienię, że sesja przypominania nie następuje na tych zajęciach bezpośrednio po nauce. Pomiędzy nauką a „kartkówką” omawiamy metodę Feynmana, czym absorbuje się uwagę uczniów, dzięki czemu nie mogą oni powtarzać podtrzymująco listy nerwów."
  },
  {
    "objectID": "posts/uczenie-sie.html#instrukcja",
    "href": "posts/uczenie-sie.html#instrukcja",
    "title": "Pamięć w służbie ucznia",
    "section": "2.2 Instrukcja",
    "text": "2.2 Instrukcja\nPierwszym krokiem w korzystaniu z pałacu pamięci jest wytworzenie skojarzeń. Już samo tworzenie skojarzeń, jako forma elaboracji, pozwala na zwiększenie poziomu zapamiętywania informacji (Atkinson & Raugh, 1975). Dobre skojarzenia powinny przetwarzać oryginalną informację (np. przy zapamiętywaniu listy zakupów, majonezu nie powinien reprezentować rzeczywisty majonez, a na przykład wizja kury w donicy z rzepakiem) i pobudzać emocjonalnie (Zimbardo i in., 2017). Jako relatywnie łatwy sposób stworzenia symboli zawierających ładunek emocjonalny można wskazać (1) tworzenie symboli zabawnych lub (2) nawiązujących do seksualności człowieka. Dzięki tzw. efektowi odniesienia do ja, lepiej będą zapamiętane także informacje odniesione do samego siebie (Rogers, Kuiper, & Kirker (1977) za: Wojciszke (2019)).\nDrugim krokiem jest wybór znanego miejsca lub drogi z jednego miejsca do drugiego i umieszczenie tam uprzednio skonstruowanych symboli. Prawdopodobnie pozwala to na wykorzystanie podwójnej funkcji hipokampu, który związany jest z tworzeniem wspomnień oraz pamięcią przestrzenną (Maguire i in., 2003). Jeśli symbole umieszcza się w pomieszczeniu, sugeruję umieszczanie i odczytywanie ich zawsze w tej samej kolejności, tzn. np. zgodnie z ruchem wskazówek zegara.\nW końcu – przypominanie sobie polega na wyobrażeniu sobie miejsca, w którym wcześniej umieszczono symbole. By lepiej zobrazować, jak można praktycznie wykorzystać metodę pałacu pamięci, omówię ją na przykładzie."
  },
  {
    "objectID": "posts/uczenie-sie.html#przykład",
    "href": "posts/uczenie-sie.html#przykład",
    "title": "Pamięć w służbie ucznia",
    "section": "2.3 Przykład",
    "text": "2.3 Przykład\nAgnieszka, uczennica liceum, musi nauczyć się na biologię nazw 12 nerwów czaszkowych, tj. węchowy, wzrokowy, bloczkowy, okoruchowy, trójdzielny, odwodzący, twarzowy, przedsionkowo-ślimakowy, językowo-gardłowy, błędny, dodatkowy i podjęzykowy (Woźniak, 2003). Jest to lista nazw, których musi się „po prostu” nauczyć na pamięć, przez co wybiera metodę pałacu pamięci.\nNajpierw Agnieszka musi wybrać miejsce lub drogę, które jest w stanie odtworzyć z pamięci. Wybiera drogę z przystanku autobusowego do szkoły, ponieważ chodzi tamtędy codziennie i jest ona dla niej łatwa do wyobrażenia.\nNastępnie, na podstawie listy nerwów, Agnieszka musi stworzyć symboliczne reprezentacje każdej nazwy. Pierwszy jest nerw węchowy, więc wyobraża sobie swoją polonistkę, która w szkole znana jest ze swojego wielkiego nosa. Polonistkę tę (z groteskowo powiększonym nosem) wyobraża sobie pod wiatą przystankową, jako siedzącą na ławce z podręcznikiem i wykrzykującą nazwy epok literackich do przechodniów.\nKolejny nerw, wzrokowy, Agnieszka wyobraża sobie jako dziecko, które trzyma olbrzymi balon w kształcie gałki ocznej. Umieszcza je przy słupie ogłoszeniowym, który jest niedaleko jej przystanku. Nie jest on bardzo oddalony od wiaty przystankowej, ale wystarczająco daleko, by reprezentacja nerwu wzrokowego była wyraźnie oddzielona od reprezentacji nerwu węchowego. Aby wzmocnić ślad pamięciowy, Agnieszka wyobraża sobie, że dziecko pozbawione jest własnych oczu i kieruje głowę ku plakatowi z napisem „Wielki Brat patrzy”. Podobny proces stosuje dla każdego kolejnego nerwu.\nSymbole Agnieszki są silnie przekształcone – nie są na przykład nazwami nerwów zapisanymi na tabliczkach. Ponadto, są bardzo wyraziste i emocjonalnie pobudzające – obraz krzyczącej polonistki jest dla niej zabawny, a pozbawione oczu dziecko to wizja, którą odbiera jako makabryczną, kojarzoną z horrorem. Sądzimy, że uczniowie powinni mieć pozostawioną pełną dowolność w konstruowaniu symboli, a ich treść powinna pozostać ich prywatną sprawą. W końcu – Agnieszka wykorzystuje efekt odniesienia do ja. Część (niekoniecznie wszystkie) reprezentacji nerwów odnosi się wprost do niej: polonistki nie są powszechnie znane z wielkich nosów, jednak dla Agnieszki jest to cecha charakterystyczna jej własnej nauczycielki, natomiast z hasłem „Wielki Brat patrzy” Agnieszka jest o tyle związana, że Rok 1984 Orwella uznaje za swoją ulubioną książkę.\nJakiś czas po skonstruowaniu pałacu pamięci, Agnieszka chce sprawdzić swoją wiedzę. Wyobraża sobie więc, jak wysiada z autobusu i dostrzega swoją polonistkę z karykaturalnie wielkim nosem, wykrzykującą słowo „Rokoko”, co interpretuje jako nerw węchowy. Przechodząc w wyobraźni drogę do szkoły, nazywa kolejne nerwy. Pomija jednak nerw trójdzielny, który wyobraziła sobie, jako pas ruchu, z którego można skręcić w prawo, w lewo lub jechać prosto. Ma niejasne wrażenie, że „coś tu było”, ale nie potrafi sobie przypomnieć, co konkretnie. Po sprawdzeniu w literaturze, zamienia ten symbol na bardziej wyrazisty – wyobraża sobie trzy osoby, które stoją w kółku trzymając się za ręce. Jest to przewodniczący jej ulubionej partii, sędzia z paradokumentu, który Agnieszka ogląda z mamą i królowa Elżbieta II. W ten sposób kojarzy nazwę nerwu trójdzielnego z trójpodziałem władzy, o którym uczyła się na wiedzy o społeczeństwie."
  },
  {
    "objectID": "posts/uczenie-sie.html#instrukcja-1",
    "href": "posts/uczenie-sie.html#instrukcja-1",
    "title": "Pamięć w służbie ucznia",
    "section": "3.1 Instrukcja",
    "text": "3.1 Instrukcja\nPierwszym krokiem jest wybranie zagadnienia. Ponieważ głównym celem metody Feynmana jest zwiększenie zrozumienia materiału, nie ma ona zastosowania w nauce typowo pamięciowej (np. uczenie się słownictwa w obcym języku) ani w nauce umiejętności (np. pisanie rozprawek). Przykładem zastosowania, który omawiam w dalszej części artykułu, jest zrozumienie mechanizmów warunkowania klasycznego. Innymi przykładami mogą być mechanizmy fotosyntezy czy opis przyczyn, przebiegu i skutków rewolucji październikowej. Gdy uczeń skończy zapoznawać się z literaturą poświęconą danemu zagadnieniu, nazwę zagadnienia powinien (w roli tytułu) zapisać na kartce papieru lub w dokumencie elektronicznym.\nDrugim krokiem jest opisanie zagadnienia tak, jakby tłumaczyło się je komuś innemu. Zwykle wskazuje się tutaj wyobrażenie sobie ucznia, który przyszedł z innej szkoły i nie rozumie zadanego materiału. Ja zazwyczaj polecam wyobrażenie sobie inteligentnego dziewięciolatka. Tym samym, opis powinien być możliwie najprostszy. Pisać należy z pamięci, nie wolno w trakcie korzystać z literatury i tworzyć notatki jednocześnie, bo grozi to popadnięciem w powtarzanie utrzymujące i zwykłe przepisywanie.\nTrzecim krokiem jest powrót do literatury. Należy go dokonać dopiero wtedy, gdy wystąpi trudność w konstruowaniu notatki. Na czas uzupełniania wiedzy należy przerwać pisanie. „Douczanie” powinno następować do momentu, w którym uzyska się subiektywną pewność, że jest się w stanie opisać dany koncept własnymi słowami.\nCzwartym i ostatnim krokiem jest uproszczenie powstałej w ten sposób notatki i (o ile to możliwe) wzbogacenie jej o analogie. Należy w tym kroku usunąć przedmiotowy żargon i zastąpić go wyrażeniami bardziej opisowymi. Prowadzi to do uproszczeń, jednak uczeń uczący się danego konceptu po raz pierwszy, często może sobie na takie uproszczenia pozwolić. Czwarty krok należy powtarzać aż do uzyskania opisu maksymalnie prostego. Powstała w ten sposób notatka może w przyszłości posłużyć do powtórek."
  },
  {
    "objectID": "posts/uczenie-sie.html#przykład-1",
    "href": "posts/uczenie-sie.html#przykład-1",
    "title": "Pamięć w służbie ucznia",
    "section": "3.2 Przykład",
    "text": "3.2 Przykład\nDla zilustrowania metody Feynmana, przytoczymy przykładową notatkę dotyczącą eksperymentów Pawłowa, opracowaną na podstawie podręcznika Zimbardo i współpracowników (2017).\nWarunkowanie klasyczne – wersja 1. Eksperymenty Pawłowa polegały na mierzeniu ilości śliny, którą toczyły psy pod wpływem określonych bodźców. Bodźcem bezwarunkowym był widok lub zapach jedzenia. Pawłow chciał przenieść tę reakcję na inny bodziec – na dźwięk kamertonu. Zestawiał on więc bodziec bezwarunkowy z dźwiękiem kamertonu wielokrotnie, aż dźwięk powodował u psa toczenie śliny.\nW takim tekście identyfikujemy wyrażenia trudne i język specjalistyczny, a następnie zastępujemy je wyrażeniami opisowymi. Jeśli okaże się, że notatka jest niepełna, należy ją uzupełnić o brakujące informacje.\nWarunkowanie klasyczne – wersja 2. Eksperymenty Pawłowa polegały na mierzeniu ilości śliny, którą toczyły psy w różnych sytuacjach. Psy naturalnie toczyły ślinę na widok lub zapach jedzenia. Pawłow chciał sprawić, żeby śliniły się też na dźwięk kamertonu. Zestawiał on więc jedzenie z dźwiękiem kamertonu wielokrotnie, aż dźwięk (bez obecności jedzenia) powodował u psa toczenie śliny.\nKrok czwarty, czyli recenzję, powtarzamy do uzyskania satysfakcjonującej notatki. Jeśli jesteśmy w stanie, wprowadzamy analogie. Pomimo założenia, że notatka ma być skierowana do kogoś innego, można stosować analogie odnoszące się do własnej osoby, ze względu na korzystny dla zapamiętywania efekt odniesienia do ja (Rogers i in., 1997).\nWarunkowanie klasyczne – wersja 3. Eksperymenty Pawłowa polegały na mierzeniu ilości śliny, którą toczyły psy w różnych sytuacjach. Psy naturalnie śliniły się na widok lub zapach jedzenia, tak jak robi to mój pies Emes, kiedy niosę mu miskę z karmą. Pawłow chciał sprawić, żeby śliniły się też na dźwięk kamertonu. Za każdym razem, gdy podawał psom jedzenie, dzwonił kamertonem, aż w końcu sam dźwięk (bez obecności jedzenia) powodował u psa ślinienie się."
  },
  {
    "objectID": "posts/uczenie-sie.html#fiszki-tradycyjne-a-elektroniczne",
    "href": "posts/uczenie-sie.html#fiszki-tradycyjne-a-elektroniczne",
    "title": "Pamięć w służbie ucznia",
    "section": "4.1 Fiszki tradycyjne a elektroniczne",
    "text": "4.1 Fiszki tradycyjne a elektroniczne\nZe względu na to, że jego książka została oryginalnie wydana w roku 1972, Leitner do stworzenia kartoteki autodydaktycznej proponował wykorzystanie kartonu, papieru i długopisów. Wskazywał nawet na potencjalne problemy, jakie może sprawić próba stworzenia fiszek za pomocą komputera, takie jak drogie i trudne w obsłudze oprogramowanie. XXI wiek, będący erą powszechnej dostępności i mobilności komputerów, a także wolnego oprogramowania, zażegnał te problemy. Wymienić można co najmniej kilka zasadniczych zalet fiszek elektronicznych nad tradycyjnymi:\n\nWiele fiszek automatycznie powstaje z jednej notatki bez konieczności przepisywania (np. tradycyjne stworzenie fiszek ze słownictwem angielski → polski wymaga tyle samo pracy, ile stworzenie pierwotnego zestawu polski → angielski, zaś przy fiszkach elektronicznych, obydwa zestawy można stworzyć jednocześnie).\nTworzenie fiszek elektronicznych zajmuje znacznie mniej czasu niż tworzenie tych tradycyjnych (np. przez różnice w tempie pisania).\nCzas między sesjami nauki jest regulowany przez program, co usuwa zagrożenie zbyt częstego korzystania z fiszek, na jakie wskazują Wissman, Rawson i Pyc (2012).\nSą łatwiejsze do modyfikacji.\nŁatwiej mieć przy sobie telefon niż pudełko z fiszkami tradycyjnymi (Leitner opracował nawet wariant przenośny swojej kartoteki, który miałby polegać na wykorzystaniu wolnych przegródek w portfelu, dzięki czemu miałoby się uniknąć ciekawskich spojrzeń ludzi w miejscach publicznych).\nKażda fiszka ma osobno liczony czas rozłożonej powtórki, podczas gdy forma tradycyjna uzależnia czas rozłożonej powtórki od tworzenia nowych fiszek – gdy uczeń przestaje tworzyć nowe fiszki, nie powtarza też tych starych.\nForma elektroniczna zdaje się być atrakcyjniejsza dla uczniów.\nŁatwo tworzy się fiszki obrazkowe i dźwiękowe, co dla formy tradycyjnej jest trudne lub wręcz niemożliwe.\nKlasa może łatwo współpracować w tworzeniu fiszek elektronicznych (patrz Sekcja 4.7)."
  },
  {
    "objectID": "posts/uczenie-sie.html#skuteczność-1",
    "href": "posts/uczenie-sie.html#skuteczność-1",
    "title": "Pamięć w służbie ucznia",
    "section": "4.2 Skuteczność",
    "text": "4.2 Skuteczność\nWykorzystanie fiszek pozwala na zapamiętanie większej liczby informacji niż wielokrotne powtórki przy takim samym czasie przeznaczonym na naukę, co jest prawdą zarówno dla fiszek elektronicznych (Schmidmaier i in., 2011), jak i tradycyjnych (Vaughn & Rawson, 2011). Ponadto piśmiennictwo sugeruje, że testowanie własnej wiedzy, na którym opiera się technika fiszek, zwiększa skuteczność uczenia się (np. Roediger & Karpicke, 2006) i wydłuża czas przechowywania informacji (Larsen, Butler, & Roediger III, 2009) w stosunku do zwykłych rozłożonych powtórek z wielokrotnym czytaniem. Należy zaznaczyć, że już samo rozłożenie powtórek zdaje się zwiększać długoterminową skuteczność nauki, w stosunku do nauki skomasowanej (Bahrick & Phelphs, 1987), takiej jak „zarywanie nocki” przed sprawdzianem."
  },
  {
    "objectID": "posts/uczenie-sie.html#jak-robić-skuteczne-fiszki",
    "href": "posts/uczenie-sie.html#jak-robić-skuteczne-fiszki",
    "title": "Pamięć w służbie ucznia",
    "section": "4.3 Jak robić skuteczne fiszki?",
    "text": "4.3 Jak robić skuteczne fiszki?\nNie każda fiszka będzie skuteczna, na co wskazuje Wozniak (1999). Podaje on dwadzieścia zasad skutecznego ujmowania wiedzy. Omówię tylko kilka, które uznaję za najważniejsze, a najważniejszą z nich jest zasada minimum informacji.\nZasada minimum informacji. Podstawowy błąd, jaki popełniają uczniowie zaczynający pracę z fiszkami, to umieszczanie zbyt dużej ilości informacji na jednej fiszce. By zobrazować ten problem, podam przykład na podstawie podręcznika do biologii do liceum autorstwa Hosera (1996). Błędnie wykonana fiszka może wyglądać tak:\nPrzód: Opisz proces tworzenia moczu Tył: Najpierw w ciałkach nerkowych nefronu zachodzi filtracja, co prowadzi do powstania moczu pierwotnego, którego powstaje 150-180 litrów dziennie. Następnie dochodzi do resorpcji kanalikowej, w trakcie którego aktywnie wchłaniana jest między innymi glukoza. Ostatnim etapem jest sekrecja kanalikowa, mająca na celu m.in. utrzymanie stabilnego pH moczu.\nFiszka ta przypomina raczej notatkę. Jak wskazuje Wozniak (1999), uczeń może nie tyle nie pamiętać, że człowiek produkuje 150-180 litrów moczu pierwotnego dziennie, ile zapomnieć, że w ogóle ta informacja może być podana. Zapytany wprost o dzienną ilość wytwarzanego moczu pierwotnego, odpowie bez wahania. By nauka za pomocą fiszek była skuteczna, taka karta musi zostać rozbita na wiele kart mniejszych – jeśli to możliwe, każda powinna zawierać pytanie o tylko jedną informację. Powyższy tekst, w myśl zasady minimum informacji, mógłby zostać rozbity w sposób następujący:\nPrzód: Gdzie zachodzi filtracja? Tył: W ciałkach nerkowych nefronu.\nPrzód: Jaki proces zachodzi w ciałkach nerkowych nefronu? Tył: Filtracja.\nPrzód: Jaki proces prowadzi do powstania moczu pierwotnego? Tył: Filtracja.\nPrzód: […] powstaje w wyniku filtracji. Tył: Mocz pierwotny powstaje w wyniku filtracji.\nPrzód: Ile moczu pierwotnego powstaje dziennie w organizmie człowieka? Tył: 150-180 litrów.\nPrzód 1.: W trakcie […] dochodzi do aktywnego wchłaniania glukozy. Przód 2.: W trakcie resorpcji kanalikowej dochodzi do [aktywnego/biernego] wchłaniania glukozy. Przód 3.: W trakcie resorpcji kanalikowej dochodzi do aktywnego wchłaniania [najważniejszy związek]. Tył: W trakcie resorpcji kanalikowej dochodzi do aktywnego wchłaniania glukozy.\nPrzód: Jaki jest główny cel sekrecji kanalikowej? Tył: Utrzymanie stabilnego pH moczu.\nPrzód: Wymień kolejno etapy powstawania moczu. Tył: (1) Filtracja, (2) resorpcja i (3) sekrecja.\nPrzykład obrazuje też kilka innych możliwości związanych z tworzeniem skutecznych fiszek. W przykładach 4. i 6. pojawiają się fiszki z luką, czyli inny sposób formułowania pytań, polegający na uzupełnianiu zdań, podobnie jak formułowane są pytania w krzyżówkach. Pozwala to na stworzenie kart wtedy, gdy trudno sformułować pytanie o daną informację. Ponadto, jak pokazano w przykładzie 6., z jednego zdania Anki jest w stanie wygenerować kilka osobnych kart. Takie karty będą miały taki sam tył, ale przody mogą prosić o uzupełnienie różnych części wyjściowego zdania.\nW przykładzie to samo zdanie (“W trakcie resorpcji kanalikowej dochodzi do aktywnego wchłaniania glukozy.”) należy uzupełnić informacją o (1) nazwę procesu, (2) bilans energetyczny i (3) najważniejszy związek. Jest to zgodne z zasadą minimum informacji, bo każda z tych fiszek zadaje pytanie, na które odpowiedź stanowi pojedyncza informacja. Cenne jest to, że program te karty traktuje jako oddzielne. Tym samym fakt, że uczeń nie może przypomnieć sobie, czy filtracja jest procesem aktywnym czy biernym, nie wpłynie na częstotliwość pokazywania się pytania o nazwę tego procesu.\nZa pomocą fiszek z luką łatwo jest tworzyć fiszki z pytaniami o definicje zjawisk. Polecam w tym celu korzystać ze schematu „[Termin] to [definicja].”, by uczeń był w stanie zarówno nazwać opisywane zjawisko, jak i potrafił je zdefiniować. Łatwo jest się spotkać z sytuacją, w której uczeń zastanawia się nie tyle „Co to jest?”, a raczej „Jak to się nazywało?”. Dla przykładu fiszka z definicją efektu odniesienia do ja według Wojciszke (2019, s. 533):\nPrzód 1.: […] to „lepsze zapamiętywanie informacji odnoszonych do własnej osoby niż do innych osób bądź przetwarzanej w inny sposób.” Przód 2.: Efekt odniesienia do ja to […] Tył: „Efekt odniesienia do ja to lepsze zapamiętywanie informacji odnoszonych do własnej osoby niż do innych osób bądź przetwarzanej w inny sposób.”\nListy elementów a fiszki. Jeśli uczeń musi stworzyć fiszkę, na którą odpowiada lista elementów (tak jak w przykładzie 8.) za Wozniakiem (1999) radzę skorzystać wtedy z metody pałacu pamięci lub innej mnemotechniki. Może nie być to konieczne w prostej liście trzech jednowyrazowych elementów, ale często uczniowie muszą zapamiętywać listy dłuższe i bardziej skomplikowane. Ponadto jeśli da się rozbić długą listę na mniejsze, należy to zrobić. Dla przykładu – uczeń chcący zapamiętać listę wszystkich krajów UE może rozbić tę listę na 8 mniejszych list stworzonych na zasadzie roku przystąpienia. W ten sposób zamiast listy 27-elementowej, otrzyma zestaw list, z których najdłuższa będzie miała tylko 10 elementów (przykład na podstawie: Wozniak, 1999).\nFiszki obrazkowe. Pośród zalet fiszek elektronicznych wymieniliśmy łatwość w tworzeniu fiszek obrazkowych. Jeśli to możliwe, uczeń powinien korzystać z tego typu materiałów w swoich fiszkach. Aby ułatwić tworzenie fiszek obrazkowych z luką (zasłonięty podpis na schemacie) w programie Anki, zalecamy zapoznanie się z dodatkiem Image Occlusion Enhanced (patrz Sekcja 4.8).\nFiszki a rozumienie. Wozniak zaznacza również fakt, że fiszki nie zastępują uczenia się. Można z nich korzystać dopiero po sesji nauki. Innymi słowy, nie należy tworzyć fiszek z materiału, którego się nie rozumie.\nProstota fiszek. Inną wymienioną przez Wozniaka zasadą jest, by fiszki były proste. Nie należy zakładać, że zapamięta się jakąś informację „przy okazji”, a raczej dla każdej z informacji zrobić osobną fiszkę. Dla przykładu, fiszka „Kto odkrył mechanizm warunkowania klasycznego?” powinna mieć odpowiedź „Iwan Pawłow”, nie zaś „Iwan Pawłow na początku XX wieku”. Jeśli uczeń chce zapamiętać, kiedy eksperymenty Pawłowa były przeprowadzane, powinien stworzyć dla tego pytania osobną fiszkę."
  },
  {
    "objectID": "posts/uczenie-sie.html#anki",
    "href": "posts/uczenie-sie.html#anki",
    "title": "Pamięć w służbie ucznia",
    "section": "4.4 Anki",
    "text": "4.4 Anki\nNarzędzi do tworzenia fiszek elektronicznych jest bardzo wiele, ja jednak polecam program Anki. Posiada on kilka zasadniczych zalet:\n\nJest to narzędzie całkowicie darmowe.\nJest dostępne w języku polskim.\nJako program otwartoźródłowy, posiada mnóstwo darmowych dodatków, pozwalających na zwiększenie jego możliwości (patrz Sekcja 4.8).\nJest to narzędzie dostępne dla wszystkich popularnych systemów operacyjnych – zarówno tych stacjonarnych (Windows, Linux, MacOS) jak i mobilnych (Android, iOS).\nPosiada wbudowany system synchronizacji, dzięki czemu fiszki stworzone na komputerze w bardzo łatwy sposób można zsynchronizować z aplikacją na telefonie.\nUczeń samodzielnie stwierdza, czy dobrze odpowiedział na pytanie. Dzięki temu, możliwe jest sprawne korzystanie z fiszek zadających pytania o definicje – inne programy często wymagają od ucznia udzielenia poprawnej odpowiedzi słowo w słowo, podczas gdy tutaj, to uczeń decyduje, czy jest swoją odpowiedzią usatysfakcjonowany.\n\nAnki, jako narzędzie, jest bardzo rozbudowane i posiada niezwykle duże możliwości, których opis wykracza poza obszar zainteresowania tego artykułu. Nie mniej, wszystkie jego funkcje opisane są w dokumentacji programu, a w Internecie można znaleźć dziesiątki przystępnych tekstów i wideoporadników na temat korzystania z programu Anki."
  },
  {
    "objectID": "posts/uczenie-sie.html#przeglądanie-fiszek",
    "href": "posts/uczenie-sie.html#przeglądanie-fiszek",
    "title": "Pamięć w służbie ucznia",
    "section": "4.5 Przeglądanie fiszek",
    "text": "4.5 Przeglądanie fiszek\nFiszki zgrupowane są w tzw. talie. Zgodnie z instrukcją programu Anki, talie powinny być możliwie szerokie tematycznie (czyli jedna talia „Angielski” zamiast osobnych „Angielski – owoce”, „Angielski – zwierzęta” i „Angielski – kolory”). Dzięki temu kontekst talii nie będzie stanowił podpowiedzi przy uczeniu się (Wozniak, 1999). Ponadto, przemieszanie kart oznacza przejście z uczenia się blokowego na uczenie interwałowe, które może być skuteczniejsze (Rohrer, 2012), chociaż dowody są w tej kwestii ograniczone (Dunlosky, Rawson, Marsh, Nathan, & Willingham, 2013).\nCo więcej, zalecam wyłączenie zwykłego licznika kart, który w programie Anki, poza ilością pozostałych w danej sesji fiszek, wskazuje, czy aktualnie wyświetlana karta jest nowa, pojawia się dzisiaj po raz pierwszy, czy po raz kolejny. Może to stanowić niechcianą podpowiedź. Zwykły licznik można zastąpić dodatkiem Remaining Time (patrz Sekcja 4.8).\nKażdego dnia, program proponował będzie przejrzenie określonej porcji fiszek z danej talii. To, jakie konkretnie karty zostaną pokazane, zależy od uprzednio udzielanych na nią odpowiedzi. W ten sposób karta widziana po raz pierwszy pokaże się dwukrotnie w tej samej sesji nauki. Jeśli uczniowi uda się odpowiedzieć poprawnie dwa razy z rzędu, karta pojawi się następnego dnia. Jeśli i wtedy odpowie poprawnie, pojawi się po trzech dniach. Przerwa będzie się stopniowo wydłużała. Błąd powoduje rozpoczęcie procesu od nowa dla danej karty."
  },
  {
    "objectID": "posts/uczenie-sie.html#rozłożone-powtórki-fiszek-w-kontekście-egzaminów",
    "href": "posts/uczenie-sie.html#rozłożone-powtórki-fiszek-w-kontekście-egzaminów",
    "title": "Pamięć w służbie ucznia",
    "section": "4.6 Rozłożone powtórki fiszek w kontekście egzaminów",
    "text": "4.6 Rozłożone powtórki fiszek w kontekście egzaminów\nTaki sposób prezentowania fiszek pozwala uczniowi na bieżąco powtarzać te fragmenty materiału, których mógł już zapomnieć. Dla przykładu – Dawid, uczeń klasy biologiczno-chemicznej w liceum, we wrześniu przerabiał w klasie grzyby. Dawid nie korzystał z fiszek, więc w listopadzie nie pamięta już dużej części materiału, a zanim przyjdzie czas powtórek przed maturą, okaże się, że Dawid musi nauczyć się grzybów od nowa.\nZ kolei Paweł, uczeń tej samej klasy, korzysta z fiszek. W ramach jego codziennej praktyki, pojedyncze fiszki dotyczące grzybów zaczynają wracać już pod koniec października. W tym czasie Paweł jeszcze pamięta nazwy zarodników workowców, ale nie pamięta nazw zarodników podstawczaków. Ślad pamięciowy dla informacji, które był w stanie wydobyć, ulega wzmocnieniu w ramach tzw. małego cyklu pamięciowego Strelau & Doliński (2018), co można porównać do skoku na trampolinie – człowiek, który spada na trampolinę z większej wysokości, wybije się wyżej, tak samo informacja, którą uda się wydobyć po dłuższym czasie, będzie mogła być jeszcze dłużej przechowywana. Ponieważ pod koniec października Paweł pamięta większość materiału z grzybów, tylko nieliczne fiszki wracają do normalnego cyklu. Tym samym, nie zakłócają normalnej nauki bieżącego materiału, a jednak nie są zapominane. W ten sposób, Paweł przed maturą nie będzie musiał uczyć się całości grzybów od nowa, a jedynie je sobie przypomni.\nGodną uwagi funkcją programu Anki jest tworzenie tzw. talii filtrowanych. Jeśli uczeń w trakcie tworzenia fiszek oznaczy je odpowiednią etykietą (np. „grzyby”), będzie mógł później łatwo stworzyć dodatkową talię, zawierającą wszystkie fiszki z grzybów, jakie posiada. Dzięki temu, uczeń może upewnić się, że pamięta wszystkie informacje, jakie zawarł w fiszkach o grzybach, co może być przydatne na przykład przed kartkówką powtórkową."
  },
  {
    "objectID": "posts/uczenie-sie.html#sec-wspolpraca",
    "href": "posts/uczenie-sie.html#sec-wspolpraca",
    "title": "Pamięć w służbie ucznia",
    "section": "4.7 Potencjał do współpracy w fiszkach elektronicznych",
    "text": "4.7 Potencjał do współpracy w fiszkach elektronicznych\nO ile samo tworzenie fiszek jest korzystne z punktu widzenia uczenia się, gdyż wymaga selekcji i wymusza opracowanie materiału do formy pytań, warty zauważenia jest potencjał do współpracy, jaki zyskuje klasa korzystając z tej techniki.\nGrupy wewnątrz klasy (lub nawet całe klasy) mogą podzielić się materiałem do opracowania i w ten sposób zmniejszyć nałożone na siebie obciążenie. Ma to szczególne znaczenie, jeśli uczniowie nie korzystają z fiszek od początku, a chcą wykorzystać je np. w powtórkach maturalnych. Jak wspomniano wyżej, jedna osoba nie jest w stanie szybko opracować całości materiału do egzaminu, jednak co innego grupa. Olga może poświęcić weekend na opracowywanie metabolizmu wiedząc, że fiszki o rybach dostanie od Adama, a o grzybach od Pawła.\nWarta podkreślenia jest łatwość dzielenia się fiszkami elektronicznymi. Przesłanie pliku z pakietem fiszek jest szybkie, nie wymaga przepisywania i nie wiąże się z żadnym kosztem dla udostępniającego. Odbywa się niejako „przy okazji”. Niezależnie od tego, kto dane fiszki stworzył, odpowiedzialność za ich przeglądanie i tak spoczywa na uczniu – posiadanie fiszek nie sprawi, że nie trzeba się zawartych w nich informacji nauczyć, nie mniej znacząco to zadanie ułatwi."
  },
  {
    "objectID": "posts/uczenie-sie.html#sec-dodatki",
    "href": "posts/uczenie-sie.html#sec-dodatki",
    "title": "Pamięć w służbie ucznia",
    "section": "4.8 Dodatki",
    "text": "4.8 Dodatki\nBy zebrać rozrzucone w różnych miejscach tego artykułu informacje o kilku wartych uwagi dodatkach do programu Anki. Jako program otwartoźródłowy, Anki posiada dużą liczbę bezpłatnych dodatków, rozszerzających jego możliwości o różne dodatkowe funkcje. Poniżej omówimy kilka dodatków, o których wspominaliśmy wyżej oraz kilka innych, również wartościowych. Ze względu na praktyczną naturę tego podrozdziału, warto najpierw samodzielnie zapoznać się z podstawowymi funkcjami programu Anki. Inaczej Czytelnik może nie być w stanie wyobrazić sobie działania części z omawianych niżej rozszerzeń.\nImage Occlusion Enhanced. Jest to dodatek, dzięki któremu można w prosty sposób tworzyć fiszki obrazkowe z luką (patrz Rysunek 1). Uczeń może za jego pomocą nauczyć się rozpoznawać elementy schematów. Szczególnie użyteczne dla przedmiotów przyrodniczych, gdzie można użyć obrazów zamiast opisów (np. ryciny anatomiczne, mapy).\n{!fig-image}\nReview Heatmap. Jest to dodatek, który dodaje do głównego ekranu Anki schemat, na którym kolorem zaznaczone są dni, w których przeglądano karty. Im więcej kart przejrzano w danym dniu, tym ciemniejszy jest kolor (patrz Rysunek 2). Poza tym podaje średnią ilość dziennie przeglądanych kart, czy dni, w jakie z rzędu uczeń dokonywał powtórek. Chęć nieprzerywania tej passy może okazać się bodźcem motywującym do codziennych powtórek. Co więcej metaanaliza Harkina i współpracowników (2016) wskazuje, że monitorowanie swoich postępów znacząco zwiększa skuteczność osiągania celów.\n\n\n\n\n\n\nRysunek 2: Schemat Review Heatmap (materiały własne)\n\n\n\nRemaining time. Jak pisałem wyżej, zalecam, by wyłączyć standardowy licznik pozostałych kart, ze względu na to, że podpowiada, czy dana karta jest nowa, przeglądana czy uczona ponownie. Zamiast tego zalecam instalację dodatku Remaining time, który jest niczym innym, jak paskiem postępu, który wyświetla się w górnej części ekranu i wypełnia w trakcie przeglądania talii. Ponadto podaje szacowany czas do zakończenia powtórki i czas, jaki upłynął od jej początku.\nLife Drain. Jest to dodatek ułatwiający utrzymanie uwagi w trakcie powtórek. Umieszczony u dołu talii „pasek życia” maleje z upływem czasu, zaś udzielenie odpowiedzi (niezależnie od tego, czy będzie ona poprawna) odnawia część straconego życia. Pozwala to uczniowi utrzymać uwagę skoncentrowaną na powtórce, ponieważ rozpraszanie się będzie skutkowało utratą życia. Jeśli Life Drain zostanie uzupełniony dodatkiem Life Drain Extra, utrata całości życia będzie skutkowała wyświetleniem ekranu „Game over”, jednak nie będzie to miało żadnych innych konsekwencji. Uczeń może zacząć od nowa w miejscu, w którym skończył. Jest to prosty do wprowadzenia element grywalizacji, która posiada potencjał jako skuteczna metoda dydaktyczna (Cewińska & Krasnova, 2014). Zaznaczamy, że dodatek jest wysoko konfigurowalny – pozwala np. włączyć kary za błędne odpowiedzi (czego w zwykłej praktyce nie zalecam), a także posiada opcję, by utrata życia zatrzymywała się podczas sprawdzania odpowiedzi (co może być korzystne przy taliach z bardziej skomplikowanymi kartami, jak pytania o definicje, ale jest niepotrzebne przy kartach prostych, jak słownictwo w językach obcych)."
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html",
    "href": "posts/gestosc-prawdopodobienstwa.html",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "",
    "text": "Pomimo tego, że rozkład normalny jest podstawą statystyki jako takiej, nie trzeba go głęboko rozumieć, żeby używać statystyki. Jest to użyteczna wiedza, którą musimy posiąść, jeśli naprawdę chcemy rozumieć, co robimy. Jednak da się żyć i sprawnie wyciągać wnioski statystyczne i bez tej wiedzy. Ten wpis można potraktować więc jako spojrzenie w głębię statystyki, ale na pierwsze zetknięcie z tą dziedziną znacznie lepiej przeczytać wpisy o wariancji i o wartości \\(p\\)."
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#szacowanie-prostokątami-riemanna",
    "href": "posts/gestosc-prawdopodobienstwa.html#szacowanie-prostokątami-riemanna",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "3.1 Szacowanie prostokątami Riemanna",
    "text": "3.1 Szacowanie prostokątami Riemanna\nZałóżmy ponownie, że chcemy policzyć, jaka część populacji wykazuje inteligencję między 95 a 105. Od razu podam odpowiedź – 26,11%. Nic nam po samej odpowiedzi, jeśli nie umiemy tego liczyć, ale miejmy to w głowie. Za moment będziemy określać te wartości z pewnym przybliżeniem, więc dobrze jest znać dokładną wartość, żeby móc stwierdzić, czy dobrze przybliżyliśmy. Zaznaczmy sobie na wykresie przestrzeń, o którą nam chodzi.\n\n\nKod\nnormal &lt;- list(stat = \"function\", fun = dnorm, args = list(mean = 100, sd = 15))\n\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(95, 105)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\n# definicja zmiennej `normal` w trybie dziennym\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(95, 105)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15), colour = \"white\") +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nŻeby odczytać prawdopodobieństwo z takiego wykresu, musimy oszacować, wielkość przestrzeni, którą zaznaczyłem na czerwono. Wielkość przestrzeni, czyli… pole. Na czerwono zaznaczony jest prawie prostokąt, tylko ma brzuszek na górze. Jeśli będę mógł oszacować pole tego prostokąta, to to pole właśnie będzie naszym prawdopodobieństwem. Jak więc to zrobić? Bardzo z grubsza możemy brzuszek zignorować i udawać, że jest to rzeczywiście prostokąt.\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\", colour = \"black\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(100, 100, 15), xmin = 95, xmax = 105, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\", colour = \"white\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(100, 100, 15), xmin = 95, xmax = 105, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nSzerokość tego prostokąta wynosi 10 (bo od 95 do 105 na osi X), zaś jego wysokość odczytujemy z osi Y, czyli jest taka, jak gęstość prawdopodobieństwa dla \\(x = 100\\). Odczytywanie tej wielkości z wykresu szybko robi się niepraktyczne, dlatego możemy ją wyliczyć. Możemy to zrobić podstawiając bezpośrednio do wzoru (który wygląda tak strasznie, że go nie pokażę), wykorzystując funkcję dnorm(100, 100, 15) w R (pierwszy argument to wartość, drugi to średnia, trzeci odchylenie standardowe), korzystając z internetowego kalkulatora albo kalkulatora naukowego. Możemy teraz skorzystać ze wzoru na pole prostokąta i, po wybiciu się z flashbacków ze szkoły podstawowej, otrzymać wynik:\n\\[\nP \\approx 0.02659 \\times 10 = 0,2659 = 26,59\\%\n\\]\nNieźle! Pomimo tak zgrubnego oszacowania, błąd wynosi mniej niż 1 punkt procentowy. Tworzy nam się więc schemat – wystarczy pomnożyć gęstość prawdopodobieństwa przez szerokość widełek i wyjdzie! Co nie? No i tak, i nie. Powyższy przykład pokazuje, że czasem to zadziała. Zobaczmy jednak, co się stanie, jak będziemy chcieli tak oszacować wartość dla szerszego przedziału 70-130.\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\", colour = \"black\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(100, 100, 15), xmin = 70, xmax = 130, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\", colour = \"white\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(100, 100, 15), xmin = 70, xmax = 130, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nWysokość się nie zmieniła, zaś szerokość wzrosła do 60. Przemnożenie tych wartości da nam prawdopodobieństwo 159,58%. No. Przesadziliśmy odrobinę. Jak widać na wykresie, nasz prostokąt wystaje poza krzywą tak bardzo, że aż jego pole jest większe niż 100%. To może zmieńmy strategię i wciśnijmy nasz prostokąt pod krzywą, żeby na pewno nie wystawał?\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\", colour = \"black\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(70, 100, 15), xmin = 70, xmax = 130, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\", colour = \"white\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(70, 100, 15), xmin = 70, xmax = 130, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nWysokość tego prostokąta równa jest gęstości prawdopodobieństwa w punktach 70 i 130. Nie jest istotne, który wybierzemy, bo rozkład normalny jest symetryczny. Ponieważ oba te są tak samo oddalone od środka (o 30 punktów), mają taką samą wysokość. Policzenie pola naszego prostokąta ujawnia nam to, co widać już na oko, czyli że pole jest znacznie zaniżone, bowiem wychodzi \\(P \\approx 60 \\times 0,0036 = 0,216 = 21,6\\%\\). Wiemy więc tyle, że nasze pole jest nie mniejsze niż 0,216 i nie większe niż 1,6. Super. Tyle to widzę na oko.\nMożemy jednak zmienić strategię i zamiast wyznaczać pole jednego dużego prostokąta, podzielić go wiele małych. Na przykład nasz przedział możemy rozbić na 7 prostokątów po 101. W ten sposób wykres gęstości zaczyna trochę przypominać histogram, ale trzeba pamiętać, że po to rysujemy prostokąty, żeby policzyć pole. Wartość z osi Y ciągle nie jest prawdopodobieństwem.\n\n\nKod\nriemann &lt;- tibble(X = seq(70, 130, 10), Y = dnorm(X, 100, 15))\n\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\", colour = \"black\") +\n    geom_col(data = riemann, aes(X, Y), fill = \"#DB3D3D\", colour = \"#8F1A1A\", alpha = 0.25, width = resolution(riemann$X)) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\n# definicja zmiennej `riemann` w trybie dziennym\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\", colour = \"white\") +\n    geom_col(data = riemann, aes(X, Y), fill = \"#DB3D3D\", colour = \"#8F1A1A\", alpha = 0.25, width = resolution(riemann$X)) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nNasze prostokąty mają środki w punktach 70, 80, 90, 100, 110, 120 i 130. Gdy policzymy gęstość prawdopodobieństwa dla każdego z tych punktów i przemnożymy przez szerokość 10, to wychodzi nam pole \\(P \\approx 0,9826 = 98,26\\%\\). Wartość prawdziwa to \\(P \\approx 0,9545\\), więc błąd ewidentnie zmalał. I maleje bardziej, gdy narysujemy jeszcze cieńsze prostokąty.\nWróćmy pamięcią do tabeli 1. Zaznaczyłem tam 41 punktów od 95 do 105 licząc co 0,25. Następnie zsumowałem te wartości i otrzymałem 107%. Ale wyobraźmy sobie to, co zrobiłem, nieco inaczej. Wyznaczyłem wysokość 41 prostokątów, z których każdy miał szerokość 0,25. Następnie dodałem wszystkie wysokości, ale zignorowałem szerokość. Jeśli teraz uwzględnię, że liczę pole i każdy prostokąt2 przemnożę przez 0,25, to powinienem otrzymać niezłe oszacowanie! I rzeczywiście, 41 prostokątów o szerokości 0,25 pozwala nam oszacować powierzchnię między 95 a 105 na 26,74%. W porównaniu z prawdziwą wartością 26,11%, wygląda to całkiem przyzwoicie."
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#zapis-całkowy",
    "href": "posts/gestosc-prawdopodobienstwa.html#zapis-całkowy",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "3.2 Zapis całkowy",
    "text": "3.2 Zapis całkowy\nSzacowanie pola pod krzywą za pomocą prostokątów Riemanna, czyli to, co zrobiliśmy wyżej, to stara i skuteczna metoda. Jeśli chcemy otrzymać naprawdę dokładne wartości, możemy dzielić pole na setki, tysiące, miliony prostokątów i uzyskać przez to dowolną dokładność naszego oszacowania. Technicznie jednak żadna z tych wartości nie będzie doskonale dokładna. Nie ma to praktycznego znaczenia, ale jeśli chcemy zachować matematyczną czystość, musimy skorzystać z pewnego szczególnego zapisu.\nWiemy już, że prostokąty pod krzywą są nieskończenie cienkie. Wiemy też, że wzór na pole prostokąta to długość razy szerokość. Spójrzmy na taki „nieskończenie” cienki prostokąt.\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\", colour = \"black\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(100, 100, 15), xmin = 99.9, xmax = 100.1, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\", colour = \"white\") +\n    annotate(geom = \"rect\", fill = \"#DB3D3D\", colour = \"#8F1A1A\", ymin = 0, ymax = dnorm(100, 100, 15), xmin = 99.9, xmax = 100.1, alpha = 0.25) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nWysokość tego prostokąta to gęstość prawdopodobieństwa dla \\(IQ = 100\\). Możemy zapisać to pewną przerażająco wyglądającą notacją \\(\\mathcal{N}(100;\\ 100,\\ 15^2)\\). Oznacza to „wartość rozkładu normalnego o średniej \\(\\mu = 100\\) i wariancji \\(\\sigma^2 = 15^2\\) w punkcie \\(x = 100\\)”. Każdy rozkład normalny daje się dokładnie opisać dwiema liczbami – średnią i odchyleniem standardowym. Jak znam te dwie rzeczy, mogę liczyć. Warto zwrócić uwagę, że podajemy w tej notacji wariancję, a nie odchylenie standardowe, ale (jak pamiętamy z wpisu o wariancji) wariancja to odchylenie standardowe podniesione do kwadratu.\nSzerokość naszego prostokąta jest nieskończenie mała, co możemy zapisać jako \\(dx\\). W uproszczeniu w matematyce tak oznaczamy nieskończenie małą wartość z osi X albo nieskończenie małą zmianę wartości z osi X.\nPole takiego prostokąta zapisalibyśmy więc jako \\(P = \\mathcal{N}(100;\\ 100,\\ 15^2) \\times dx\\). Ponieważ taki prostokąt jest nieskończenie cienki, to \\(dx\\) to właściwie zero, więc i pole jednego takiego prostokąta jest zerowe. Ale jeśli dodamy nieskończoną liczbę takich prostokątów, może coś nam z tego wyjdzie. Oczywiście w praktyce one nigdy nie będą nieskończenie cienkie, ale gdy zawężamy je coraz bardziej, to nasz wynik zbliża się do prawdziwego wyniku.\nMatematyka pozwala nam zapisać nasze zamiary w dokładny sposób. Możemy na przykład zapisać coś w rodzaju „dodaję pola wszystkich nieskończenie małych prostokątów między 95 a 105”. W praktyce takie dodawanie musi mieć jakąś konkretną szerokość tych prostokątów. Jeśli założylibyśmy sobie szerokość 0,001, nasze obliczenia mogłyby wyglądać tak:\n\\[\n\\begin{align*}\nP & \\approx \\mathcal{N}(95;\\ 100,\\ 15^2) \\times 0.001 + \\\\\n& + \\mathcal{N}(95,001;\\ 100,\\ 15^2) \\times 0.001 + \\\\\n& + \\mathcal{N}(95,002;\\ 100,\\ 15^2) \\times 0.001 + \\\\\n& + \\dots + \\\\\n& + \\mathcal{N}(105;\\ 100,\\ 15^2) \\times 0.001\n\\end{align*}\n\\]\nMatematyka pozwala nam jednak naprawdę zapisać, że dodajemy wszystkie, naprawdę nieskończenie cienkie prostokąty. Dla przedziału między 95 a 105 zapis taki wygląda następująco:\n\\[\nP = \\int^{105}_{95} \\mathcal{N}(x;\\ 100,\\ 15^2) dx\n\\]\nRozciągnięta litera S oznacza sumę nieskończenie małych prostokątów, liczby w jej indeksach mówią, odkąd dokąd suma się dzieje, zaś to, co następuje dalej to to, co ma zostać zsumowane – pole prostokąta o wysokości równej gęstości prawdopodobieństwa w punkcie i nieskończenie małej szerokości. Nazywamy to całką oznaczoną. Oznaczona, bo ma jakieś konkretne granice, konkretny przedział, w którym ma robić wykonać sumowanie. Teraz i Ty możesz śmiać się słysząc, że wchodzi całka oznaczona do wagonu, a to nie jej przedział.\nŻeby lepiej było widać, co to oznacza w naszym wypadku, zapiszę to jeszcze w taki sposób:\n\\[\nP (95 &lt; IQ &lt; 105) = \\int^{105}_{95} \\mathcal{N}(IQ;\\ 100,\\ 15^2) dIQ\n\\]"
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#mniejsze-niż",
    "href": "posts/gestosc-prawdopodobienstwa.html#mniejsze-niż",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "4.1 Mniejsze niż",
    "text": "4.1 Mniejsze niż\nSpróbujmy wykorzystać współczesne narzędzia3, żeby odpowiedzieć na zadane pytanie – jakie jest prawdopodobieństwo, że losowo wybrana z populacji osoba będzie miała inteligencję 85 albo mniejszą?\n\npnorm(85, 100, 15)\n\n[1] 0.1586553\n\n\nW ten prosty sposób otrzymaliśmy odpowiedź, że jest to około \\(15,87\\%\\)."
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#większe-niż",
    "href": "posts/gestosc-prawdopodobienstwa.html#większe-niż",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "4.2 Większe niż",
    "text": "4.2 Większe niż\nMożemy też odpowiadać na pytania odwrotne, np. jakie jest prawdopodobieństwo, że losowo wybrany człowiek klasyfikuje się do Mensy (czyli ma IQ powyżej 130)? Na wykresie wygląda to tak:\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(130, max(p_tys$IQ))) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(130, max(p_tys$IQ))) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15), colour = \"white\") +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nMożemy to ugryźć na dwa sposoby. Jeśli nasz kalkulator nam to umożliwia, możemy przełączyć dystrybuantę na jej lustrzane odbicie, czyli wprost powiedzieć, żeby wyrzuciła nam wartości większe niż 130 zamiast zwyczajnych mniejszych. W R możemy to zrobić ustawiając argument lower.tail = FALSE.\n\npnorm(130, 100, 15, lower.tail = FALSE)\n\n[1] 0.02275013\n\n\nBardziej klasyczny sposób opiera się na tym, że prawdopodobieństwo sumarycznie musi wynosić 1. Możemy więc (ze zwykłej dystrybuanty) policzyć prawdopodobieństwo, że IQ wynosi mniej niż 130, a następnie otrzymaną wartość odjąć od 1. Tak też możesz zrobić korzystając z kalkulatora internetowego.\n\n1 - pnorm(130, 100, 15)\n\n[1] 0.02275013\n\n\nSwoją drogą przypomnę, że prawdopodobieństwo odpowiada tutaj odsetkowi osób w próbie lub w populacji. Możemy więc policzyć na przykład, ilu potencjalnych Mensan zamieszkuje całą Polskę.\n\n# liczba mieszkańców Polski (2022) razy odsetek potencjalnych Mensan\n37766327 * pnorm(130, 100, 15, lower.tail = FALSE)\n\n[1] 859188.9"
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#sec-pomiedzy",
    "href": "posts/gestosc-prawdopodobienstwa.html#sec-pomiedzy",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "4.3 Między danymi wartościami",
    "text": "4.3 Między danymi wartościami\nNa początku tego wpisu powiedziałem, że odchylenie standardowe \\(\\sigma = 15\\) oznacza, że około \\(\\frac{2}{3}\\) populacji znajduje się w widełkach 85-115, czyli 100±15. Skąd to wiedziałem? Cóż, możemy to policzyć.\nWiele kalkulatorów pozwala na policzenie wartości według danych widełek, ale część pozwala tylko na liczenie czystej dystrybuanty czyli prawdopodobieństwa, że wartość będzie mniejsza niż zadana. Na wykresie od naszej wartości w lewo. Możemy jednak wykorzystać dystrybuantę do policzenia odsetka populacji pomiędzy jakimiś wartościami. Robimy to w dwóch krokach.\nPo pierwsze, liczymy wartość dystrybuanty dla górnych widełek, czyli w naszym przypadku 115.\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(min(p_tys$IQ), 115)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(min(p_tys$IQ), 115)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15), colour = \"white\") +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nNastępnie liczymy dystrybuantę dla dolnych widełek, czyli w tym wypadku 85.\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(min(p_tys$IQ), 115)) +\n    exec(geom_area, !!!normal, fill = \"#3DDB6A\", xlim = c(min(p_tys$IQ), 85)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(min(p_tys$IQ), 115)) +\n    exec(geom_area, !!!normal, fill = \"#3DDB6A\", xlim = c(min(p_tys$IQ), 85)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15), colour = \"white\") +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nA na koniec odejmujemy jedno od drugiego.\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"gray95\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(85, 115)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15)) +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(p_tys, aes(IQ)) +\n    exec(geom_area, !!!normal, fill = \"#595959\") +\n    exec(geom_area, !!!normal, fill = \"#DB3D3D\", xlim = c(85, 115)) +\n    stat_function(fun = dnorm, args = list(mean = 100, sd = 15), colour = \"white\") +\n    labs(x = \"IQ\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent, limits = c(0, 0.03)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nW R możemy to zrobić tak:\n\npnorm(115, 100, 15) - pnorm(85, 100, 15)\n\n[1] 0.6826895\n\n\nJak widzimy, można z grubsza powiedzieć, że w rozkładzie normalnym ±1 odchylenie standardowe przekłada się na \\(\\frac{2}{3}\\) populacji albo \\(P(\\mu \\pm 1\\sigma) = 68,2\\%\\). Na tej samej zasadzie \\(P(\\mu \\pm 2\\sigma) = 95,4\\%\\) i \\(P(\\mu \\pm 3\\sigma) = 99,5\\%\\). Ta intuicja bywa nazywana prawem trzech sigm.\n\n\n\nRycina ilustrująca prawo trzech sigm na Wikipedii\n\n\n\n\n\n\n\n\nZadanie\n\n\n\nJakie jest prawdopodobieństwo wylosowania osoby o inteligencji:\n\nmniejszej niż -2 odchylenia standardowe (czyli \\(100 - 2 \\times 15 = 70\\)),\nwiększej niż +1 odchylenie standardowe,\nmiędzy -1 odchyleniem standardowym a średnią,\ndokładnie 100.\n\n\n\n\n\n\n\n\n\nOdpowiedź\n\n\n\n\n\n\n\\(P(IQ &lt; 70) = \\int^{70}_{-\\infty} \\mathcal{N}(x;\\ 100,\\ 15^2) dx \\approx 0,02275013 \\approx 2,28\\%\\)\n\\(P(IQ &gt; 115) = \\int^{\\infty}_{115} \\mathcal{N}(x;\\ 100,\\ 15^2) dx \\approx 0,1586553 \\approx 15,87\\%\\)\n\\(P(85 &gt; IQ &gt; 100) = \\int^{100}_{85} \\mathcal{N}(x;\\ 100,\\ 15^2) dx \\approx 0,3413447 \\approx 34,13\\%\\)\nDo tego pytania można podejść dwojako. Możemy stwierdzić, że dokładnie 100 to jest pojedynczy nieskończenie cienki prostokąt, a więc prawdopodobieństwo (jego pole) wynosi zero. Lepiej to widać na rozszerzeniu źrenic – dokładnie 7 mm oznacza brak nawet jednej milionowej odchylenia, więc jest właściwie niemożliwe. Pamiętaj – wykres gęstości działa jak histogram, więc potrzebuje widełek. Możemy jednak stwierdzić, że wyniki między 99,5 a 100,5 zaokrąglają się do 100 i to uznać za odpowiedź. W takim wypadku \\(P(IQ \\approx 100) = \\int^{100,5}_{99,5} \\mathcal{N}(x;\\ 100,\\ 15^2) dx \\approx 0.02659123 \\approx 2,66\\%\\)."
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#sec-inverse-norm",
    "href": "posts/gestosc-prawdopodobienstwa.html#sec-inverse-norm",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "4.4 Odwrotna dystrybuanta",
    "text": "4.4 Odwrotna dystrybuanta\nKażda funkcja ma swoją funkcję odwrotną4. Tak samo dystrybuanta. Do dystrybuanty wrzucamy jakąś wartość z rozkładu (np. IQ), a ona zwraca, jaki procent populacji cechuje taka lub niższa wartość. Do odwrotnej dystrybuanty wrzucamy, jaki procent populacji chcemy objąć, a ona wyrzuca maksymalną wartość z rozkładu.\nZałóżmy, że chcemy do projektu naukowego włączyć ludzi z górnych 10% rozkładu inteligencji. Czyli jak dobrze musi wypaść w teście inteligencji dana osoba, żeby zostać włączona do projektu? Jakie minimalne IQ musi uzyskać? Na takie właśnie pytania odpowiada odwrotna dystrybuanta. Przykładowy kalkulator można znaleźć tutaj, zaś w R odpowiada za nią funkcja qnorm().\n\n# patrząc od lewej\nqnorm(0.9, 100, 15)\n\n[1] 119.2233\n\n# patrząc od prawej\nqnorm(0.1, 100, 15, lower.tail = FALSE)\n\n[1] 119.2233"
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#tablice-statystyczne",
    "href": "posts/gestosc-prawdopodobienstwa.html#tablice-statystyczne",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "4.5 Tablice statystyczne",
    "text": "4.5 Tablice statystyczne\nZanim komputery stały się oczywiste, dystrybuantę rozkładu normalnego odczytywano z drukowanych tablic. Do dziś można je znaleźć w podręcznikach do statystyki czy tablicach do zadań. Warto umieć je czytać, dlatego omówię je krótko. Jest to też świetna wymówka, by omówić proces standaryzacji.\n\n4.5.1 Standaryzacja\nKażdy rozkład normalny jest opisywany przez swoją średnią i odchylenie standardowe. Możemy wymyślać dowolne rozkłady normalne manipulując tymi dwiema wartościami. Cały ten wpis posługiwaliśmy się skalą inteligencji, a więc rozkładem normalnym o parametrach \\(\\mu = 100\\) i \\(\\sigma = 15\\). Istnieją jeszcze inne rozkłady stosowane w psychologii, np. steny (m.in. wyniki NEO-PI-R, czyli testu osobowości wg Wielkiej Piątki) z parametrami \\(\\mu = 5,5\\) i \\(\\sigma = 2\\) czy teny (m.in. potężny kliniczny test MMPI) z parametrami \\(\\mu = 50\\) i \\(\\sigma = 10\\). Matką wszystkich rozkładów normalnych jest jednak tzw. standardowy rozkład normalny, czyli rozkład wyliczony tak, by jego średnia wynosiła \\(\\mu = 0\\), a odchylenie standardowe \\(\\sigma = 1\\).\nWynik z dowolnej skali normalnej da się przeliczyć na standardowy rozkład normalny, co zresztą często się robi. Prawo trzech sigm pokazuje, że człowiek może nauczyć się myśleć w odchyleniach standardowych i naukowcy prowadzący obliczenia statystyczne często myślą raczej w odchyleniach standardowych niż w jednostkach.\nPonieważ standardowy rozkład normalny jest… standardowy, to wszystkie tablice są skalibrowane pod niego. By móc skorzystać ze zwykłych tablic statystycznych, musimy umieć wystandaryzować daną wartość, czyli przedstawić ją w postaci odchyleń standardowych. Istnieją do tego kalkulatory, ale wzór na to jest naprawdę prosty. Od wyniku odejmujemy średnią naszego wyjściowego rozkładu, dzięki czemu będzie on dodatni, jeśli jest wyższy od średniej i ujemny, jeśli jest niższy. Potem dzielimy otrzymaną liczbę przez odchylenie standardowe i już. Tak otrzymaną wartość oznaczamy zwykle literą \\(Z\\) (tzw. Z-score).\n\\[\nZ(x) = \\frac{x - \\mu}{\\sigma}\n\\]"
  },
  {
    "objectID": "posts/gestosc-prawdopodobienstwa.html#korzystanie",
    "href": "posts/gestosc-prawdopodobienstwa.html#korzystanie",
    "title": "Gęstość prawdopodobieństwa i rozkład normalny",
    "section": "4.6 Korzystanie",
    "text": "4.6 Korzystanie\n\n\nKod\nnorm_table &lt;- seq(0, 3.09, by = 0.01) %&gt;%\n    pnorm() %&gt;%\n    matrix(\n        ncol = 10,\n        byrow = TRUE,\n        dimnames = list(\n            format(seq(0, 3, 0.1), decimal.mark = \",\"),\n            paste0(\"0,0\", 0:9)\n        )\n    ) %&gt;%\n    round(4) %&gt;%\n    format(decimal.mark = \",\")\n\nnorm_table[\"1,6\", \"0,07\"] &lt;- cell_spec(norm_table[\"1,6\", \"0,07\"], bold = TRUE)\nnorm_table[\"1,3\", \"0,03\"] &lt;- cell_spec(norm_table[\"1,3\", \"0,03\"], bold = TRUE)\n\nkbl(norm_table, escape = FALSE) %&gt;%\n    column_spec(1, bold = TRUE)\n\n\n\n\nTabela 2: Przykładowa tablica dystrybuanty rozkładu normalnego z zaznaczonymi wartościami dla Z = 1,67 i Z = 1,33.\n\n\n\n\n\n\n\n0,00\n0,01\n0,02\n0,03\n0,04\n0,05\n0,06\n0,07\n0,08\n0,09\n\n\n\n\n0,0\n0,5000\n0,5040\n0,5080\n0,5120\n0,5160\n0,5199\n0,5239\n0,5279\n0,5319\n0,5359\n\n\n0,1\n0,5398\n0,5438\n0,5478\n0,5517\n0,5557\n0,5596\n0,5636\n0,5675\n0,5714\n0,5753\n\n\n0,2\n0,5793\n0,5832\n0,5871\n0,5910\n0,5948\n0,5987\n0,6026\n0,6064\n0,6103\n0,6141\n\n\n0,3\n0,6179\n0,6217\n0,6255\n0,6293\n0,6331\n0,6368\n0,6406\n0,6443\n0,6480\n0,6517\n\n\n0,4\n0,6554\n0,6591\n0,6628\n0,6664\n0,6700\n0,6736\n0,6772\n0,6808\n0,6844\n0,6879\n\n\n0,5\n0,6915\n0,6950\n0,6985\n0,7019\n0,7054\n0,7088\n0,7123\n0,7157\n0,7190\n0,7224\n\n\n0,6\n0,7257\n0,7291\n0,7324\n0,7357\n0,7389\n0,7422\n0,7454\n0,7486\n0,7517\n0,7549\n\n\n0,7\n0,7580\n0,7611\n0,7642\n0,7673\n0,7704\n0,7734\n0,7764\n0,7794\n0,7823\n0,7852\n\n\n0,8\n0,7881\n0,7910\n0,7939\n0,7967\n0,7995\n0,8023\n0,8051\n0,8078\n0,8106\n0,8133\n\n\n0,9\n0,8159\n0,8186\n0,8212\n0,8238\n0,8264\n0,8289\n0,8315\n0,8340\n0,8365\n0,8389\n\n\n1,0\n0,8413\n0,8438\n0,8461\n0,8485\n0,8508\n0,8531\n0,8554\n0,8577\n0,8599\n0,8621\n\n\n1,1\n0,8643\n0,8665\n0,8686\n0,8708\n0,8729\n0,8749\n0,8770\n0,8790\n0,8810\n0,8830\n\n\n1,2\n0,8849\n0,8869\n0,8888\n0,8907\n0,8925\n0,8944\n0,8962\n0,8980\n0,8997\n0,9015\n\n\n1,3\n0,9032\n0,9049\n0,9066\n0,9082\n0,9099\n0,9115\n0,9131\n0,9147\n0,9162\n0,9177\n\n\n1,4\n0,9192\n0,9207\n0,9222\n0,9236\n0,9251\n0,9265\n0,9279\n0,9292\n0,9306\n0,9319\n\n\n1,5\n0,9332\n0,9345\n0,9357\n0,9370\n0,9382\n0,9394\n0,9406\n0,9418\n0,9429\n0,9441\n\n\n1,6\n0,9452\n0,9463\n0,9474\n0,9484\n0,9495\n0,9505\n0,9515\n0,9525\n0,9535\n0,9545\n\n\n1,7\n0,9554\n0,9564\n0,9573\n0,9582\n0,9591\n0,9599\n0,9608\n0,9616\n0,9625\n0,9633\n\n\n1,8\n0,9641\n0,9649\n0,9656\n0,9664\n0,9671\n0,9678\n0,9686\n0,9693\n0,9699\n0,9706\n\n\n1,9\n0,9713\n0,9719\n0,9726\n0,9732\n0,9738\n0,9744\n0,9750\n0,9756\n0,9761\n0,9767\n\n\n2,0\n0,9772\n0,9778\n0,9783\n0,9788\n0,9793\n0,9798\n0,9803\n0,9808\n0,9812\n0,9817\n\n\n2,1\n0,9821\n0,9826\n0,9830\n0,9834\n0,9838\n0,9842\n0,9846\n0,9850\n0,9854\n0,9857\n\n\n2,2\n0,9861\n0,9864\n0,9868\n0,9871\n0,9875\n0,9878\n0,9881\n0,9884\n0,9887\n0,9890\n\n\n2,3\n0,9893\n0,9896\n0,9898\n0,9901\n0,9904\n0,9906\n0,9909\n0,9911\n0,9913\n0,9916\n\n\n2,4\n0,9918\n0,9920\n0,9922\n0,9925\n0,9927\n0,9929\n0,9931\n0,9932\n0,9934\n0,9936\n\n\n2,5\n0,9938\n0,9940\n0,9941\n0,9943\n0,9945\n0,9946\n0,9948\n0,9949\n0,9951\n0,9952\n\n\n2,6\n0,9953\n0,9955\n0,9956\n0,9957\n0,9959\n0,9960\n0,9961\n0,9962\n0,9963\n0,9964\n\n\n2,7\n0,9965\n0,9966\n0,9967\n0,9968\n0,9969\n0,9970\n0,9971\n0,9972\n0,9973\n0,9974\n\n\n2,8\n0,9974\n0,9975\n0,9976\n0,9977\n0,9977\n0,9978\n0,9979\n0,9979\n0,9980\n0,9981\n\n\n2,9\n0,9981\n0,9982\n0,9982\n0,9983\n0,9984\n0,9984\n0,9985\n0,9985\n0,9986\n0,9986\n\n\n3,0\n0,9987\n0,9987\n0,9987\n0,9988\n0,9988\n0,9989\n0,9989\n0,9989\n0,9990\n0,9990\n\n\n\n\n\n\n\n\n\n\nSpróbujmy wykorzystać tablicę 2, żeby rozwiązać problem z prawdopodobieństwem wylosowania osoby z IQ między 80 a 125. Najpierw musimy przeliczyć wartości naszych widełek ze skali IQ na skalę standardową.\n\\[\n\\begin{align*}\nZ(125) & = \\frac{125 - 100}{15} = \\frac{25}{15} \\approx 1,67 \\\\\nZ(80) & = \\frac{80 - 100}{15} = \\frac{-20}{15} \\approx -1,33\n\\end{align*}\n\\]\nTeraz możemy odczytać stosowne wartości dystrybuanty z tabeli. Zacznijmy od górnych widełek. Z lewej strony szukam wartości 1,6 i patrzę w kolumnę oznaczoną 0,07, bo razem daje to 1,67. Więcej problemu sprawi wartość -1,33, bo jest ujemna, zaś tabela składa się z wartości dodatnich. Jednak wiemy, że rozkład normalny jest symetryczny, czyli wartość przy \\(Z = -1,33\\) będzie taka sama, jak odwrotność wartości dla \\(Z = 1,33\\). Słowny opis jest mało zrozumiały, więc posłużmy się wykresem.\n\n\nKod\nggplot(tibble(X = seq(-3.2, 3.2, 0.01)), aes(X)) +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"gray95\") +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#DB3D3D\", xlim = c(-3.2, -1.33)) +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#DB3D3D\", xlim = c(1.33, 3.2)) +\n    stat_function(fun = dnorm) +\n    labs(x = \"Z\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent) +\n    scale_x_continuous(breaks = c(-3, -2, -1.33, 0, 1.33, 2, 3)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(tibble(X = seq(-3.2, 3.2, 0.01)), aes(X)) +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#595959\") +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#DB3D3D\", xlim = c(-3.2, -1.33)) +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#DB3D3D\", xlim = c(1.33, 3.2)) +\n    stat_function(fun = dnorm, colour = \"white\") +\n    labs(x = \"Z\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent) +\n    scale_x_continuous(breaks = c(-3, -2, -1.33, 0, 1.33, 2, 3)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nObszary zaznaczone na czerwono są identyczne. Możemy więc policzyć wartość dla -1,33 stosując sztuczkę z podrozdziału 4.3. – wziąć wartość dla 1,33 i odjąć ją od jedności. Otrzymamy w ten sposób powierzchnię z prawej, która jest taka sama, jak powierzchnia z lewej.\nW ten sposób możemy odczytać, że górne widełki mają wartość dystrybuanty \\(P(x &lt; 1,67) = 0,9525\\), zaś dolne \\(P(x &lt; -1,33) = 1 - 0,9082 = 0,0918\\). Teraz możemy odjąć jedno od drugiego, żeby otrzymać prawdopodobieństwo, że wylosowane IQ będzie między 80 a 125.\n\\[\n\\begin{align*}\nP(80 &lt; IQ &lt; 125) & = \\\\\n= P(-1,33 &lt; Z(IQ) &lt; 1,67) & = \\\\\n& = 0,9525 - 0,0918 = \\\\\n& = 0,8607\n\\end{align*}\n\\]\n\n\nKod\nggplot(tibble(X = seq(-3.2, 3.2, 0.01)), aes(X)) +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"gray95\") +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#DB3D3D\", xlim = c(-1.33, 1.67)) +\n    stat_function(fun = dnorm) +\n    labs(x = \"Z\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent) +\n    scale_x_continuous(breaks = seq(-3, 3)) +\n    theme_Publication()\n\n\n\n\n\n\n\n\n\n\n\nKod\nggplot(tibble(X = seq(-3.2, 3.2, 0.01)), aes(X)) +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#595959\") +\n    geom_area(stat = \"function\", fun = dnorm, fill = \"#DB3D3D\", xlim = c(-1.33, 1.67)) +\n    stat_function(fun = dnorm, colour = \"white\") +\n    labs(x = \"Z\", y = \"gęstość prawdopodobieństwa\") +\n    scale_y_continuous(labels = scales::percent) +\n    scale_x_continuous(breaks = seq(-3, 3)) +\n    theme_dark_blue()\n\n\n\n\n\n\n\n\n\nJak wygląda dokładność tego wyniku w porównaniu do tego wyrzucanego przez R? Całkiem nieźle. Rozbieżności pojawiają się dopiero na czwartym miejscu po przecinku.\n\n# ze standardowego rozkładu normalnego\npnorm(1.67) - pnorm(-1.33)\n\n[1] 0.8607812\n\n# z rozkładu inteligencji\npnorm(125, 100, 15) - pnorm(80, 100, 15)\n\n[1] 0.8609984\n\n\n\n\n\n\n\n\nZadanie\n\n\n\nKorzystając z tablic rozkładu normalnego oblicz, jakie jest prawdopodobieństwo, że losowo wybrana osoba będzie miała IQ typu „tak blisko!”, czyli między 125 a 129. Zanim sprawdzisz odpowiedź niżej, spróbuj otrzymać ją na kalkulatorze lub w R.\n\n\n\n\n\n\n\n\nOdpowiedź\n\n\n\n\n\n\npnorm(129, 100, 15) - pnorm(125, 100, 15)\n\n[1] 0.02119278"
  },
  {
    "objectID": "posts/powtarzalny-tekst.html",
    "href": "posts/powtarzalny-tekst.html",
    "title": "Nie pisz (prawie) tego samego 100 razy",
    "section": "",
    "text": "W tym tekście opisuję generowanie nowego tekstu o określonej strukturze. Do wyszukiwania i zmieniania określone rzeczy w istniejącym już tekście służą wyrażenia regularne (RegEx), o których piszę w tym tekście. Łącznie to bardzo proste, a jednocześnie bardzo potężne narzędzia, które pozwalają szybko i niskim kosztem odjąć dużo bezsensownej, mechanicznej pracy każdemu. Nie tylko naukowcom czy studentom, ale każdemu, kto pisze tekst na komputerze.\nTo, co tutaj opiszę, pierwszy raz poważnie wykorzystałem, gdy pomagałem swojej siostrze w pracy. Miała ona wydłużyć plik, w którym zapisywane były teksty, jakie mają pojawić się w live’ie tego dnia (siostra pracuje przy kanale na YouTube). Plik ten miał prostą strukturę.\nGdy całość się kończyła, osoba odpowiedzialna dopisywała tę strukturę na ileś dni do przodu i tak co jakiś czas. Jest to ten rodzaj pracy, którego nie znoszę i który jest łatwy do zautomatyzowania. Można to zrobić z kilku powodów, z których najważniejszy jest ten – plik miał przewidywalną, z góry określoną strukturę. Miał konkretne stałe elementy i konkretne elementy zmienne. Tutaj elementem zmiennym była data, która zmienia się w sposób przewidywalny1. Mamy więc określony wzór, schemat, który tylko musimy wypełnić datami. To też zrobiłem i w ten sposób wygenerowałem dla siostry plik dla rok do przodu. Powiedziała w pracy, że miała trochę czasu, to dopisała więcej. Podobno się zdziwili.\nTa cecha, tzn. przewidywalność jakiegoś tekstu, pozwala nam zautomatyzować jego pisanie. Nieważne, czy tym tekstem są oznaczenia kolumn (MMPI_1, MMPI_2, …, MMPI_567), czy złożone zagnieżdżone struktury np. pytań i odpowiedzi w ankiecie, jeśli tekst jest przewidywalny, da się go wygenerować."
  },
  {
    "objectID": "posts/powtarzalny-tekst.html#pętla-for",
    "href": "posts/powtarzalny-tekst.html#pętla-for",
    "title": "Nie pisz (prawie) tego samego 100 razy",
    "section": "3.1 Pętla for",
    "text": "3.1 Pętla for\nPętla for jest najprostszym rodzajem pętli i tym, z czego będziemy stale korzystać. Omówmy sobie ją na przykładzie generowania nazw kolumn.\n\nfor i in range(1, 11):\n    print(f\"MMPI_{i}\")\n\nMMPI_1\nMMPI_2\nMMPI_3\nMMPI_4\nMMPI_5\nMMPI_6\nMMPI_7\nMMPI_8\nMMPI_9\nMMPI_10\n\n\ni jest nazwą dla zmiennej, która po kolei przyjmie wartości od 1 do 10. Najpierw wszystko, co znajduje się w pętli, wykona się tak, jakby i miało wartość 1. Potem wykona się to znowu, ale z i = 2 itd. To jest podstawowy sposób działania zmiennej for. Potem mamy słowo in, a za nim zbiór wartości, które i ma po kolei przyjmować. W tym wypadku tym zbiorem jest funkcja range(), która sama generuje nam liczby od 1 do 10.\nDlaczego jednak napisałem range(1, 11) a nie range(1, 10)? Python działa tutaj specyficznie. Wynika to z faktu, że w informatyce liczy się od 0, nie od 1. Jeśli do funkcji range() wrzucę tylko jedną liczbę, czyli na przykład range(10), to dostanę 10 elementów. Ponieważ jednak pierwszy element to 0, to będą to liczby od 0 do 9. Mogę podać dwie liczby, żeby powiedzieć funkcji range(), od czego ma zacząć, ale wtedy muszę mieć w głowie, że skoro range(0, 10) oznacza 10 liczb od 0 do 9, to liczby od 1 do 10 muszę zapisać jako range(1, 11). Innymi słowy koniec skali nie wlicza się do zakresu.\nJeśli piszemy to w konsoli, a nie w pliku, możemy zapisać taką pętlę w jednej linijce – for i in range(1, 11): print(f\"MMPI_{i}\"). Możemy też zapisać samo for i in range(1, 11): (nie zapominając o dwukropku) i potwierdzić enterem. W obu wypadkach wyświetli nam się w konsoli wielokropek i będziemy mogli dopisywać kolejne komendy z pętli. Gdy będziemy usatysfakcjonowani, klikamy enter po raz kolejny, a pętla wykonuje się.\nTak jak wspomniałem, domyślnie print() wyrzuca do konsoli to, co tej funkcji podaliśmy, dodając na koniec nową linię. Możemy jednak chcieć, żeby nasze elementy pojawiły się po przecinku albo oddzielone spacjami (albo jedno i drugie) i wtedy możemy zmienić argument end.\n\nfor i in range(1, 11):\n    print(f\"MMPI_{i}\", end = \", \")\n\nMMPI_1, MMPI_2, MMPI_3, MMPI_4, MMPI_5, MMPI_6, MMPI_7, MMPI_8, MMPI_9, MMPI_10, \n\n\nCo prawda po ostatnim elemencie też dostajemy przecinek i spację, ale to już możemy usunąć ręcznie. W Pythonie też da się to zaprogramować, ale nie chcę za bardzo gmatwać."
  },
  {
    "objectID": "posts/powtarzalny-tekst.html#pętla-while",
    "href": "posts/powtarzalny-tekst.html#pętla-while",
    "title": "Nie pisz (prawie) tego samego 100 razy",
    "section": "3.2 Pętla while",
    "text": "3.2 Pętla while\nPętla while to bardziej podstawowy, prosty rodzaj pętli. Większość pętli while da się napisać w formie pętli for, dlatego nie będę się nad tym jakoś szczególnie rozwodził, ale warto wiedzieć, że coś takiego istnieje. Napiszmy przykład z poprzedniej sekcji w postaci pętli while.\n\ni = 1\nwhile i &lt;= 10:\n    print(f\"MMPI_{i}\")\n    i = i + 1  # ewentualnie i += 1\n\nMMPI_1\nMMPI_2\nMMPI_3\nMMPI_4\nMMPI_5\nMMPI_6\nMMPI_7\nMMPI_8\nMMPI_9\nMMPI_10\n\n\nW pętli while potrzebujemy jakiejś wcześniej określonej zmiennej, w tym wypadku i. Pierwszą rzeczą, którą while robi, jest sprawdzenie, czy warunek jest prawdziwy. Prawdą jest, że 1 jest mniejsze lub równe 10, więc while puszcza wszystko, co znajduje się w środku pętli. Instrukcja print() jest identyczna. Kolejna linijka może wydawać się nieco tajemnicza. Służy ona powiększeniu i o 1. Matematycznie zapis i = i + 1 może wydawać się dziwny, ale trzeba pamiętać, że = nie oznacza tutaj porównania (to się robi poprzez ==), tylko przypisanie. Można więc tę komendę przeczytać „Niech i przyjmie wartość równą aktualnej wartości i plus jeden”. W skrócie możemy to zapisać jako i += 1. Po co to robimy? Bo w następnym kroku pętla while znów sprawdzi, czy warunek jest prawdziwy. Teraz i = 2, a 2 to ciągle mniej niż 10, więc pętla wykona się znów. Tak będzie robić aż do momentu, w którym warunek nie będzie prawdziwy, a wiec w tym wypadku aż i nie przyjmie wartości 11. Jeśli nie umieściłbym w kodzie linijki i = i + 1, warunek i &lt;= 10 byłby zawsze prawdziwy i pętla działałaby wiecznie. Czy raczej do wyczerpania pamięci."
  },
  {
    "objectID": "posts/powtarzalny-tekst.html#ankieta-w-psytoolkit",
    "href": "posts/powtarzalny-tekst.html#ankieta-w-psytoolkit",
    "title": "Nie pisz (prawie) tego samego 100 razy",
    "section": "4.1 Ankieta w PsyToolKit",
    "text": "4.1 Ankieta w PsyToolKit\nPokażę teraz, jak sprawnie przerobić kwestionariusz na ankietę w PsyToolKit. Najsprawniej byłoby, co prawda, użyć programiku PsyToolKit Questionnaire Formatter, który opiera się na tym, co tutaj opisuję. Poznajmy ten mechanizm od kuchni, żeby w razie czego móc go dopasować do własnych, specyficznych celów, niekoniecznie związanych w ogóle z PsyToolKit.\nZałóżmy, że chcielibyśmy wykorzystać w naszym badaniu kwestionariusz samooceny Rosenberga (1965). Musimy go w takim razie zapisać tak, jak PsyToolKit każe nam formatować pytania do ankiety. Struktura pytania w PsyToolKit wygląda tak:\nl: RSES_1\nt: radio\nq: I feel that I am a person of worth, at least on an equal plane with others.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\nPo pierwsze mamy l, czyli label. Posłuży nam to jako wewnętrzna „nazwa” pozycji testowej i nagłówek kolumny w bazie danych. Dalej mamy t, czyli type, gdzie radio oznacza pytanie jednokrotnego wyboru. Inne typy znajdziemy w dokumentacji. Następnie mamy q, czyli question, właściwa treść pozycji testowej i pod nią odpowiedzi wypisane od myślników. Możemy tu dodawać inne rzeczy (np. o: random, żeby kolejność odpowiedzi była losowa), ale załóżmy, że na ten moment tyle nam wystarczy.\nPo pierwsze spróbujmy zidentyfikować, co w naszym schemacie jest stałe i co się zmienia. Tutaj zmieniają się dwie rzeczy – treść pozycji testowej i numerek przy RSES. Cała reszta jest identyczna dla każdej pozycji testowej.\nSkoro musimy mieć w naszych pytaniach treść pozycji testowej, musimy nasz kwestionariusz wkleić do skryptu. Zapiszemy go w postaci listy.\n\nRSES = [\n    \"I feel that I am a person of worth, at least on an equal plane with others.\",\n    \"I feel that I have a number of good qualities.\",\n    \"All in all, I am inclined to feel that I am a failure.\",\n    \"I am able to do things as well as most other people.\",\n    \"I feel I do not have much to be proud of.\",\n    \"I take a positive attitude toward myself.\",\n    \"On the whole, I am satisfied with myself.\",\n    \"I wish I could have more respect for myself.\",\n    \"I certainly feel useless at times.\",\n    \"At times I think I am no good at all.\"\n]\n\nCała lista jest nawiasach kwadratowych, każdy item jest w cudzysłowie, zaś itemy rozdzielone są przecinkami. Całą listę zapisałem do zmiennej o nazwie RSES. Teraz możemy powiedzieć Pythonowi, żeby zrobił całą serię pytań w stylu PsyToolKit, gdzie po q za każdym razem wstawi jedną z pozycji testowych.\n\nfor item in RSES:\n    print(\"l: RSES_1\")\n    print(\"t: radio\")\n    print(f\"q: {item}\")\n    print(\"- Strongly Agree\\n- Agree\\n- Disagree\\n- Strongly Disagree\")\n    print() # pusta linijka\n\nl: RSES_1\nt: radio\nq: I feel that I am a person of worth, at least on an equal plane with others.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: I feel that I have a number of good qualities.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: All in all, I am inclined to feel that I am a failure.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: I am able to do things as well as most other people.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: I feel I do not have much to be proud of.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: I take a positive attitude toward myself.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: On the whole, I am satisfied with myself.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: I wish I could have more respect for myself.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: I certainly feel useless at times.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_1\nt: radio\nq: At times I think I am no good at all.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\n\n\nJak widzimy, nasza zmienna w pętli (1) nie musi nazywać się i oraz (2) nie musi być liczbą. Jak widzimy, możemy wykonać pętlę za każdym razem przypisując do zmiennej kolejny tekst z listy. Każdą linijkę możemy zapisać w osobnej komendzie print() lub też całość wpisać w jedną komendę, zaznaczając nowe linijki za pomocą \\n. Tak zrobiłem w przedostatniej linijce.\nNasz wynik ma jednak problem – każde pytanie nazywa się RSES_1. Liczba po RSES_ musi się zmieniać. Tym razem jest to trudniejsze niż wcześniej, bo item nie jest tutaj liczbą, tylko treścią pytania, więc nie możemy zapisać RSES_{item}. Z pomocą przychodzi nam jednak funkcja enumerate(). Pozwala ona przerobić listę na tzw. krotki (ang. tuples, tutaj 2-tuples czyli dwukrotki). Każda taka dwukrotka zawiera numer pozycji na liście (licząc od 0) oraz samą pozycję. Numer jest pierwszy, więc dostaniemy się do niego pisząc item[0]. Jeśli chcemy dostać treść pozycji testowej, zapiszemy item[1]. Całość wyglądałaby więc tak:\n\nfor item in enumerate(RSES):\n    print(f\"l: RSES_{item[0] + 1}\")\n    print(\"t: radio\")\n    print(f\"q: {item[1]}\")\n    print(\"- Strongly Agree\\n- Agree\\n- Disagree\\n- Strongly Disagree\")\n    print() # pusta linijka\n\nl: RSES_1\nt: radio\nq: I feel that I am a person of worth, at least on an equal plane with others.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_2\nt: radio\nq: I feel that I have a number of good qualities.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_3\nt: radio\nq: All in all, I am inclined to feel that I am a failure.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_4\nt: radio\nq: I am able to do things as well as most other people.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_5\nt: radio\nq: I feel I do not have much to be proud of.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_6\nt: radio\nq: I take a positive attitude toward myself.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_7\nt: radio\nq: On the whole, I am satisfied with myself.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_8\nt: radio\nq: I wish I could have more respect for myself.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_9\nt: radio\nq: I certainly feel useless at times.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\nl: RSES_10\nt: radio\nq: At times I think I am no good at all.\n- Strongly Agree\n- Agree\n- Disagree\n- Strongly Disagree\n\n\n\nZwróćmy uwagę, że w pierwszym print() napisałem item[0] + 1. item[0] to numer pozycji testowej, ale czemu + 1? Bo liczenie w informatyce zaczyna się od 0 (co ciągle powoduje problemy u całej reszty ludzkości), więc jeśli chcę mieć numerację od 1 do 10 zamiast od 0 do 9, to do każdego numeru muszę dodać 1."
  },
  {
    "objectID": "posts/powtarzalny-tekst.html#zapisywanie-do-pliku",
    "href": "posts/powtarzalny-tekst.html#zapisywanie-do-pliku",
    "title": "Nie pisz (prawie) tego samego 100 razy",
    "section": "4.2 Zapisywanie do pliku",
    "text": "4.2 Zapisywanie do pliku\nWynik działania takiej funkcji możemy od razu zapisać do pliku tekstowego za pomocą specjalnego operatora &gt; w PowerShell4. Da się to zrobić nie wychodząc z Pythona, ale to niepotrzebnie skomplikowane. Druga opcja to po prostu skopiować wygenerowany tekst z konsoli. Jeśli ktoś rzadko z niej korzysta, to ostrzegam, że do kopiowania i wklejania zamiast Ctrl+C i Ctrl+V w konsoli używamy Ctrl+Shift+C i Ctrl+Shift+V. Głównie dlatego, że Ctrl+C ma tam inną funkcję – przerywa aktualnie wykonywane zadanie. Ten sposób może jednak nie być odpowiedni, jeśli tekst jest długi, bo wtedy konsola może zjeść nam kilka (lub bardzo dużo) linijek. Jak więc wykorzystać &gt;? W PowerShell (nie w konsoli Pythona! w zwykłym, gołym PowerShell) wpisujemy coś takiego:\nPython \"C:\\ścieżka\\do\\skryptu.py\" &gt; \"C:\\ścieżka\\do\\pliku.txt\"\nJako podpowiedź mogę podrzucić, że Windows 11 pozwala kopiować ścieżki po kliknięciu na plik prawym przyciskiem myszy. W Windowsie 10 też możemy sobie w ten sposób ułatwić życie, tylko klikając prawy przycisk myszy musimy jeszcze przytrzymać shift. Ostateczna komenda mogłaby więc wyglądać tak:\nPython \"C:\\Users\\Jakub\\Desktop\\RSES.py\" &gt; \"C:\\Users\\Jakub\\Desktop\\RSES.txt\"\nPowoduje to zapisanie tego, co normalnie skrypt wydrukowałby w konsoli, w pliku RSES.txt na pulpicie. Rozszerzenie .txt jest konieczne. Oczywiście jeśli Twoja nazwa użytkownika to Jakub. Niestety wpisywanie własnych ścieżek jest konieczne."
  },
  {
    "objectID": "posts/powtarzalny-tekst.html#daty",
    "href": "posts/powtarzalny-tekst.html#daty",
    "title": "Nie pisz (prawie) tego samego 100 razy",
    "section": "4.3 Daty",
    "text": "4.3 Daty\nWróćmy do pierwotnego przykładu z plikiem mojej siostry. Jest to przykład o tyle specyficzny, że zmiennym elementem jest tam data. Daty zmieniają się przewidywalne, ale potrzebują specjalnych funkcji, które ogarną takie rzeczy jak to, że różne miesiące mają różną liczbę dni, istnieją lata przestępne itd. Kiedy rzeczywiście miałem ten problem, użyłem funkcji date w Linuksie, która sama z siebie pozwala na robienie takich rzeczy. Większość osób (niestety) nie korzysta z linuksa, dlatego na potrzeby tego wpisu zaadaptuję to rozwiązanie do Pythona. Albo chociaż spróbuję.\nŻeby operować na datach, musimy na szczycie skryptu (lub najpierw w konsoli) zapisać:\n\nimport datetime\n\nZaładuje to pakiet datetime pozwalający operować na datach. Robimy to tylko raz na daną sesję, czyli jak raz załadujemy ten pakiet, możemy z niego korzystać dopóty, dopóki nie wyjdziemy z Pythona. Jeśli chcemy dostać się do funkcji z pakietu datetime, musimy zapisać je z „przedrostkiem\" datetime., jak zobaczymy za chwilę.\nPrzypomnijmy strukturę pliku, który chcemy stworzyć:\nData:11.05.2023\nI\n\nII\n\nIII\nPo pierwsze musimy ustalić, od jakiej daty chcemy zacząć. Możemy wykorzystać dzisiejszą datę wpisując datetime.date.today(). Możemy też wybrać datę początkową arbitralnie, używając czegoś w rodzaju datetime.date(2023, 5, 11). Data jest w kolejności ISO 8601, czyli rok, miesiąc, dzień.\nPo drugie będziemy musieli dodawać do naszej daty dni. Robimy to funkcją datetime.timedelta(days = 1). W tej formie do naszej daty dodamy jeden dzień. Takie coś rzeczywiście do daty dodajemy, czyli piszemy na przykład datetime.date(2023, 5, 11) + datetime.timedelta(days = 1). Wynikiem będzie tutaj 12 maja 2023 roku.\nPo trzecie nasza data musi być w określonym formacie, w tym wypadku DD.MM.RRRR. Domyślnie daty wyświetlają się w formacie ISO 8601, czyli RRRR-MM-DD. Formatować daty można metodą strftime(). Metody to szczególny rodzaj funkcji, który wykorzystujemy tak, że doklejamy je po kropce do nazwy naszego obiektu np. z datą. Najlepiej będzie to widać w przykładzie. Do samej metody wrzucamy zakodowany format, w jakim datę chcemy uzyskać. Wykorzystamy tutaj specjalne kody, których listę możemy znaleźć tutaj. Potrzebny nam format zakodujemy jako \"%d.%m.%Y\".\nZbierając to wszystko do kupy uzyskujemy coś takiego:\n\nimport datetime\n\nstart_date = datetime.date(2023, 5, 11)\n\nfor i in range(7):\n    date = start_date + datetime.timedelta(days = i)\n    date_formatted = date.strftime(\"%d.%m.%Y\")\n    print(f\"Data:{date_formatted}\")\n    print(\"I\\n\\nII\\n\\nIII\\n\\n\")\n\nData:11.05.2023\nI\n\nII\n\nIII\n\n\nData:12.05.2023\nI\n\nII\n\nIII\n\n\nData:13.05.2023\nI\n\nII\n\nIII\n\n\nData:14.05.2023\nI\n\nII\n\nIII\n\n\nData:15.05.2023\nI\n\nII\n\nIII\n\n\nData:16.05.2023\nI\n\nII\n\nIII\n\n\nData:17.05.2023\nI\n\nII\n\nIII\n\n\n\n\nWykorzystałem tutaj kilka zmiennych, które nazwałem start_date, date i date_formatted. Nazwy zmiennych mogą być jakiekolwiek. Wybrałem takie, żeby to było czytelne. Jak to w programowaniu, możemy to napisać na parę sposobów. Dla przykładu tutaj datę już sformatowaną zapisałem w osobnej zmiennej, ale mógłbym też napisać:\n\nimport datetime\n\nstart_date = datetime.date(2023, 5, 11)\n\nfor i in range(7):\n    date = start_date + datetime.timedelta(days = i)\n    print(f\"Data:{date.strftime('%d.%m.%Y')}\")\n    print(\"I\\n\\nII\\n\\nIII\\n\\n\")\n\n…albo w ogóle wszystko zapisać już wewnątrz pętli:\n\nimport datetime\n\nfor i in range(7):\n    date = datetime.date(2023, 5, 11) + datetime.timedelta(days = i)\n    print(f\"Data:{date.strftime('%d.%m.%Y')}\")\n    print(\"I\\n\\nII\\n\\nIII\\n\\n\")\n\n…i to też zadziała. Zależy co uznajemy za bardziej czytelne. Zwróćmy uwagę, że piszemy days = i, a nie days = {i}. Nawiasy klamrowe potrzebne są tylko w f-strings. Pamiętamy jeszcze, że domyślnie range(7) generuje liczby od 0 do 6, więc na początku timedelta() dodaje 0 dni, potem 1 dzień, 2 dni i aż do 6 dni. Tym razem jest to nam na rękę, bo dzięki temu pierwszą datą jest wybrana przez nas data, a nie dzień później. Uzyskujemy więc tydzień rozpiski. Siła automatyzacji polega na tym, że mając te 4 linijki kodu, taki sam nakład pracy potrzebny jest do zrobienia takiej rozpiski dla tygodnia, miesiąca czy 30 lat5."
  },
  {
    "objectID": "posts/metaprogramowanie.html",
    "href": "posts/metaprogramowanie.html",
    "title": "Metaprogramowanie w R",
    "section": "",
    "text": "Wyobraźmy sobie, że chcemy napisać własną funkcję, która będzie za nas odwalać jakąś głupią robotę. Możemy na przykład mieć jakiś skomplikowany proces modelowania, który zawsze wygląda podobnie. Prostszy przykład – załóżmy, że zawsze liczymy dokładnie taki sam zestaw statystyk opisowych i chcemy mieć już funkcję na przyszłość, która policzy je nam sama. Dokładnie tak zrobiłem, jak na III roku psychologii miałem kolokwium ze statystyki. Zapiszmy więc taką funkcję.\n\nopisowe &lt;- function(df, group, ...) {\n  df %&gt;%\n    group_by(group) %&gt;%\n    summarise(\n      across(\n        ..., # wybrane kolumny\n        .fns = list(\n          N = \\(x) sum(!is.na(x)), # liczba niepustych\n          M = \\(x) mean(x, na.rm = TRUE), # średnia\n          SD = \\(x) sd(x, na.rm = TRUE), # odchylenie standardowe\n          A = agricolae::skewness, # skośność\n          K = agricolae::kurtosis, # kurtoza\n          `NA` = \\(x) sum(is.na(x)) # liczba brakujących\n        )\n      )\n    )\n}\n\nPrzejdźmy sobie przez tę funkcję krok po kroku. Po pierwsze dałem jej 3 argumenty – df, group i tajemnicze …. df To baza danych, group to kolumna, po której chcemy agregować dane (patrz tutaj). Ostatni argument to tzw. ellipsis czyli wielokropek i czasem spotykamy go w dokumentacji, np. w funkcji select(). W wielokropku chodzi o to, że mogę tam wsadzić dowolną liczbę rzeczy, np. dowolnie wiele nazw kolumn. W naszym wypadku będziemy wrzucać tam nazwy kolumn, dla których chcemy liczyć nasze statystyki. Nie wiemy, czy będzie to jedna kolumna, 10 kolumn, czy wyrażenie tidyselect (np. starts_with(\"H\")) więc używamy wielokropka.\nBazę danych grupujemy i wrzucamy do funkcji agregującej summarise(). Ponieważ chcemy wykonać wiele razy ten sam zestaw obliczeń na wielu kolumnach, korzystamy z across() (patrz tutaj). Dalej across() przyjmuje listę funkcji, które chcemy zastosować. Listę, czyli wszystkie komendy zamykam w list()1. Większość funkcji podaję jako funkcje anonimowe (patrz tutaj), bo albo muszę podać dodatkowe argumenty, albo to funkcje kombinowane, np. \\(x) sum(!is.na(x)).\nCiekawsze rzeczy to:\n\n\\(x) sum(!is.na(x)) – sama funkcja is.na() zwraca TRUE lub FALSE w zależności od tego, czy dana wartość jest brakująca. Pod maską TRUE to 1, a FALSE to 0, więc jeśli zsumujemy wynik działania is.na(), to dostaniemy liczbę TRUE. Ponieważ is.na() zwraca TRUE, jeśli dane są brakujące, to sumując is.na() dostałbym liczbę NA. Dlatego zaprzeczam is.na() operatorem ! (patrz tutaj).\nSkośność i kurtoza – to są jedyne funkcje, które stosuję tutaj jak w mordę strzelił, bez kombinowania, więc podaję je bez nawiasów. Jak wspominałem tutaj, gdy nie ma nawiasów, wskazujemy na samą funkcję, a z nawiasami na to, co funkcja z siebie wywala.\n`NA` – słowo NA ma w R swoje znaczenie. Takich słów normalnie nie możemy używać jako nazw kolumn. Zazwyczaj. Jeśli bardzo chcemy, możemy ująć taką niesyntaktyczną2 nazwę w backticki (patrz tutaj).\n\nTaka funkcja powinna działać. Jeśli uruchomimy ją w konsoli na konkretnym przykładzie, to zadziała.\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    across(\n      c(Sepal.Length, Sepal.Width), # wybrane kolumny\n      .fns = list(\n        N = \\(x) sum(!is.na(x)), # liczba niepustych\n        M = \\(x) mean(x, na.rm = TRUE), # średnia\n        SD = \\(x) sd(x, na.rm = TRUE), # odchylenie standardowe\n        A = agricolae::skewness, # skośność\n        K = agricolae::kurtosis, # kurtoza\n        `NA` = \\(x) sum(is.na(x)) # liczba brakujących\n      )\n    )\n  )\n\n#&gt; # A tibble: 3 × 13\n#&gt;   Species    Sepal.Length_N Sepal.Length_M Sepal.Length_SD Sepal.Length_A\n#&gt;   &lt;fct&gt;               &lt;int&gt;          &lt;dbl&gt;           &lt;dbl&gt;          &lt;dbl&gt;\n#&gt; 1 setosa                 50           5.01           0.352          0.120\n#&gt; 2 versicolor             50           5.94           0.516          0.105\n#&gt; 3 virginica              50           6.59           0.636          0.118\n#&gt; # ℹ 8 more variables: Sepal.Length_K &lt;dbl&gt;, Sepal.Length_NA &lt;int&gt;,\n#&gt; #   Sepal.Width_N &lt;int&gt;, Sepal.Width_M &lt;dbl&gt;, Sepal.Width_SD &lt;dbl&gt;,\n#&gt; #   Sepal.Width_A &lt;dbl&gt;, Sepal.Width_K &lt;dbl&gt;, Sepal.Width_NA &lt;int&gt;\n\n\n\nNie jest to może najładniejsza tabela, ale jest. Spróbujmy jednak to samo wywołać za pomocą naszej funkcji, która przecież niby robi to samo.\n\nopisowe(iris, Species, Sepal.Length, Sepal.Width)\n\n#&gt; Error in `group_by()`:\n#&gt; ! Must group by variables found in `.data`.\n#&gt; ✖ Column `group` is not found.\n\n\n\nUps. Nie działa. Ale czemu? Błąd mówi, że group_by() nie znalazło kolumny o nazwie group. I bardzo słusznie, że nie znalazło, bo nie ma takiej kolumny w iris. Ale w ogóle nie miało jej szukać! Miało szukać kolumny Species, którą podaliśmy jako argument? Dlaczego group_by() szuka kolumny group?"
  },
  {
    "objectID": "posts/metaprogramowanie.html#wywoływanie-funkcji-z-zatrzymanymi-argumentami",
    "href": "posts/metaprogramowanie.html#wywoływanie-funkcji-z-zatrzymanymi-argumentami",
    "title": "Metaprogramowanie w R",
    "section": "11.1 Wywoływanie funkcji z zatrzymanymi argumentami",
    "text": "11.1 Wywoływanie funkcji z zatrzymanymi argumentami\n\nmtscr_model &lt;- function(df, id_column, item_column, score_column) {\n  id_column &lt;- rlang::ensym(id_column)\n  item_column &lt;- rlang::ensym(item_column)\n  score_column &lt;- rlang::ensym(score_column)\n\n  df &lt;- mtscr_prepare( # `mtscr_prepare` wykorzystuje Tidy Eval!\n    df,\n    !!id_column,\n    !!item_column,\n    !!score_column,\n    minimal = TRUE\n  )\n}\n\nUżyłem tu ensym(), ale spokojnie mogłoby być enquo(). Funkcja wywołuje inną funkcję z pakietu, żeby przygotować dane do modelowania. Wszystkie nazwy kolumn na początku „rozbrajam” i ponownie „uzbrajam” już wewnątrz mtscr_prepare(). Jeśli byłby to tylko ten fragment kodu, mógłbym użyć operatora { }, ale potem potrzebowałem wersji ręcznie „rozbrojonej”."
  },
  {
    "objectID": "posts/metaprogramowanie.html#formuły-z-nazw-kolumn",
    "href": "posts/metaprogramowanie.html#formuły-z-nazw-kolumn",
    "title": "Metaprogramowanie w R",
    "section": "11.2 Formuły z nazw kolumn",
    "text": "11.2 Formuły z nazw kolumn\n\nformulas[[i]] &lt;- stats::as.formula(\n  paste0(\n    \".z_score ~ -1 + \",\n    rlang::as_name(item_column),\n    \" + \",\n    rlang::as_name(item_column),\n    \":.ordering_0 + (.ordering_0 | \",\n    rlang::as_name(id_column),\n    \")\"\n  )\n)\n\nTutaj wykorzystuję as_name(), żeby połączyć nazwy kolumn w jedną formułę potrzebną potem do modelu. Równie dobrze mógłbym użyć as_label(). Wyniki zapisuję na liście formulas."
  },
  {
    "objectID": "posts/metaprogramowanie.html#warunkowa-lista-argumentów-do-summarise",
    "href": "posts/metaprogramowanie.html#warunkowa-lista-argumentów-do-summarise",
    "title": "Metaprogramowanie w R",
    "section": "11.3 Warunkowa lista argumentów do summarise()",
    "text": "11.3 Warunkowa lista argumentów do summarise()\n\nargs &lt;- list()\nif (\"all_max\" %in% model_type) {\n  args[[\".all_max\"]] &lt;- rlang::parse_expr(\"max(.all_max)\")\n}\n\nif (\"all_top2\" %in% model_type) {\n  args[[\".all_top2\"]] &lt;- rlang::parse_expr(\"max(.all_top2)\")\n}\n\ndf &lt;- df |&gt;\n  dplyr::summarise(\n    !!!args,\n    .by = dplyr::all_of(groups)\n  )\n\nZ tego jestem zadowolony. To fragment funkcji, która daje różne instrukcje do summarise() w zależności od tego, czy w argumencie model_type jest \"all_max\", \"all_top2\" czy c(\"all_max\", \"all_top2\"). Kolejne instrukcje zapisuję na liście args, którą potem otwieram operatorem !!! (bo to lista). Używam tutaj też parse_expr(), ale to z przyczyn technicznych związanych z pakietami7 raczej, niż z programowaniem. Mógłbym użyć też expr(max(.all_max)). Na pewno nie używam tutaj enexpr(), bo to nie są argumenty, tylko nazwy kolumn zapisane na sztywno."
  },
  {
    "objectID": "posts/automatyzacja.html",
    "href": "posts/automatyzacja.html",
    "title": "Listy, pętle i automatyzacja w R",
    "section": "",
    "text": "Nie znoszę mechanicznej pracy. Jestem jedną z tych osób, które wolą spędzić 30 minut na automatyzacji czegoś, co ręcznie da się zrobić w 15 minut. Może jest to strzelanie do muchy z armaty, ale ma swoje zalety. Po pierwsze, poświęciłem na to tyle czasu, że teraz jestem w stanie wiele rzeczy zautomatyzować dość szybko. Tym doświadczeniem chcę się też podzielić. Po drugie, jeśli praca manualna wielokrotnie się powtarza, brutto oszczędzam czas, mimo że na początku muszę zainwestować go więcej. Raz zautomatyzowana czynność już zautomatyzowana całkowicie, ile razy byśmy jej nie wykorzystali. Po trzecie, jeśli automatyzujemy, to tyle samo czasu zajmuje wyczyszczenie 1 pliku i 100 plików. Jeśli czyścimy ręcznie, to 100 plików przekłada się na 100 razy więcej (zmarnowanego) czasu. Po czwarte – mam ciekawsze rzeczy do roboty niż wypisywanie kolejnych liczb. Automatyzacja chociaż mnie nie nudzi, zawsze uczę się czegoś nowego (jak prezydent) i mogę w ten sposób wykorzystać swoje zasoby po prostu lepiej.\nZakładam tutaj, że osoba czytająca zna R na chociaż podstawowym poziomie. Jeśli nie, polecam swoje wprowadzenie do R."
  },
  {
    "objectID": "posts/automatyzacja.html#masowe-ładowanie-danych-z-list.files",
    "href": "posts/automatyzacja.html#masowe-ładowanie-danych-z-list.files",
    "title": "Listy, pętle i automatyzacja w R",
    "section": "2.1 Masowe ładowanie danych z list.files",
    "text": "2.1 Masowe ładowanie danych z list.files\nZ tego, co napisałem, wynika, że możemy łatwo wyczyścić nasze dane, jeśli zrobimy z nich listę. Ale jak dopiero otwieramy RStudio, to nie mamy danych jeszcze załadowanych! Nawet jakbyśmy chcieli, zrobić listę, to nie mamy czego wrzucić do komendy list(). Listy pozwolą nam jednak nie tylko masowo dane wyczyścić, ale też masowo je załadować.\nZałóżmy, że nasze 35 plików z danymi znajduje się w folderze dane, podfolder automatyzacja. Nie jest to, oczywiście, obowiązek, Twoja struktura może się różnić, ale dobrze jest mieć dane w oddzielnym folderze. Możemy teraz użyć komendy list.files(), żeby stworzyć listę naszych plików4. To samo robi komenda dir(). Obie przyjmują ścieżkę do folderu, w którym są pliki do wrzucenia na listę. Ustawimy jeszcze full.names = TRUE, bo nie chcemy dostać samych nazw plików, ale całe ścieżki.\n\nbazy_lista &lt;- list.files(\"./dane/automatyzacja\", full.names = TRUE)\n\nJeśli wyświetlimy teraz obiekt bazy_lista, zobaczymy listę ścieżek wszystkich naszych plików z danymi. Nie są to załadowane bazy, tylko lista plików.\n\nbazy_lista\n\n#&gt;  [1] \"./dane/automatyzacja/AC_eksperyment_trudny_2021_Apr_22_2124.csv\"     \n#&gt;  [2] \"./dane/automatyzacja/AG_eksperyment_latwy_2021_Apr_25_0233.csv\"      \n#&gt;  [3] \"./dane/automatyzacja/AT_eksperyment_latwy_2021_Apr_25_1402.csv\"      \n#&gt;  [4] \"./dane/automatyzacja/BD_eksperyment_latwy_2021_Apr_24_1631.csv\"      \n#&gt;  [5] \"./dane/automatyzacja/BM_eksperyment_trudny_2021_Apr_25_2042.csv\"     \n#&gt;  [6] \"./dane/automatyzacja/DF_eksperyment_latwy_2021_Apr_22_1456.csv\"      \n#&gt;  [7] \"./dane/automatyzacja/DJ_eksperyment_trudny_2021_Apr_25_2048.csv\"     \n#&gt;  [8] \"./dane/automatyzacja/DJ_eksperyment_trudny_2021_Apr_25_2102.csv\"     \n#&gt;  [9] \"./dane/automatyzacja/FB_eksperyment_latwy_2021_Apr_24_1649.csv\"      \n#&gt; [10] \"./dane/automatyzacja/GM_eksperyment_latwy_2021_Apr_24_1434.csv\"      \n#&gt; [11] \"./dane/automatyzacja/HJ_eksperyment_trudny_2021_kwi_21_1805.csv\"     \n#&gt; [12] \"./dane/automatyzacja/JJ_eksperyment_trudny_2021_Apr_24_1112.csv\"     \n#&gt; [13] \"./dane/automatyzacja/JK_eksperyment_latwy_2021_Apr_25_1422.csv\"      \n#&gt; [14] \"./dane/automatyzacja/JK_eksperyment_trudny_2021_Apr_23_2129.csv\"     \n#&gt; [15] \"./dane/automatyzacja/JK_eksperyment_trudny_2021_kwi_21_1326.csv\"     \n#&gt; [16] \"./dane/automatyzacja/julka_eksperyment_latwy_2021_Apr_25_2150.csv\"   \n#&gt; [17] \"./dane/automatyzacja/JW_eksperyment_latwy_2021_Apr_24_1657.csv\"      \n#&gt; [18] \"./dane/automatyzacja/kacperek_eksperyment_latwy_2021_Apr_25_2158.csv\"\n#&gt; [19] \"./dane/automatyzacja/Kasia_eksperyment_latwy_2021_Apr_25_1421.csv\"   \n#&gt; [20] \"./dane/automatyzacja/Kinga_eksperyment_latwy_2021_Apr_25_1419.csv\"   \n#&gt; [21] \"./dane/automatyzacja/kk_eksperyment_trudny_2021_Apr_22_2104.csv\"     \n#&gt; [22] \"./dane/automatyzacja/KK_eksperyment_trudny_2021_Apr_23_2136.csv\"     \n#&gt; [23] \"./dane/automatyzacja/KP_eksperyment_trudny_2021_kwi_21_2037.csv\"     \n#&gt; [24] \"./dane/automatyzacja/KW_eksperyment_trudny_2021_Apr_22_2254.csv\"     \n#&gt; [25] \"./dane/automatyzacja/maciek_eksperyment_latwy_2021_Apr_22_1138.csv\"  \n#&gt; [26] \"./dane/automatyzacja/Magda_eksperyment_latwy_2021_Apr_25_1932.csv\"   \n#&gt; [27] \"./dane/automatyzacja/Michal_eksperyment_latwy_2021_Apr_25_1405.csv\"  \n#&gt; [28] \"./dane/automatyzacja/MJ_eksperyment_trudny_2021_Apr_25_2054.csv\"     \n#&gt; [29] \"./dane/automatyzacja/MK_eksperyment_latwy_2021_Apr_24_1706.csv\"      \n#&gt; [30] \"./dane/automatyzacja/MK_eksperyment_trudny_2021_Apr_22_2109.csv\"     \n#&gt; [31] \"./dane/automatyzacja/OK_eksperyment_trudny_2021_Apr_24_1343.csv\"     \n#&gt; [32] \"./dane/automatyzacja/SG_eksperyment_trudny_2021_Apr_24_1546.csv\"     \n#&gt; [33] \"./dane/automatyzacja/SJ_eksperyment_trudny_2021_kwi_21_1621.csv\"     \n#&gt; [34] \"./dane/automatyzacja/SP_eksperyment_trudny_2021_kwi_21_2030.csv\"     \n#&gt; [35] \"./dane/automatyzacja/ZD_eksperyment_latwy_2021_Apr_26_1036.csv\"\n\n\n\nNasze dane możemy załadować np. komendą read_csv() z pakietu readr. Ta funkcja może przyjąć całą serię różnych argumentów, ale najważniejszym jest… ścieżka do pliku do załadowania. Dokładnie to, co mamy zgromadzone w obiekcie bazy_lista! Każdą z tych ścieżek moglibyśmy teraz wrzucić do funkcji read_csv() i dostalibyśmy nie ścieżkę, ale załadowaną bazę. Chcemy więc teraz powiedzieć R „po kolei weź każdą ścieżkę z bazy_lista i wrzuć ją do read_csv(). Do tego służą funkcje z rodziny apply, czyli apply(), sapply(), lapply() lub tapply(). Jak w przypadku wielu funkcji, rodzina apply ma też swoje odpowiedniki w tidyverse. Jak raz „oryginały” są używane częściej5, ale i tak my skorzystamy z funkcji map() z pakietu purrr, bo jest bardziej intuicyjna.\n\nbazy_lista &lt;- map(bazy_lista, read_csv, show_col_types = FALSE, name_repair = \"unique_quiet\")\n\nmap przyjmuje, w podstawowej wersji, dwa argumenty. Pierwszym jest obiekt, do którego chcemy zastosować naszą funkcję, a drugim sama funkcja. My chcemy zastosować funkcję read_csv() na każdym elemencie wektora bazy_lista. Podobnie jak w across (patrz tutaj), gdy podajemy, jaką funkcję chcemy zastosować, to musimy zapisać obiekt zawierający funkcję, a nie o efekt działania funkcji, dlatego po nazwie funkcji nie dajemy nawiasów. To bardzo częsty błąd. Na koniec możemy dorzucić parę innych argumentów, takich jak show_col_types = FALSE i name_repair = \"unique_quiet\", żeby nasza konsola nie została zalana milionem informacji o aktualnie wczytywanej bazie6.\nTeraz R weźmie każdą ścieżkę i wrzuci ją do funkcji read_csv(). Każda ścieżka zostanie więc załadowana, a wynikowe bazy wrzucone na listę. Jeśli więc teraz byśmy spojrzeli w obiekt bazy_lista, to nie zobaczymy tam już ścieżek, ale prawdziwe ramki danych."
  },
  {
    "objectID": "posts/automatyzacja.html#masowe-czyszczenie-z-map-lub-lapply",
    "href": "posts/automatyzacja.html#masowe-czyszczenie-z-map-lub-lapply",
    "title": "Listy, pętle i automatyzacja w R",
    "section": "2.2 Masowe czyszczenie z map() lub lapply()",
    "text": "2.2 Masowe czyszczenie z map() lub lapply()\nSkoro poznaliśmy już funkcję map(), wyczyszczenie naszych baz nie powinno stanowić problemu. W końcu są one na liście, a my właśnie nauczyliśmy się, zastosować jakąś funkcję do każdego elementu listy z osobna. Problem polega jednak na tym, że nie mamy pojedynczej funkcji czyszczącej, a cały wielki zestaw tych funkcji. Jednak jak dowiedzieliśmy się tutaj, możemy nasz zestaw po prostu przerobić na pojedynczą funkcję.\n\nwyczysc &lt;- function(df) {\n    metryczka &lt;- df %&gt;%\n        slice(1:2) %&gt;% # wybierz wiersze z samej metryczki\n        select(participant, expName, form.itemText, form.response) %&gt;% # wybierz kolumny z id, warunkiem, pytaniem i odpowiedzią\n        pivot_wider(names_from = form.itemText, values_from = form.response) %&gt;% # format długi\n        set_names(\"id\", \"warunek\", \"plec\", \"wiek\") %&gt;%\n        mutate(\n            wiek = parse_number(wiek),\n            warunek = str_remove(warunek, \"eksperyment_\") # zostaw samo \"łatwy\" albo \"trudny\"\n        )\n\n    pytania &lt;- df %&gt;%\n        slice(4:11) %&gt;%\n        select(form_2.itemText, form_2.response) %&gt;%\n        mutate(\n            form_2.response = case_match( # tak i nie na 1 i 0\n            form_2.response,\n            \"tak\" ~ 1,\n            \"nie\" ~ 0\n            )\n        ) %&gt;%\n        pivot_wider(names_from = form_2.itemText, values_from = form_2.response) %&gt;%\n        set_names(paste0(\"pyt_\", 1:8))\n\n    bind_cols(metryczka, pytania) %&gt;%\n        mutate(\n            id = stringi::stri_rand_strings(1, 5) # zamień id na losowe znaki\n        )\n}\n\nUzyskałem w ten sposób funkcję wyczysc(), która wykonuje wszystkie nasze przekształcenia. Warto zwrócić uwagę, że bazę danych nazwałem df i zrobiłem z niej argument naszej funkcji, a także, że ostatni blok nie ma przypisania do zmiennej baza. Wynika to z tego, że domyślnie funkcje w R zwracają ostatnią rzecz, którą zrobią. W tym wypadku wynikiem działania funkcji wyczysc() będzie efekt działania bind_cols(). Jeśli zapisałbym ostateczną bazę do zmiennej baza, musiałbym w ostatniej linijce dopisać samo baza albo return(baza), ponieważ to jest to, co ostatecznie chcemy dostać – wyczyszczoną bazę.\nPrzy tej okazji ostrzegam, że używanie nazw kolumn jako argumentów takich własnych funkcji może spowodować niespodziewane problemy. Jeśli nasza funkcja dostanie argument nazwa_kolumny, to funkcje typu select() będą w bazie szukać kolumny, która nazywa się nazwa_kolumny, a nie wartości tego argumentu (np. jeśli ustawimy nazwa_kolumny = pyt_1, to select() czy mutate() nie będą szukały kolumny pyt_1, tylko kolumny o nazwie nazwa_kolumny). Więcej o tym piszę tutaj, ale ad hoc można sobie z tym poradzić pisząc nazwę argumentu wewnątrz funkcji typu select() w podwójnych nawiasach klamrowych, np. { nazwa_kolumny }.\nUżyjmy teraz naszej nowej funkcji wyczysc do wyczyszczenia naszych plików.\n\nbazy &lt;- map(bazy_lista, wyczysc)\n\nEfektem działania tej funkcji jest lista wyczyszczonych już baz. Każda taka baza ma tylko jeden wiersz (bo tak zrobiliśmy nasze czyszczenie). Jednak do analizy statystycznej nie jest potrzebna lista, tylko jedna całościowa baza. Połączmy więc wszystkie bazy na liście w jedną bazę za pomocą funkcji bind_rows. Możemy ją wywołać na wyczyszczonej zmiennej bazy albo już wcześniej dodać ją potokiem do mapowania. Wygląda to tak:\n\n# tak jest dobrze\nbazy &lt;- map(bazy_lista, wyczysc)\nbazy &lt;- bind_rows(bazy)\n\n# tak też jest dobrze\nbazy &lt;- map(bazy_lista, wyczysc) %&gt;%\n    bind_rows()\n\nbazy\n\n#&gt; # A tibble: 35 × 12\n#&gt;    id    warunek plec       wiek pyt_1 pyt_2 pyt_3 pyt_4 pyt_5 pyt_6 pyt_7 pyt_8\n#&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n#&gt;  1 nN3zb trudny  kobieta      20     0     1     1     0     1     0     0     1\n#&gt;  2 9X7x2 latwy   kobieta      21     1     1     1     1     1     0     0     1\n#&gt;  3 Awyis latwy   mezczyzna    24     1     1     0     1     1     1     0     0\n#&gt;  4 y2Rdz latwy   kobieta      17     1     0     1     1     1     0     0     1\n#&gt;  5 JrXsV trudny  mezczyzna    17     0     0     1     1     1     1     1     1\n#&gt;  6 rrL0C latwy   mezczyzna    23     1     0     1     1     1     1     0     1\n#&gt;  7 wHsx0 trudny  kobieta      17     1     0     1     1     1     0     0     1\n#&gt;  8 YDejs trudny  kobieta      17     0     1     1     1     1     0     0     0\n#&gt;  9 Ntqj2 latwy   mezczyzna    20     0     0     1     1     1     1     0     0\n#&gt; 10 KkuCT latwy   kobieta      20     1     1     0     1     0     1     1     0\n#&gt; # ℹ 25 more rows\n\n\n\nOstatecznie uzyskujemy piękną, czystą i pojedynczą bazę danych, na której możemy wykonywać analizy."
  },
  {
    "objectID": "posts/automatyzacja.html#pętle-for",
    "href": "posts/automatyzacja.html#pętle-for",
    "title": "Listy, pętle i automatyzacja w R",
    "section": "3.1 Pętle for",
    "text": "3.1 Pętle for\nPętla for służy do wykonania danej czynności określoną liczbę razy albo dla określonych rzeczy. Tutaj przykład wykorzystania tej pętli do wykonania czynności, którą robiliśmy wcześniej, czyli ładowania baz danych na podstawie listy plików.\n\nbazy_lista &lt;- list.files(\"./dane/automatyzacja\", full.names = TRUE)\nbazy &lt;- list() # pusta lista na przyszłość\n\nfor (i in 1:35) {\n    bazy[[i]] &lt;- read_csv(bazy_lista[[i]], show_col_types = FALSE, name_repair = \"unique_quiet\")\n}\n\nZapis ten jest bardziej programistyczny i korzysta z klasycznego R. Najpierw tworzymy pustą listę bazy. Następnie pętla for wykona to, co zapisaliśmy w jej klamrach, za każdym razem zamieniając i na kolejną liczbę. Czyli najpierw wczyta bazę ze ścieżki bazy_lista[[1]], potem bazy_lista[[2]] i tak dalej aż do bazy_lista[[35]]. Wczytane bazy zapisze do listy bazy na stosownym miejscu. 1:35 moglibyśmy zamienić na 1:length(bazy_lista), żeby nie zapisywać na sztywno 35, jakby miała się ta liczba zmienić.\nMoglibyśmy też użyć nieco innej składni.\n\nbazy_lista &lt;- list.files(\"./dane/automatyzacja\", full.names = TRUE)\nbazy &lt;- list() # pusta lista na przyszłość\n\nfor (i in bazy_lista) {\n    bazy &lt;- c(\n        bazy,\n        list(read_csv(i, show_col_types = FALSE, name_repair = \"unique_quiet\"))\n    )\n}\n\nNie jest to najlepszy, najbardziej wydajny kod, ale nie chcę wchodzić w bardziej zaawansowane koncepcje jak rezerwowanie miejsca w pamięci przed uruchomieniem pętli. Ta składnia działa nieco inaczej, bo i nie jest tutaj liczbą, tylko kolejnymi ścieżkami do naszych plików i to one lądują w read_csv(). Tak wczytaną bazę dołączam do listy baz za pomocą c. Co ważne, nasza nowa baza też musi mieć format listy, dlatego całe read_csv opakowałem w list.\nJak widać, takie rozwiązanie wymaga podejścia do R od bardziej programistycznej strony. Tak jak jednak wspominałem, w rzeczywistości rzadko jest to potrzebne, bo możemy używać funkcji lapply() lub map()."
  },
  {
    "objectID": "posts/automatyzacja.html#pętle-while",
    "href": "posts/automatyzacja.html#pętle-while",
    "title": "Listy, pętle i automatyzacja w R",
    "section": "3.2 Pętle while",
    "text": "3.2 Pętle while\nPętle while wykonują jakieś polecenie dopóty, dopóki spełniony jest warunek, który jej podamy. Musimy być jednak ostrożni, bo jeśli ten warunek nie zostanie osiągnięty nigdy (bo np. źle go zaplanowaliśmy), to pętla będzie działała w nieskończoność, nierzadko zapychając pamięć komputera. Pętla while, choć wykorzystywana nawet rzadziej niż pętla for, przydała mi się w celach testowo-dydaktycznych, bo wykorzystywałem ją do znalezienia zbioru losowych liczb o określonych parametrach statystycznych. Żeby podać przykład:\n\nliczby &lt;- rnorm(100, mean = 101, sd = 15)\n\nwhile (mean(liczby) != 105) {\n    liczby &lt;- rnorm(100, mean = 101, sd = 15) %&gt;%\n        round()\n}\n\nTen kod wykorzystałem do znalezienia zestawu 100 losowych liczb z rozkładu normalnego IQ (\\(M = 100\\), \\(SD = 15\\)), których średnia będzie wynosiła dokładnie 101. Wykorzystałem to w tekście o wartości \\(p\\) do znalezienia ładnej próbki, której średnia nie będzie wynosiła dokładnie tyle, ile średnia z populacji. Funkcja rnorm losuje liczby z rozkładu normalnego o podanych parametrach 5, round zaokrągla je do całości. Pętla while mówi tutaj „Sprawdź, czy średnia wylosowanych liczb nie równa się 105. Jeśli nie, to wylosuj ponownie.” albo inaczej „Powtarzaj losowanie dopóki średnia liczb nie będzie się równała 101”."
  },
  {
    "objectID": "posts/git.html",
    "href": "posts/git.html",
    "title": "Jak wspólnie pisać analizę statystyczną?",
    "section": "",
    "text": "Kiedyś (a niekiedy do dziś) wspólne pisanie dokumentów czy prezentacji polegało na wysyłaniu sobie serii załączników mailem. Gdy zespół liczył więcej niż 2 osoby, kolejne wersje danego pliku szybko stawały się trudne do śledzenia. Podobnie gdy pisaliśmy coś na więcej niż jednym komputerze. Mam kolegę, który tak wysłał nie tę wersję pracy zaliczeniowej do prowadzącej. Nie znalazł zrozumienia. Te problemy w większości odeszły w niepamięć, gdy nastała era chmury i współpracy online. Znacznie wcześniej powstawały jednak inne rozwiązania – systemy kontroli wersji, spośród których praktyczny monopol zdobyło otwartoźródłowe dzieło Linusa Torvaldsa (twórcy Linuksa) o nazwie Git."
  },
  {
    "objectID": "posts/git.html#git",
    "href": "posts/git.html#git",
    "title": "Jak wspólnie pisać analizę statystyczną?",
    "section": "2.1 Git",
    "text": "2.1 Git\nŻeby móc korzystać z dobrodziejstw Gita, musimy go sobie najpierw zainstalować. Chyba że korzystamy z którejś dystrybucji Linuksa, wtedy mamy Git od początku. Nic dziwnego, Git pierwotnie powstał po to, żeby cała społeczność mogła dołożyć cegiełkę do jądra Linuksa. Jeśli korzystamy z Windowsa, możemy pobrać podstawowy pakiet stąd.\nGdy Git zostanie zainstalowany, musimy dokonać podstawowej konfiguracji, do której my wykorzystamy GitHub CLI i zrobimy ją w kolejnym podrozdziale. Jeśli jednak nie chcemy korzystać z jakiegoś powodu z GitHub CLI, możemy się skonfigurować lokalnie. W tym celu otwieramy konsolę (PowerShell lub wiersz polecenia w Windowsie) i wpisujemy tam dwie komendy, jedną po drugiej. Nie polecam tego robić, jeśli nie ma się powodu. Pierwsza komenda przypisze do lokalnego Gita nasz email:\ngit config --global user.email \"adres@email.com\"\nOczywiście za adres@email.com podstawiamy nasz własny adres. Powinien to być ten sam adres, którego potem użyjemy do założenia konta na GitHubie. Komendę zatwierdzamy enterem. Jeśli konsola nie znajduje polecenia git, trzeba sprawdzić, czy na pewno zainstalowaliśmy Git oraz zrestartować komputer. Druga komenda ustawia nazwę użytkownika.\ngit config --global user.name \"nazwa-uzytkownika\"\nPonownie, powinna być to taka sama nazwa użytkownika, jaką będziemy mieć na GitHubie. W razie czego zawsze można ją zmienić tą samą komendą."
  },
  {
    "objectID": "posts/git.html#github-i-github-cli",
    "href": "posts/git.html#github-i-github-cli",
    "title": "Jak wspólnie pisać analizę statystyczną?",
    "section": "2.2 GitHub i GitHub CLI",
    "text": "2.2 GitHub i GitHub CLI\nGit to protokół działający lokalnie. Innymi słowy nasz własny Git jest w naszym własnym folderze i inni ludzie nie mają do niego dostępu. To jest OK, jeśli chcemy robić kontrolę wersji do własnego użytku. Jeśli jednak chcemy współpracować z innymi albo mieć dostęp do naszego kodu na różnych komputerach, potrzebujemy jakiegoś rodzaju usługi internetowej, z którą każdy może się połączyć, żeby mieć stały dostęp do aktualnej wersji. Podstawowym de facto hostingiem tego typu jest GitHub. To jest więc dobry moment, żeby wejść na GitHuba i założyć tam konto.\nSzybkie wtrącenie, które może nam ułatwić życie w późniejszej konfiguracji RStudio. Warto jest w tym momencie wejść w ustawienia RStudio, tam w zakładkę Git/SVN i sprawdzić, czy RStudio podaje jakąś ścieżkę dla SSH key. Jeśli nie, klikamy „Create SSH key” i zatwierdzamy. Nie musimy wpisywać hasła.\nGitHub dysponuje też wersją konsolową, GitHub CLI, która pozwoli nam bezboleśnie połączyć naszego lokalnego Gita z kontem na GitHubie. GitHub CLI pobieramy stąd. Po zainstalowaniu (i ewentualnym restarcie komputera) możemy dokonać konfiguracji. W tym celu otwieramy konsolę (PowerShell lub wiersz polecenia w Windowsie) i wpisujemy tam:\ngh auth login\nPo zatwierdzeniu komendy enterem powinien uruchomić się kreator, który bezboleśnie przeprowadzi nas przez proces logowania. Najpierw wybieramy, że chcemy się się logować do zwykłego GitHub.com. Przesuwamy się strzałkami, zatwierdzamy enterem. Następnie musimy wybrać protokół. Na ogół łatwiejszy do ogarnięcia jest HTTPS, jednak na potrzeby RStudio lepiej jest wybrać SSH. Jeśli wytworzyliśmy wcześniej klucz SSH w RStudio, GitHub CLI zapyta nas teraz, czy wysłać ów klucz na serwer. Tak, to jest dokładnie to, co chcemy zrobić. Następnie klucz tytułujemy jak chcemy (lub klikamy enter, jeśli odpowiada nam tytuł GitHub CLI). Dalej wybieramy logowanie za pomocą przeglądarki. W tym momencie wyświetli nam się ośmioznakowy kod, który powinniśmy skopiować. Jeśli chcesz użyć skrótu klawiaturowego, nie używaj Ctrl+C, bo wyłączysz GitHub CLI. Użyj Ctrl+Shift+C. Możesz też użyć myszki. Po kliknięciu enter otworzy nam się okno przeglądarki, gdzie wklejamy wcześniej skopiowany kod i zatwierdzamy.\nW ten sposób dokonaliśmy konfiguracji GitHub CLI. Pozostaje nam jeszcze połączyć GitHub CLI z naszym lokalnym Gitem za pomocą prostej komendy:\ngh auth git-setup"
  },
  {
    "objectID": "posts/git.html#github-desktop-i-gitkraken",
    "href": "posts/git.html#github-desktop-i-gitkraken",
    "title": "Jak wspólnie pisać analizę statystyczną?",
    "section": "2.3 GitHub Desktop i GitKraken",
    "text": "2.3 GitHub Desktop i GitKraken\nDomyślnie Git jest narzędziem konsolowym. Dla większości świeżych użytkowników jest to nie do przejścia. Sam widok okna terminala może wywoływać obfite pocenie, przyspieszenie akcji serca i wyrzut kortyzolu do krwi. Na szczęście istnieją przynajmniej dwa narzędzia, które mogą nam oszczędzić męki uczenia się poleceń konsolowych.\nPierwsze z nich to GitHub Desktop, który jest intuicyjnym programem pozwalającym nam na szybkie zarządzanie Gitem i synchronizację z GitHubem. Jest to narzędzie darmowe i otwartoźródłowe. Pozwala nam nie tylko zatwierdzać, wysyłać i odbierać zmiany, ale także możemy za jego pomocą zarządzać pull requests (więcej o tym później).\n\n\n\nGitHub Desktop\n\n\nDruga możliwość to GitKraken. Potężne narzędzie do zarządzania Gitem z wieloma zaawansowanymi możliwościami jak wyświetlanie wykresów commitów i modyfikacje na miejscu. Niestety GitKraken to w wielu funkcjach program płatny, ale dostępny za darmo dla studentów i pracowników naukowych w ramach GitHub Education. Nie jest to jedyna korzyść płynąca z tego programu (inne to np. GitHub Copilot), także polecam się zainteresować. Wiele funkcji jest też dostępnych, jeśli nasze repozytorium jest publiczne.\n\n\n\nGitKraken\n\n\nMożemy wybrać dowolne z tych narzędzi, bo funkcjonalność mają podobną. Jeśli jednak mamy statuts studenta albo pracownika naukowego, polecam GitKraken o tyle, że pozwala dość łatwo rozwiązać specyficzne problemy. Tak czy inaczej, będą to dla nas raczej dodatkowe narzędzia, jeśli planujemy korzystać z Gita głównie do wspólnego pisania kodu R w RStudio, ponieważ RStudio ma już wbudowaną podstawową integrację z Gitem."
  },
  {
    "objectID": "posts/git.html#gitignore",
    "href": "posts/git.html#gitignore",
    "title": "Jak wspólnie pisać analizę statystyczną?",
    "section": "4.1 .gitignore",
    "text": "4.1 .gitignore\nPlik .gitignore to informacja dla Git, żeby pewne konkretne pliki albo rodzaje plików (np. wszystkie pliki .csv) ignorował. Innymi słowy, żeby nie krzyczał nam, że wykrywa taki plik i trzeba go śledzić. Podstawowe rodzaje plików, które w projektach R powinny zostać zignorowane, dodaje wspomniana już komenda usethis::git_vaccinate(). Jeśli tworzyliśmy repozytorium za pomocą RStudio, stosowny plik .gitignore powinien się już znajdować w repozytorium. Programy do zarządzania Gitem również potrafią nam przygotować taki roboczy plik podczas tworzenia repozytorium.\nUwaga! Domyślnie pliki i foldery z nazwami zaczynającymi się kropką są ukryte. Możemy odkrywać ukryte pliki skrótem klawiszowym Ctrl+H albo za pomocą menu Widok w Windows Explorer.\nPlik .gitignore możemy otworzyć w dowolnym edytorze tekstu. Możemy też w RStudio użyć komendy usethis::edit_git_ignore(), która nam ten plik otworzy. Możemy dopisać tam nazwy lub ścieżki plików, które Git ma ignorować. Możemy też dopisać *.csv, żeby zignorować wszystkie pliki .csv. Gwiazdka * oznacza dowolne znaki. Możemy również dodawać pojedyncze pliki w R za pomocą komendy usethis::use_git_ignore()."
  },
  {
    "objectID": "posts/git.html#readme.md",
    "href": "posts/git.html#readme.md",
    "title": "Jak wspólnie pisać analizę statystyczną?",
    "section": "4.2 README.md",
    "text": "4.2 README.md\nPlik README to jest to, co wyświetla się, jak na GitHubie wejdziemy na stronę repozytorium. Możemy tam zapisywać podstawowe informacje dla odbiorców naszego repozytorium. Sam plik korzysta z formatowania GitHub Markdown, którego opis znajdziemy tutaj. Jeśli piszemy analizę w R, warto użyć komendy usethis::use_readme_md(), żeby taki wzorcowy przykładowy plik wytworzyć.\nDla kodu w R warto rozważyć, czy nie przerzucić się na plik README.Rmd wytwarzany przez usethis::use_readme_rmd(). Pozwala on do pliku README dorzucać np. bloki kodu R. Dla publicznego kodu to szczególnie cenne, bo możemy pokazać przykładowe działanie naszych funkcji. Wtedy edytujemy plik README.Rmd zamiast Readme.md, zaś przed samym wypchnięciem używamy funkcji devtools::build_readme(). Generuje to aktualną wersję z rozszerzeniem .md, którą GitHub potrafi odczytać. Więcej o składni w plikach R Markdown przeczytamy tutaj. Obecnie standard R Markdown jest systematycznie zastępowany przez Quarto, ale w chwili, gdy to piszę, usethis nie planuje go wdrażać. O możliwości korzystania z README.Rmd więcej napiszę we wpisie o pakietach. Kiedy już powstanie."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nieobliczalne",
    "section": "",
    "text": "Sortuj według\n       Domyślnie\n         \n          Tytuł\n        \n         \n          Data - Od najstarszych\n        \n         \n          Data - Od najnowszych\n        \n         \n          Autor\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nRozkłady próbkowania i rozkłady \\(t\\)\n\n\nNajwiększe nieporozumienie w statystyce\n\n\n\nStatystyka\n\n\n\nW tym wpisie wprowadzam bardzo ważne rozróżnienie, które uwielbia się mylić wszystkim – od studentów po profesorów (tych mniej siedzących w statystyce). Czym innym jest rozkład zmiennej, a czym innym jest rozkład próbkowania. Co więcej, rozkłady próbkowania leżą u podstaw wnioskowania statystycznego, to koncept fundamentalny, więc warto je docenić. Są one podstawą wartości \\(p\\). To one (a nie rozkłady zmiennych!) powinny być normalne do analiz parametrycznych. Omawiam tutaj też błąd standardowy i Centralne Twierdzenie Graniczne.\n\n\n\n\n\n29 października 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nGęstość prawdopodobieństwa i rozkład normalny\n\n\n\n\n\n\nStatystyka\n\n\nMatematyka\n\n\n\nIstnieje szczególny rodzaj wykresów zwany wykresami gęstości. A nie jest to pierwszy lepszy wykres – wykresy gęstości prawdopodobieństwa to fundament całej statystyki. I bardzo łatwo jest czytać je źle. Dlatego w tym wpisie omawiam, jak czytać takie wykresy i jak je liczyć. Jest to łagodne wprowadzenie do rozkładu normalnego i jego dystrybuanty.\n\n\n\n\n\n27 sierpnia 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nFunkcje matematyczne\n\n\n\n\n\n\nStatystyka\n\n\nMatematyka\n\n\n\nFunkcja to absolutnie fatalna nazwa na dość prosty w swojej idei twór. W tym wpisie postaram się wyjaśnić po ludzku czym są funkcje w matematyce i jak z nich korzystać. Najważniejsze, co chcę jednak zrobić, to przygotować grunt pod zrozumienie funkcji liniowych w analizie regresji.\n\n\n\n\n\n27 lipca 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nTworzenie pakietów w R\n\n\n\n\n\n\nR\n\n\n\nPo co pisać własne pakiety w R? Widzę dwa powody. Po pierwsze możemy mieć pewną kolekcję funkcji, których często używamy w różnych analizach. Możemy je w pewnym momencie chcieć zebrać w jedno miejsce. Po drugie możemy chcieć się podzielić naszymi funkcjami z innymi, czy to z zespołem, czy ze światem. W obu przypadkach pakiet jest lepszą, bardziej uporządkowaną i łatwiejszą w używaniu opcją niż pliki ze skryptami.\n\n\n\n\n\n26 maja 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nZnajdź i zmień to narzędzie ostateczne\n\n\nOszczędzanie czasu z wyrażeniami regularnymi\n\n\n\nNarzędzia\n\n\n\nJest takie narzędzie, które nie ma szerokiej publiczności poza światem technicznym, a pozwala oszczędzić naprawdę dużo czasu. Całkowicie poważnie twierdzę że powinno ono być nauczane w szkole, razem z obsługą programów do pisania. Sądzę, że przyda się ono każdemu, kto cokolwiek pisze na komputerze, nieważne, czy jest to profesor, księgowa czy zwykły Kowalski. Mowa tutaj o wyrażeniach regularnych (Regular Expressions, RegEx), które przybliżam w tym tekście.\n\n\n\n\n\n20 maja 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nJak wspólnie pisać analizę statystyczną?\n\n\nGit, GitHub i integracja z RStudio\n\n\n\nNarzędzia\n\n\nR\n\n\n\nPrzestawienie się z programu typu SPSS na R daje nam wiele korzyści, w tym umożliwia łatwe wspólne pisanie analiz statystycznych. W końcu kod to tekst. Skoro możemy w Google Docs wspólnie tworzyć dokumenty, to czemu nie mielibyśmy wspólnie pisać analizy statystycznej? Do współpracy przy kodzie nie wykorzystujemy jednak nie Google Docs, ale specjalny protokół znany jako Git.\n\n\n\n\n\n15 maja 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nNie pisz (prawie) tego samego 100 razy\n\n\nAutomatyzacja powtarzalnego tekstu za pomocą Pythona\n\n\n\nPython\n\n\nNauka\n\n\nNarzędzia\n\n\n\nWspominałem już, że nie znoszę mechanicznej pracy. A królową wśród prac mechanicznych jest pisanie wiele razy prawie tego samego tekstu, który różni się tylko w szczegółach. Zdarza się to non-stop. Jednocześnie jest to ten rodzaj pracy, który można niskim kosztem zautomatyzować. W tym wpisie postaram się pokazać, jak można uniknąć góry bezsensownej pracy za pomocą prostych pętli i funkcji print() w Pythonie.\n\n\n\n\n\n12 maja 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nMetaprogramowanie w R\n\n\nDlaczego moja funkcja nie działa?\n\n\nPiękno pakietu tidyverse polega na tym, że zupełnie zmienia sposób w jaki wpisujemy nazwy kolumn do funkcji – bez głębszego zastanawiania się. To zupełnie inaczej, niż w większości języków programowania. To tzw. Tidy Evaluation (Tidy Eval) ułatwia pisanie zwykłego kodu, ale zaskakująco utrudnia pisanie własnych funkcji.\n\n\n\n\n\n5 maja 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nLepiej niż ChatGPT\n\n\nAI do pomocy w researchu naukowym\n\n\n\nNauka\n\n\n\nW świecie Diuny komputery były zakazane (po wielkim buncie maszyn). Ich zadania przejęli mentaci – ludzie specjalnie wyszkoleni do tego, żeby pamiętać wielkie ilości informacji, łączyć je i podawać. Mentaci byli o tyle lepsi od komputerów, że można było z nimi rozmawiać naturalnym językiem. Dziś jesteśmy coraz bliżej zatoczenia koła i stworzenia elektronicznego mentata. Kompletnie znikąd napadła na nas rewolucja technologiczna. Nie wiadomo skąd pojawiły się ChatGPT, DALL·E, GitHub Copilot i im podobne. Nie da się jednak zaprzeczyć, że te narzędzia są użyteczne, w tym w nauce. Ale nie wszystkie narzędzia sprawdzają się równie dobrze.\n\n\n\n\n\n29 kwietnia 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nListy, pętle i automatyzacja w R\n\n\n\n\n\n\nR\n\n\n\nAutomatyzacja to jedna z najpotężniejszych zalet R, której nie mają programy statystyczne oparte na interfejsie graficznym. Jeśli chcemy zrobić jakąś czynność wiele razy tak samo lub prawie tak samo, możemy wykorzystać siłę programowania i zrobić to setki razy używając ledwie kilku linijek. Podstawowym sposobem automatyzacji w R – czym różni się od wielu innych języków programowania – są listy, czyli zbiory obiektów.\n\n\n\n\n\n8 marca 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nPodstawy programowania w R\n\n\n\n\n\n\nStatystyka\n\n\nR\n\n\n\nPisanie analiz statystycznych w R jest coraz bardziej powszechne. Nie bez powodu. W R da się zrobić wszystko, jest darmowe i pozwala na automatyzację. Jak zacząć uczyć się R? Odpowiadam na to w tym tekście.\n\n\n\n\n\n21 lutego 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nKombinatoryka\n\n\n\n\n\n\nMatematyka\n\n\n\nIle wykonano uścisków dłoni w pomieszczeniu, w którym jest 100 osób? Omawiam tutaj trzy podstawowe pojęcia kombinatoryki – wariacje, kombinacje i permutacje.\n\n\n\n\n\n17 lutego 2023\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nTesty statystyczne i wartość p\n\n\n\n\n\n\nStatystyka\n\n\n\nCzymże jest prawda? Czy czegokolwiek możemy być pewni? Nie lubimy się pakować w takie pytania, bo łatwo dojść do odpowiedzi „nie”, a wtedy tracimy grunt pod nogami. Część z nas zgodzi się, że tak naprawdę niczego nie możemy być pewni. W końcu zawsze może się okazać, że żyjemy w Matrixie, a wtedy żadne dowody nie mają znaczenia. Ale jednak pewnych rzeczy jesteśmy bardziej pewni niż innych.\n\n\n\n\n\n21 września 2022\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nWyjaśnianie wariancji i test F\n\n\n\n\n\n\nStatystyka\n\n\n\nCo to znaczy, że geny wyjaśniają 90% wariancji wzrostu? Że z moich 170 cm wzrostu 153 cm zawdzięczam genom, a pozostałe 17 cm środowisku? Czym w ogóle jest wariancja? Odpowiem na to pytanie, jak i wprowadzę pojęcie modelu i jego testowania.\n\n\n\n\n\n19 maja 2022\n\n\nJakub Jędrusiak\n\n\n\n\n\n\n\n\n\n\n\n\nPamięć w służbie ucznia\n\n\nPotencjał technik uczenia się w edukacji\n\n\n\nPsychologia\n\n\n\nPrzy coraz powszechniej obserwowanym zjawisku wypalenia szkolnego pośród uczniów, a także niskiej długoterminowej skuteczności ogólnie stosowanych technik uczenia się, pojawia się potrzeba nabycia przez uczniów nowych, skuteczniejszych sposobów zdobywania wiedzy. Jest to szczególnie ważne, biorąc pod uwagę wysokie rozpowszechnienie fałszywych informacji na temat uczenia się. Artykuł redefiniuje pojęcie technik uczenia się odróżniając je od mnemotechnik, podaje ogólne prawidłowości rządzące zapamiętywaniem, a także omawia trzy konkretne techniki – pałac pamięci, metodę Feynmana i fiszki elektroniczne z wykorzystaniem otwartego programu Anki.\n\n\n\n\n\n7 lutego 2020\n\n\nJakub Jędrusiak\n\n\n\n\n\n\nBrak pasujących"
  }
]