[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publikacje",
    "section": "",
    "text": "(Jędrusiak, 2021)"
  },
  {
    "objectID": "publications.html#książki",
    "href": "publications.html#książki",
    "title": "Publikacje",
    "section": "Książki",
    "text": "Książki\n\n(Burzyński, 2020)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jakub Jędrusiak",
    "section": "",
    "text": "Student psychologii Uniwersytetu Wrocławskiego. Strona pogądowa do gromadzenia materiałów. Aktualnie niewiele się tu dzieje."
  },
  {
    "objectID": "statystyka/procent-wariancji.html",
    "href": "statystyka/procent-wariancji.html",
    "title": "Jakub Jędrusiak",
    "section": "",
    "text": "W psychologii można spotkać się z przytaczaniem wartości odziedziczalności. Możemy, dla przykładu, przeczytać, że wzrost jest odziedziczalny w 90% (Plomin, DeFries, McClearn, & McGuffin, 2001). Ale co to znaczy? Że z moich 170 cm wzrostu 153 cm zawdzięczam genom, a pozostałe 17 cm środowisku? To jest bez sensu. Czy odziedziczalność powie mi, czy byłem skazany na bycie niskim od początku albo czy dało się tego uniknąć? W Genetyce zachowania możemy przeczytać, że odziedziczalność to:\n\nCzęść zmienności fenotypowej pomiędzy osobnikami, która może być przypisana różnicom genetycznym w określonej populacji (Plomin i in., 2001, s. 401).\n\nW innym miejscu spotykamy uszczegółowienie:\n\nOdziedziczalność to odsetek (część) wariancji fenotypowej, którą można przypisać różnicom genetycznym pomiędzy jednostkami (Plomin i in., 2001, s. 104).\n\nPo którym następuje groźnie brzmiące ostrzeżenie:\n\nOdziedziczalność odnosi się do wkładu genetycznego do różnic indywidualnych (wariancji), a nie do fenotypu pojedynczej jednostki (Plomin i in., 2001, s. 106).\n\nŁatwo jest zrozumieć, że istnieje jakieś nieporozumienie co do tego, czym jest odziedziczalność. Widzimy wyraźnie, że odziedziczalność nie ma oznaczać ograniczeń, jakie biologia narzuca na jednostkę, ale że jest to miara, która w jakiś sposób dotyczy całej populacji. Pojawia się tajemnicza wariancja, co powoduje mroczne skojarzenia z matematyką. Ale ciągle możemy mieć wątpliwości co do tego, co ma znaczyć, że wzrost jest odziedziczalny w 90%? Bez tła statystycznego powyższe wyjaśnienia właściwie nic nie wyjaśniają. Ale na ich podstawie możemy wyrazić myśl o wzroście tak: czynniki genetyczne wyjaśniają 90% wariancji w zakresie wzrostu w populacji. I tak dochodzimy do clou tego tekstu – co to znaczy, że coś wyjaśnia 90% wariancji?\n\n\nBy zrozumieć sens odziedziczalności (i podobnych tworów) w kontekście wyjaśniania wariancji, musimy najpierw powiedzieć sobie coś o tym, czym jest owa tajemnicza wariancja. Wykorzystamy do tego spreparowany zbiór danych nt. związku wielkości pająków z niepokojem odczuwanym przez obserwatorów(na podstawie: Field, Miles, & Field, 2012). Wyobraźmy sobie, że wielkość pająka wyrażono na Międzynarodowej Skali Wielkości Pająków, zaś niepokój mierzono kwestionariuszowo. Dane zebrano w tabeli poniżej.\n\n\n\n\nTabela 1:  Wielkość pająka i niepokój, jaki wzbudza. \n \n  \n    wielkość pająka \n    niepokój \n  \n \n\n  \n    1 \n    2 \n  \n  \n    2 \n    8 \n  \n  \n    3 \n    5 \n  \n  \n    4 \n    8 \n  \n  \n    5 \n    12 \n  \n  \n    6 \n    11 \n  \n  \n    7 \n    18 \n  \n  \n    8 \n    15 \n  \n\n\n\n\n\n\nDla takich danych możemy policzyć całkiem sporo rzeczy. Łatwiej jest jednak zrozumieć dane, jak się je rzeczywiście widzi. Spróbujmy więc stworzyć wykres. Chcielibyśmy wiedzieć, jaki jest związek wielkości pająka z niepokojem, jaki wywołuje1.\n\n\n\n\n\nRysunek 1: Wielkość pająka i niepokój, jaki wzbudza, tyle że na wykresie.\n\n\n\n\nWidzimy więc wyraźnie, że im większy pająk, tym większy lęk wywołuje. Ale z tym wnioskiem wybiegliśmy sporo w przyszłość. Zazwyczaj w pierwszym odruchu robimy coś znacznie prostszego – liczymy średnią. Średni niepokój wyniósł tutaj dokładnie 9,875. Szybkie spojrzenie w tabelę pozwala nam stwierdzić, że żaden badany takiego wyniku niepokoju nie uzyskał i niezbyt nas to dziwi. W końcu to tylko średnia, żaden prawdziwy wynik nie musi mieć dokładnie średniej wartości. Jak w znienawidzonym przez statystyków kawale o średniej liczbie nóg na spacerze z psem. Rozumiemy więc, że średnia tylko w pewnym przybliżeniu oddaje cały zbiór danych, a konkretne wartości mogą się od średniej mniej lub bardziej odchylać. Te odchylenia możemy nanieść na wykres.\n\n\n\n\n\nRysunek 2: Średnia to rodzaj modelu, czyli narzędzia do przewidywania danych. Żaden z naszych pająków nie wywoływał średniego niepokoju, każdy punkt ma większe lub mniejsze odchylenie od średniej.\n\n\n\n\nPrzerywane linie pokazują nam odchylenie każdego punktu od średniej (residual). Dla przykładu pierwszy punkt ma wartość niepokoju 2, więc jego odchylenie to \\(2 - 9,875 = -7,875\\). Dlaczego na minusie? Bo pierwszy punkt jest poniżej średniej, więc żeby przejść od średniej do wartości tego konkretnego punktu musimy wykonać odejmowanie. Inny przykład – ostatni punkt ma wartość niepokoju 15, więc jego odchylenie od średniej wynosi \\(15 - 9,875 = 5,125\\). Tutaj odchylenie jest już dodatnie, co ma sens, bo ostatni punkt jest powyżej średniej, więc żeby przesunąć się od wartości średniej do wartości tego punktu musimy dodawać. Jak widać, żeby policzyć odchylenie, wystarczy od wartości punktu odjąć średnią. Można zapisać to matematycznie:\n\\[\nS_i = x_i - \\bar x\n\\] gdzie \\(S_i\\) to odchylenie, \\(x_i\\) to wartość konkretnego punktu, a \\(\\bar x\\) to średnia. Pozioma kreska nad czymś zawsze oznacza średnią. Dla każdego punktu moglibyśmy policzyć takie odchylenie i dopisać sobie je do tabeli.\n\n\n\n\nTabela 2:  Każdy punkt ma swoje odchylenie od średniej, które możemy policzyć. Dodatnie odchylenia oznaczają, że wartość jest wyższa od średniej, a ujemne, że jest niższa. \n \n  \n    wielkość pająka \n    niepokój \n    Si \n  \n \n\n  \n    1 \n    2 \n    2 - 9,875 = -7,88 \n  \n  \n    2 \n    8 \n    8 - 9,875 = -1,88 \n  \n  \n    3 \n    5 \n    5 - 9,875 = -4,88 \n  \n  \n    4 \n    8 \n    8 - 9,875 = -1,88 \n  \n  \n    5 \n    12 \n    12 - 9,875 = 2,12 \n  \n  \n    6 \n    11 \n    11 - 9,875 = 1,12 \n  \n  \n    7 \n    18 \n    18 - 9,875 = 8,12 \n  \n  \n    8 \n    15 \n    15 - 9,875 = 5,12 \n  \n\n\n\n\n\n\nŚrednia jest więc pewnym modelem, czyli służy nam jako uproszczony opis danych. Ponieważ modele są uproszczone, zawsze zawierają w sobie jakiś błąd. tym wypadku błędem jest po prostu odchylenie, które właśnie policzyliśmy.\nJakby ktoś zapytał mnie, jak duży niepokój wywołują pająki i uparł się, żeby podać mu jedną liczbę, to podałbym właśnie średnią. Dlaczego? Średnie mają tę niezwykłą właściwość, że starają się być tak bardzo na środku, tak blisko każdego punktu, jak to możliwe. Innymi słowy zwykła średnia daje nam mniejsze odchylenia niż jakakolwiek inna pojedyncza liczba. Dla przykładu, jakbyśmy stwierdzili sobie, że nie chcemy brać za model takiej brzydkiej liczby jak 9,875, a chcemy wziąć, powiedzmy, 15, to nagle okazałoby się, że odchylenia nam wzrosły. Obrazowo mówiąc, przerywane kreski na wykresie zrobiłyby się dłuższe.\nŚrednia gwarantuje mi, że pomylę się tak mało, jak to tylko możliwe, czyli że sumarycznie przerywane kreski na wykresie będą tak krótkie, jak się da. Średnia jest więc modelem niedoskonałym (bo ma błąd), ale lepszym niż jakakolwiek inna pojedyncza liczba (bo ma najmniejszy błąd).\n\n\n\n\n\nRysunek 3: Modelem mogłaby być dowolna liczba, jeśli się uprzeć, ale żadna pojedyncza liczba nie będzie miała tak małych odchyleń, jak średnia.\n\n\n\n\n\n\n\nPotrafimy policzyć odchylenie dowolnego punktu od średniej i wiemy, jak wyglądają te odchylenia na wykresie. Powiedziałem też, że sumarycznie średnia daje mniejsze odchylenia niż jakakolwiek inna liczba. Kusi więc, żeby odchylenia te rzeczywiście zsumować. W końcu w ten sposób wiedzielibyśmy, ile mamy błędu w modelu, czy jest go dużo, czy mało. Co więcej, poszczególne odchylenia różnią się między sobą. Fajnie więc by było policzyć średnią odchyleń. Takie średnie odchylenie powiedziałoby nam, czy ogólnie patrząc odchylenia są duże, czy niewielkie, a więc czy sama średnia jest blisko danych (dobrze je przybliża), czy może jest kompletnie od czapy (nie oddaje dobrze danych)2.\nSkuszeni matematyczną perspektywą bierzemy kalkulator w dłoń, dodajemy do siebie wartości odchyleń (co ładnie, matematycznie możemy zapisać jako \\(\\sum S_i\\), bo wielka litera sigma znaczy po prostu dodawanie)3 i bardzo się dziwimy, kiedy wychodzi nam 0. Ale jak 0? W końcu sumaryczne odchylenie nie może nam wyjść 0! Chcieliśmy jakiejś dużej liczby, która da nam ogólne pojęcie o tym, jak nasze dane odchylają się od średniej, a wychodzi nam 0, tak jakby w ogóle nie było żadnych odchyleń. Po chwili jednak orientujemy się dlaczego – część odchyleń jest dodatnich, a część ujemnych, to jak się je doda, to się zerują. No tak. A średnia (jak sama nazwa wskazuje) jest dokładnie pośrodku tych danych.\nTo w takim razie, jak chcemy dostać to nasze ogólne odchylenie, to musimy pozbyć się minusów. Moglibyśmy je po prostu zignorować (tzn. dodawać wartości bezwzględne odchyleń \\(|S_i|\\)), ale to by przecież by łoza proste. Statystycy, chcąc utrudnić wszystkim życie4, wybrali inny sposób na pozbywanie się minusów, czyli podnoszenie do potęgi drugiej. Minus razy minus daje plus, głosi szkolna formułka. Opanowujemy więc flashbacki z liceum, zaciskamy zęby, podnosimy odchylenia do kwadratu i znowu je do siebie dodajemy. Matematycznie wyrazilibyśmy to tak:\n\\[\nSS_T = \\sum_{i=1}^N S_i^2 = \\sum_{i=1}^N (x_i - \\bar x)^2\n\\]\nZa tym przerażającym zapisem kryje się prosta idea – weź wszystkie odchylenia (a mamy ich N, bo tak się oznacza liczbę zebranych obserwacji) i dodaj je wszystkie po kolei, zaczynając od 1. i na N-tym (w naszym przypadku 8.) kończąc. Po znaku równa się przypominam, co właściwie oznaczaliśmy przez \\(S_i\\), czyli obserwacja odjąć średnia do kwadratu. Jak to rzeczywiście zrobimy, wyjdzie nam 190,875. Niezbyt ładna liczba, ale daje więcej satysfakcji niż 0. Tę liczbę we wzorze nazwałem \\(SS_T\\), co jest skrótem od total sum of squares, czyli całkowita suma kwadratów. Jak się zastanowić, to jest to niezła nazwa, bo od razu jest praktycznie wzorem.\nAle z całkowitą sumą kwadratów jest pewien problem. Siłą rzeczy, im będzie więcej obserwacji, tym ta liczba będzie większa, nawet jeśli odchylenia będą mniejsze. 100 ciężarówek z 50 paletami każda dadzą w sumie więcej towaru, niż 1 ciężarówka z 200 paletami. Suma ocen wzrasta z każdą oceną, mimo że średnia może nawet nie drgnąć. Ale możemy sobie z tym poradzić dość łatwo – po prostu zróbmy z tego średnią. Średnia to suma dzielona przez liczbę przypadków. \\(SS_T\\) jest sumą kwadratów odchyleń (w tym miejscu tekstu należy się zatrzymać i upewnić się, że rozumie się wyrażenie „suma kwadratów odchyleń”), więc jak podzielimy ją przez liczbę obserwacji, to wyjdzie nam średnia z kwadratów odchyleń. Prawda? No prawie, bo to znowu byłoby za proste.\nNie rozwodząc się za bardzo, bo to też jest szeroki temat, zamiast dzielić \\(SS_T\\) przez 8, musimy podzielić ją przez 7. Mówiąc w dużym skrócie, nas w badaniach obchodzi cała populacja. Badamy tylko (losową) próbę z tej populacji, ale tak naprawdę to o samej populacji chcemy wnioskować. Nie wystawiłem 8 osób na kontakt z dużymi i małymi pająkami, bo ciekawiły mnie te konkretne osoby, tylko dlatego, że chcę powiedzieć coś ogólnie o ludziach, o Polakach, o nastolatkach czy jakiejkolwiek innej interesującej mnie populacji. Jeśli podzielilibyśmy przez 8, to wyszłaby nam średnia, która jest prawdziwa dla tej konkretnej próby, ale prawie na pewno niższa, niż ta prawdziwa, populacyjna. Ale jak podzielimy przez 7, to będziemy znacznie, znacznie bliżej prawdziwym wartościom. To da się matematycznie udowodnić, ale bez przesady, nie wszystko naraz. Ta skorygowana liczba nazywa się stopniami swobody i dla średniej zawsze jest ich \\(N-1\\). To będzie ogólnie problem w statystyce, że od czasu do czasu będziemy musieli dzielić przez stopnie swobody zamiast normalnie przez wielkość próby, żeby dostać bardziej wiarygodne liczby. Jeśli uczy się tego pierwszy raz, to można to \\(N-1\\) przyjąć na wiarę, bo sama wariancja jest wystarczająco skomplikowana. Nie trzeba wszystkiego w pełni zrozumieć od razu.\nTakże jeśli chcemy uzyskać średnią kwadratów odchyleń, to bierzemy naszą całkowitą sumę kwadratów i dzielimy ją przez \\(N-1\\). Tak jak średnią ocen liczymy dzieląc sumę ocen przez ich liczbę. Możemy to zapisać matematycznie na kilka sposobów:\n\\[\n\\sigma^2 = \\frac{SS_T}{N-1} = \\frac{\\sum^N_{i = 1} S_i^2}{N-1} = \\frac{\\sum^N_{i = 1} (x_i - \\bar x)^2}{N-1}\n\\]Spokojnie. To są tylko wzory, nie trzeba się ich bać. Każdy z tych trzech wzorów znaczy to samo i opiera się na tym, co już wiemy z poprzednich części tekstu. Tak naprawdę to ciągle wzór na \\(SS_T\\), tylko teraz podzielony na \\(N-1\\). Po pokonaniu pierwszego szoku można zacząć świętować! W ten sposób udało nam się wreszcie policzyć średnią kwadratów odchyleń (ponownie – trzeba się zatrzymać, upewnić się, że się rozumie, ewentualnie trochę się cofnąć i dopiero można iść dalej).\nTrzeba przyznać, że „średni kwadrat odchylenia” albo „średnia z odchyleń podniesionych do potęgi drugiej” to nie są zbyt chwytliwe nazwy. Dlatego właśnie nazwano to wariancją i oznaczono przez \\(\\sigma^2\\). To jest mała grecka litera sigma. A dlaczego \\(\\sigma^2\\) a nie po prostu \\(\\sigma\\)? Bo to średnia z kwadratów odchyleń. Jeśli byśmy chcieli wycofać się teraz z tej gmatwającej wszystko decyzji o podnoszeniu czegokolwiek do kwadratu i uzyskać upragnione średnie odchylenie, to możemy teraz wyciągnąć pierwiastek ze wzoru na wariancję, jaki nam przed chwilą wyszedł i który na pierwszy rzut oka może przytłaczać.\n\\[\n\\sigma = \\sqrt{\\frac{SS_T}{N-1}} = \\sqrt{\\frac{\\sum^N_{i = 1} S_i^2}{N-1}} = \\sqrt{\\frac{\\sum^N_{i = 1} (x_i - \\bar x)^2}{N-1}}\n\\]Każdy z tych wzorów znaczy ostatecznie to samo, więc można używać któregokolwiek. Ważne jest to, że wychodzi nam z tego średnie odchylenie, które nazywamy odchyleniem standardowym i które oznaczamy literą \\(\\sigma\\) lub skrótem SD (standard deviation). Odchylenie standardowe, jak już wyżej wspomniałem, mówi nam, czy średnia dobrze reprezentuje dane. Jeśli jest wysokie, to znaczy, że dane są mocno rozproszone (odchylenia są ogólnie duże) i sama średnia jest mało wiarygodna. Jeśli SD jest niewielkie, to średniej zasadniczo można zaufać. To jakie SD jest duże, a jakie małe, zależy od tego, co mierzymy. Dla średniej pensji SD = 10 uznalibyśmy za raczej niewielkie, ale takie samo odchylenie standardowe dla średniej liczby nóg takie małe już się nie wydaje. Także może i mamy średnio po 3 nogi jak wychodzimy z psem na spacer, ale ta średnia ma całą nogę odchylenia standardowego!\nPodsumowując ten fragment:\n\nZauważyliśmy, że każdy punkt ma swoje większe lub mniejsze odchylenie od średniej.\nPróbowaliśmy sumować te odchylenia, ale przeszkadzały nam minusy, więc przed sumowaniem podnosiliśmy nasze odchylenia do kwadratu, uzyskując w ten sposób całkowitą sumę kwadratów (\\(SS_T\\)).\nWykorzystaliśmy całkowitą sumę kwadratów do policzenia wariancji (średniej z kwadratów odchyleń) w taki sposób, że podzieliliśmy \\(SS_T\\) przez \\(N-1\\) (stopnie swobody). Nie mogliśmy podzielić \\(SS_T\\) przez \\(N\\), bo to zaniżyłoby prawdziwą wariancję w populacji.\nWyciągnęliśmy pierwiastek z wariacji, żeby wreszcie dostać średnie odchylenie, które nazywamy odchyleniem standardowym.\n\n\n\n\nWiemy już bardzo dokładnie czym jest wariancja i jaki ma związek ze średnią. Ale nie oszukujmy się, jesteśmy w stanie wymyślić lepsze modele niż średnia. Od samego początku korci nas, żeby na wykresie niepokoju od wielkości pająka narysować piękną, skośną, rosnącą linię, która będzie zależała nie tylko od wartości niepokoju, ale również od wielkości pająka. Przecież od razu widać, że pasowałaby do naszych danych znacznie bardziej.\n\n\n\n\n\nRysunek 4: Linia rosnąca sugeruje nam, że im większy pająk, tym większy niepokój wywołuje. Na pierwszy rzut oka widać, że odchylenia są mniejsze, niż przy średniej.\n\n\n\n\nOd razu lepiej. Od początku chcieliśmy móc powiedzieć, że im większy pająk, tym większy niepokój, a teraz mamy tego dowód. W końcu ta linia ewidentnie pasuje do danych lepiej – na pierwszy rzut oka widać, że odchylenia są mniejsze. No właśnie! Dzięki łamaniu sobie głowy nad odchyleniami od średniej zyskaliśmy sposób na stwierdzenie, czy jakiś model jest lepszy od średniej, czy nie – wystarczy sprawdzić, czy odchylenia zrobiły się istotnie mniejsze.\nSpróbujmy więc zrobić dla tych nowych odchyleń to samo, co zrobiliśmy dla odchyleń od średniej. Ponieważ samo wyznaczanie wartości tych odchyleń wymaga bardziej złożonych obliczeń ze wzoru na prostą, po prostu je podam (bo policzył je za mnie program statystyczny). Powiem tylko, że procedura jest identyczna, jak w przypadku liczenia \\(SS_T\\), ale licząc odchylenia zamiast wartości średniej (która poprzednio była naszym modelem) odejmujemy wartość, którą przewiduje nasz nowy, lepszy model, a którą w tabeli oznaczyłem jako \\(x_M\\). Podnosimy więc wartości odchyleń (oznaczone w tabeli \\(S_R\\)) do kwadratu, sumujemy i wychodzi nam w zaokrągleniu \\(SS_R = 32,726\\). \\(SS_R\\) oznacza residual sum of squares i w gruncie rzeczy mówi nam o tych odchyleniach, które ciągle są, które nie zniknęły, mimo że nowy model jest lepszy.\n\n\n\n\nTabela 3:  Nowy model magicznie przewiduje niepokój z wielkości pająka (xM), ale nie robi tego doskonale. Tak jak dla średniej, możemy policzyć, jak wartość prawdziwa odchyla się od tej z modelu. \n \n  \n    wielkość pająka \n    niepokój \n    xM \n    SR \n  \n \n\n  \n    1 \n    2 \n    3,08 \n    -1,08 \n  \n  \n    2 \n    8 \n    5,02 \n    2,98 \n  \n  \n    3 \n    5 \n    6,96 \n    -1,96 \n  \n  \n    4 \n    8 \n    8,9 \n    -0,9 \n  \n  \n    5 \n    12 \n    10,85 \n    1,15 \n  \n  \n    6 \n    11 \n    12,79 \n    -1,79 \n  \n  \n    7 \n    18 \n    14,73 \n    3,27 \n  \n  \n    8 \n    15 \n    16,67 \n    -1,67 \n  \n\n\n\n\n\n\nMożemy w pewnym sensie mówić o wariancji wokół średniej i wariancji wokół modelu5. Wariancja, niezależnie wokół czego, to suma kwadratów podzielona przez \\(N-1\\), więc naturalnie musielibyśmy teraz \\(SS_T\\) i \\(SS_R\\) podzielić przez \\(N-1\\). Jednak to, co chcę pokazać dalej, będzie łatwiejsze do zrozumienia, jeśli będziemy się posługiwać surowymi sumami kwadratów. Możemy więc zignorować dzielenie przez \\(N-1\\). Jeśli ktoś mi nie ufa, to może robić to, co za moment zrobię, ale wcześniej podzielić nasze sumy kwadratów przez \\(N-1\\) i gwarantuję, że wyjdzie mu to samo6.\nMożemy więc spojrzeć na to w ten sposób – na początku mieliśmy 190,875 jednostek wariancji, a teraz, po dopasowaniu nowego modelu, mamy ich tylko 32,726. Dla wygody zaokrąglijmy te liczby do całości. Także cała różnica, \\(191 - 33 = 158\\), gdzieś nam wyparowała. Ta wariancja była, a teraz nagle jej nie ma. O takiej wariancji możemy powiedzieć, że została wyjaśniona i możemy ją oznaczyć \\(SS_M\\) od model sum of squares. Czyli stworzyliśmy nowy model, który pozwala nam przewidywać niepokój z wielkości pająka i w ten sposób wyjaśniliśmy jakąś część wariancji. Jaką? Coż, wystarczy to przeliczyć na procenty. Cała wariancja7 wynosiła191 jednostek, a wariacja wyjaśniona wyniosła158 jednostek. Odsetek wariancji, który udało nam się wyjaśnić, oznaczamy \\(R^2\\) i możemy go wyrazić w procentach8.\n\\[\nR^2 = \\frac{158}{191}\\times 100\\% = 82,7\\%\n\\]Możemy więc powiedzieć, że wielkość pająka (bo nowy model przewiduje niepokój na podstawie wielkości pająka) wyjaśnia 82,7% wariancji niepokoju. Ludzie ciągle różnią się niepokojem w reagowaniu na pająki, ale odchylenia od nowego modelu są znacznie mniejsze. Ogólny wzór na \\(R^2\\) wygląda więc tak9:\n\\[\nR^2 = \\frac{SS_M}{SS_T} = \\frac{SS_T - SS_R}{SS_T}\n\\]\nMoglibyśmy się pytać, co odpowiada za pozostałe 17,3% wariancji. Mogą to być dodatkowe czynniki, takie jak doświadczenia z pająkami w dzieciństwie albo uszkodzenia mózgu. To wymaga dalszych badań.\nMożemy wyobrazić sobie, że pojawia nam się nowa osoba badana, która pająków nie boi się w ogóle, co pokazałem na wykresie. Jak to wpływa na nasz model?\n\n\n\n\n\nRysunek 5: Nowa osoba badana, która nie boi się pająków wcale, nie przystaje do naszego modelu. Może to sugerować, że nasz model jest adekwatny tylko dla pewnej grupy osób (czyli możemy stworzyć bardziej złożony model, który weźmie to pod uwagę) albo że odważny badany kłamie (i powinien zostać wykluczony z bazy).\n\n\n\n\nJak widzimy model się trochę pozmieniał. \\(R^2\\) spadło dramatycznie, bo w tym drugim modelu wyniosło zaledwie 21,9%. Ale pojawienie się tego nowego, odważnego badanego nie zmieniło wyników pozostałych.\nTak samo jest z odziedziczalnością. Odziedziczalność mówi nam, jaką część różnic w populacji da się wyjaśnić czynnikami genetycznymi, ale nawet bardzo wysoka odziedziczalność nie mówi nam, jakie są biologiczne, genetyczne granice ludzkich możliwości rozwoju. Jeśli pojawi się zdeterminowana jednostka, to może ona wstrząsnąć naszym modelem albo, co ciekawsze, nie zrobić żadnej różnicy w modelu (jeśli próba byłaby odpowiednio duża)10. W takim wypadku model ciągle byłby bardzo adekwatny, wyjaśniałby bardzo dużo wariancji, a jednocześnie nie wykluczał istnienia jednostek, które całkowicie do niego nie przystają.\nDlatego właśnie odziedziczalność tej samej cechy może być różna w różnych populacjach albo zmieniać się w czasie – odziedziczalność, tak jak każdy model statystyczny, opisuje różnice tu i teraz, na poziomie populacji. Niestety (albo na szczęście) sama odziedziczalność nie może rozstrzygnąć, czy ja, konkretna jednostka, od urodzenia byłem skazany na bycie tak niskim. Może mi jednak powiedzieć, że czynniki genetyczne wyjaśniają 90% wariancji wzrostu w mojej populacji. Tylko teraz już wiem, co to znaczy.\n\n\n\nMoglibyśmy w tym momencie skończyć, ale skoro mamy wszystkie skróty powtórzone na świeżo, dobrze byłoby jeszcze jedną sprawę omówić. Wiedzę o tym, czym jest \\(SS_M\\) i \\(SS_R\\) (tzn. cała wariancja wyjaśniona przez model i cała wariancja niewyjaśniona przez model) możemy wykorzystać, do zrozumienia potężnej, uniwersalnej statystyki diagnostycznej – \\(F\\). Najpierw wyjaśnię, jak to się liczy, a potem jak się to interpretuje.\nStatystyka \\(F\\) to stosunek (czyli wynik dzielenia) wariancji wyjaśnionej do wariancji niewyjaśnionej. Matematycznie możemy zapisać to tak:\n\\[\nF = \\frac{MS_M}{MS_R}\n\\]\nMoment. Dopiero akapit wyżej pisałem, że wariancja wyjaśniona to \\(SS_M\\), a niewyjaśniona to \\(SS_R\\), a teraz nagle piszę \\(MS_M\\) i \\(MS_R\\). Czy to podważa moją wiarygodność? Czy należy rzucić komputerem w proteście przeciw umowności matematyki? Nie. Bez przesady. \\(M\\) znaczy średnia.\nPrzypominam – \\(SS_M\\) i \\(SS_R\\) to sumy kwadratów, tak jakbyśmy tylko zsumowali swoje oceny bez liczenia średniej. Do policzenia \\(F\\) potrzebujemy nie sum kwadratów, a średniego kwadratu. Ponownie, średnią ocen liczymy dzieląc sumę ocen przez ich liczbę. Mamy już policzone sumy kwadratów \\(SS_M\\) i \\(SS_R\\), więc dzielimy je przez liczbę obserwacji. Uważnemu czytelnikowi zapaliła się właśnie w głowie lampka ostrzegawcza, bo pisałem przecież, że liczba przypadków jest zakłamana i nie można jej ufać. Nie pożyczamy jej pieniędzy. Ufać można stopniom swobody i przez nie powinniśmy podzielić. Już raz to zrobiliśmy, podzieliliśmy \\(SS_T\\) przez stopnie swobody i nazwaliśmy to wariancją. Teraz tę samą logikę chcemy zastosować do \\(SS_R\\) i \\(SS_M\\).\nTylko pojawia się trudne pytanie – ile stopni swobody mają nasze \\(SS_M\\) i \\(SS_R\\)? Już odpowiadam. Liczba stopni swobody dla \\(SS_M\\) to liczba zmiennych niezależnych (w przykładzie z pająkami to 1, bo lęk przewidujemy tylko na podstawie wielkości pająka). Liczba stopni swobody dla \\(SS_R\\) to liczba obserwacji – liczba zmiennych niezależnych – 1 (w naszym przypadku \\(8-1-1 = 6\\)). I to trzeba po prostu wziąć na wiarę.\nTakże bierzemy nasze sumy kwadratów i robimy z nich średnie. \\(MS_M = \\frac{158,15}{1} = 158,15\\) oraz \\(MS_R = \\frac{32,73}{6} = 5,45\\). Mając średnie możemy policzyć wartość naszej statystyki \\(F = \\frac{158,15}{5,45} \\approx 29\\). Czad. Tylko co z tego?\nTa liczba, 29, informuje nas, że ilość wariancji wyjaśnionej przez model jest 29 razy większa niż ilość wariancji niewyjaśnionej. Czyli że model więcej wyjaśnia niż nie wyjaśnia, bo wyjaśnia średnio 158,15 jednostek wariancji, a nie wyjaśnia średnio 5,45 jednostek wariacji. Dobry model ma co najmniej \\(F > 1\\), bo – jak się zastanowić – wtedy właśnie więcej wyjaśnia, bo nie wyjaśnia11. Licznik jest większy niż mianownik. \\(SS_M > SS_R\\). Jakby wyszło nam, że \\(F = 0,25\\), to znaczyłoby to, że model jest fatalny, bo ma więcej (4 razy więcej!) błędu niż racji. W skrócie mówiąc – im większe \\(F\\), tym lepiej, a \\(F > 1\\) to absolutne minimum.\n\n\n\nMożna zauważyć ciekawą rzecz związaną z obliczaniem \\(SS_M\\). Jak się okazuje, \\(SS_M\\) da się policzyć bez liczenia \\(SS_T\\) i \\(SS_R\\). \\(SS_M\\) uzyskujemy, jeśli zignorujemy obserwacje i zsumujemy kwadraty różnic między średnią a nowym modelem. Zaznaczyłem te różnice na poniższym wykresie przerywanymi liniami.\n\n\n\n\n\nRysunek 6: \\(SS_M\\) można szybciej policzyć za pomocą odchyleń modelu od średniej, czyli kresek między średnią wartościami przewidywanymi przez nasz nowy model.\n\n\n\n\nMożna to przetestować za pomocą danych z tabeli niżej. \\(x_M\\) to wartość przewidywana przez model, a \\(S_M\\) to różnica między modelem a średnim niepokojem. Podniesienie wartości \\(S_M\\) do kwadratu i zsumowanie ich daje w wyniku nasze \\(SS_M\\).\n\n\n\n\nTabela 4:  Podstawą do policzenia SSM mogą być odchylenia modelu od średniej SM. Trzeba je policzyć dla każdego punktu, podnieść do kwadratu i dodać. \n \n  \n    wielkość pająka \n    niepokój \n    xM \n    SM \n  \n \n\n  \n    1 \n    2 \n    3,08 \n    3,08 - 9,875 = -6,8 \n  \n  \n    2 \n    8 \n    5,02 \n    5,02 - 9,875 = -4,86 \n  \n  \n    3 \n    5 \n    6,96 \n    6,96 - 9,875 = -2,92 \n  \n  \n    4 \n    8 \n    8,90 \n    8,9 - 9,875 = -0,97 \n  \n  \n    5 \n    12 \n    10,85 \n    10,85 - 9,875 = 0,97 \n  \n  \n    6 \n    11 \n    12,79 \n    12,79 - 9,875 = 2,91 \n  \n  \n    7 \n    18 \n    14,73 \n    14,73 - 9,875 = 4,86 \n  \n  \n    8 \n    15 \n    16,67 \n    16,67 - 9,875 = 6,8 \n  \n\n\n\n\n\n\nJest to jeszcze jeden sposób myślenia o \\(SS_M\\), który może pojawić się w niektórych opracowaniach. Zgłębienie tego może dać nam nowy sposób myślenia o wariancji wyjaśnianej, ale nie jest konieczne, bo ostatecznie liczy nam to komputer. Rozumienie tego, jak działa \\(SS_M\\) według poprzedniego sposobu (wariancja, która zniknęła, jak zmieniliśmy model ze średniej na jakiś inny) to i tak dużo i powinno wystarczyć."
  },
  {
    "objectID": "statystyka/podstawy_R.html",
    "href": "statystyka/podstawy_R.html",
    "title": "Jakub Jędrusiak",
    "section": "",
    "text": "W kilku miejscach w dalszej części wrzucam informacje, jak daną rzecz omawianą teoretycznie można zrobić w R. Absolutnie nie jest to konieczne do zrozumienia statystyki! Jest to tylko jedna z możliwości, jak można opisaną dalej teorię przekuć w praktykę. O R można myśleć jako o programie do robienia statystyki. Podobnie jak SPSS, Statistica, Stata czy (oparte na R, darmowe i otwartoźródłowe) jamovi. Jeśli jednak Czytelnik widział kiedyś program statystyczny, spodziewać się będzie ekranu podobnego do Excela, gdzie na górnej belce wybiera się testy statystyczne, jakie chce się przeprowadzić. Praca w R tak nie wygląda. Największa wada i zaleta R polega na tym, że jest on jednocześnie językiem programowania. A to daje bardzo ciekawe możliwości, o których niżej. Także praca w R wygląda tak, że w specjalnym języku piszemy komputerowi, co ma zrobić, potem uruchamiamy te instrukcje i gotowe.\nW tym miejscu spróbuję krótko opisać, jak zacząć pracę z R. Nie mam ambicji zrobić pełnego wprowadzenia, bo wyszedłby z tego osobny podręcznik. Mam ambicje dać jakikolwiek zarys fundamentu, który pozwoli Czytelnikowi wyczyścić mało brudne dane i zrobić proste testy. Jeśli ktoś po przeczytaniu tego stwierdzi „Może warto się zagłębić”, to znajdzie mnóstwo materiałów, które mu na to pozwolą. Ze swojej strony mogę polecić podręcznik „Język R. Kompletny zestaw narzędzi dla analityków danych” Wickhama i Grolemunda, interaktywne kursy na DataCamp, a pomocy w rozwiązaniu konkretnych problemów zawsze można szukać na StackOverflow."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-console",
    "href": "statystyka/podstawy_R.html#sec-console",
    "title": "Jakub Jędrusiak",
    "section": "4.1 Konsola, zmienne i matematyka",
    "text": "4.1 Konsola, zmienne i matematyka\nEkran RStudio składa się z trzech okienek. Duże okienko po lewej i dwa mniejsze po prawej. Skierujmy naszą uwagę na okienko po lewej, czyli konsolę. Wita nas ona ciepłą informacją, że R jest zainstalowany i znakiem zachęty > zachęca nas do wydawania jej poleceń. Konsola R to miejsce, w którym możemy mówić R, żeby coś dla nas liczył. Można to potraktować jako super kalkulator. Spróbuj – wpisz w konsolę 2+3*5, zatwierdź enterem i zwróć uwagę, że R stosuje poprawną kolejność wykonywania działań. Spacje nie mają znaczenia, także możemy wpisać również bardziej estetyczną wersję 2 + 3 * 5. Nie wiem po co, ale można.\nWynik takiego działania nigdzie się nie zapisuje, tylko wyświetla się w konsoli. Jeśli chcemy zapisać nasz wynik, możemy to zrobić stosując znaczek <-6. Przydatnym skrótem jest tu w RStudio jest alt+-, który od razu wstawia nam tę strzałeczkę. Wyjaśnijmy to na przykładzie.\n\nwynik <- 2 + 3 * 5\na <- wynik * 3^3\n\nPowyżej zapisałem dwa polecenia, które do konsoli powinniśmy wpisać jedno po drugim i każde z nich zatwierdzić enterem. Pierwsze polecenie mówi coś takiego – policz 2 + 3 * 5 i zapisz to w zmiennej wynik. Po zatwierdzeniu tego polecenia możemy zauważyć, że wynik działania nam się nie wyświetlił, za to w prawym górnym okienku pojawiło się słowo wynik i obok wartość 17. Od tego momentu możemy używać słowa wynik zamiast 17. Spróbuj wpisać w konsolę samo słowo wynik i zatwierdzić enterem. Konsola informuje nas, że w zmiennej wynik kryje się liczba 17. Jeśli teraz wpiszesz np. wynik * 2, to konsola zwróci to samo, co zwróciłaby po wpisaniu 17 * 2. Co więc robi drugie polecenie? Możemy je odczytać jako „W zmiennej o nazwie a zapisz wynik mnożenia wynik i 3 do potęgi 3”. Operator ^ to właśnie potęgowanie. Jeśli wpiszemy w konsolę a, naszym oczom ukaże się wynik 459.\nJak się potem okaże, w zmiennych możemy zapisywać dużo więcej, niż tylko wyniki prostych działań matematycznych. W identyczny sposób do odpowiednich zmiennych trafią wyniki testów statystycznych albo całe bazy danych. Ale o tym dalej."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-functions",
    "href": "statystyka/podstawy_R.html#sec-functions",
    "title": "Jakub Jędrusiak",
    "section": "4.2 Funkcje",
    "text": "4.2 Funkcje\nTym, co robi robotę w R (jak i w każdym innym języku programowania) są funkcje. Funkcja to taka maszynka, do której coś wrzucamy, ona nam to przekształca i wywala coś innego. Tak jak w matematyce. Podobnie jak w matematyce, funkcje zapisujemy konwencją f(x), czyli nazwa(co_wrzucam_do_funkcji). Dla przykładu funkcja seq pozwala nam wytwarzać sekwencje liczb. Musimy do tej funkcji wrzucić od jakiej liczby chcemy zacząć, na jakiej chcemy skończyć i jaki chcemy mieć krok. Dla przykładu:\n\nseq(5, 62, 3)\n\n [1]  5  8 11 14 17 20 23 26 29 32 35 38 41 44 47 50 53 56 59 62\n\n\nTo, co wrzucamy do funkcji, nazywamy argumentami. Funkcja seq wie, że ma zacząć od 5 i skończyć na 62, a nie zacząć od 62 i skończyć na 5, bo ma pod maską zapisane, w jakiej kolejności będzie dostawać te liczby. Takie argumenty nazywamy pozycyjnymi – funkcja wie, co to jest, na podstawie pozycji. W R każdy argument możemy też nazwać. Dla przykładu wiemy, że funkcja seq oczekuje argumentów from, to i by. Możemy więc wprost powiedzieć funkcji, że oto dajemy jej from, to i by.\n\nseq(from = 5, to = 62, by = 3)\n\n [1]  5  8 11 14 17 20 23 26 29 32 35 38 41 44 47 50 53 56 59 62\n\nseq(to = 62, by = 3, from = 5) # jeśli nazywamy argumenty, kolejność nie ma znaczenia\n\n [1]  5  8 11 14 17 20 23 26 29 32 35 38 41 44 47 50 53 56 59 62\n\n\nTego typu argumenty nazywamy kluczowymi (keyword). W praktyce wykorzystuje się mieszankę jednego i drugiego typu argumentów. Nazywanie argumentów zwiększa czytelność kodu, ale czasem pozycja jest wystarczająco jasna. Dla przykładu mogę napisać sqrt(x = 9), żeby wyciągnąć pierwiastek kwadratowy (square root) z 9, ale czy zapis sqrt(9) jest jakkolwiek mniej jasny?\nCzasami też używamy argumentów kluczowych, żeby przestawić jakieś ustawienia domyślne albo odblokować nowe możliwości. Dla przykładu funkcja seq dysponuje dodatkowym argumentem length.out. Jeśli ustawimy length.out, możemy ustalić długość naszego wyniku zamiast punktu końcowego albo kroku.\n\nseq(5, by = 3, length.out = 10) # daj mi 10 kolejnych liczb zaczynając od 5 i co 3\n\n [1]  5  8 11 14 17 20 23 26 29 32\n\nseq(1, 100, length.out = 10) # podaj 10 liczb między 1 a 100\n\n [1]   1  12  23  34  45  56  67  78  89 100"
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-docs",
    "href": "statystyka/podstawy_R.html#sec-docs",
    "title": "Jakub Jędrusiak",
    "section": "4.3 Dokumentacja",
    "text": "4.3 Dokumentacja\nRóżne funkcje przyjmują różne argumenty. Podobnie jak nie powiemy piekarzowi, żeby stosował białą fugę do chleba, tak samo nie wrzucimy do funkcji seq słów zamiast liczb. Tak jak musimy wiedzieć, że piekarz zajmuje się pieczywem, tak samo musimy znać funkcje, których używamy. W poprzednim podrozdziale wiedzieliśmy, co można wrzucić do funkcji seq i jak nazywają się jej argumenty, bo to napisałem. Tak samo podałem ot tak, że argument funkcji sqrt nazywa się x. Skąd mam to jednak wiedzieć?\nNie bez powodu mówimy o językach programowania – wiele funkcji nauczymy się na pamięć i będziemy po prostu wiedzieć, jak z nich korzystać. Jednak znacznie częściej, w wielu przypadkach też dla funkcji, które znamy, będziemy korzystać z dokumentacji. R dysponuje świetną dokumentacją dla każdej funkcji7. Zawiera ona opis, co dana funkcja robi, jakie argumenty przyjmuje, a często nawet tło teoretyczne jej działania. Żeby dostać się do dokumentacji danej funkcji, wywołujemy ją w konsoli ze znakiem zapytania, np. ?seq. Powoduje to, że w okienku Help po prawej wyświetla nam się pełna dokumentacja tej funkcji. Nie trzeba więc sięgać do Google, żeby uzyskać odpowiedź na podstawowe problemy. O ile wiemy, jakiej funkcji chcemy użyć. Zachęcam do częstego sięgania do dokumentacji. To absolutnie podstawowe narzędzie w programowaniu czegokolwiek.\nŚwietnym źródłem informacji o funkcjach, pozwalającym również znaleźć odpowiednią funkcję do naszego celu, są ściągi (cheat sheets). Pakiety tidyverse mają nawet swoje oficjalne ściągi, które na początku swojej nauki R wydrukowałem i zalaminowałem. Polecam je gorąco, zwłaszcza do pakietów dplyr, ggplot2 i stringr. Można je znaleźć bezpośrednio w RStudio wybierając Help → Cheat sheets → Browse all cheat sheets albo na stronie Posit, czyli firmy, która wypuszcza RStudio. Jak przejdziemy dalej, do części praktycznej, polecam, żeby mieć te ściągi już przygotowane, tuż obok."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-scripts",
    "href": "statystyka/podstawy_R.html#sec-scripts",
    "title": "Jakub Jędrusiak",
    "section": "4.4 Skrypty",
    "text": "4.4 Skrypty\nWpisaliśmy w konsolę już sporo rzeczy i historię naszych komend możemy zobaczyć przechodząc do odpowiedniej zakładki w prawym górnym okienku. Jednak wyjście z programu może nam skutecznie skasować tę historię. Jeśli mamy całą sporą analizę statystyczną, która składa się z 200 linijek kodu, to chcielibyśmy mieć jakiś sposób na zapisanie tego na przyszłość, żeby nie musieć za każdym razem wklepywać tego kodu z pamięci. Zaopatrujemy się więc w gruby zeszyt w linie i wszystkie komendy piszemy również tam. Żarcik. Do przechowywania kodu służą specjalne pliki zwane skryptami. Tak jak mamy pliki .pdf, .txt, .docx, tak w plikach .R zapisujemy kod R.\nNajprościej wytworzyć nowy skrypt klikając w biały kwadracik z plusem w lewym górnym rogu RStudio. Spowoduje to otworzenie listy rzeczy, które możemy wytworzyć. Nas w tej chwili interesuje R Script. Gdy utworzymy nasz skrypt, otworzy się on nad konsolą. Warto od razu zapisać go na dysku skrótem Ctrl+S (lub File → Save). Warto się upewnić, że zapisywany plik rzeczywiście kończy się rozszerzeniem .R.\nNa razie nasz skrypt jest pusty, ale możemy w nim pisać dowolne polecenia tak samo, jak napisalibyśmy w konsoli. Różnica jest taka, że nie są one od razu wykonywane. Skrypt to tekst. Jeśli chcemy wykonać jakieś polecenie ze skryptu, to albo kopiujemy je do konsoli, albo umieszczamy na nim kursor i klikamy ctrl+enter. Możemy też myszką zaznaczyć większy fragment kodu i kliknąć Ctrl+Enter, żeby go wykonać. Jeśli chcielibyśmy wykonać cały nasz skrypt, to zaznaczamy cały kod (Ctrl+A) i ponownie używamy Ctrl+Enter. Ewentualnie możemy skorzystać ze skrótu Ctrl+Shift+S8.\nTo jest najważniejsza różnica między skryptem a konsolą – cokolwiek wpisane w konsolę jest wykonywane natychmiast i znika. Z konsoli korzystamy, kiedy chcemy zrobić jakieś jednorazowe operacje albo coś sobie przetestować. W skrypt wpisujemy to, co chcemy zachować. Ewentualnie szkic. Gdy ludzie przechodzą nagle z konsoli do skryptu, bardzo często zaczynają wpisywać w swój skrypt różne śmieci, które wcześniej wpisaliby w konsolę. Konsola nie zniknęła, ciągle jest do naszej dyspozycji. Skrypt w swojej ostatecznej postaci powinien jednak działać tak, że jak go uruchomimy, to cały przeleci bez błędów. No, przynajmniej do tego dążymy. Czyli konsola do testów, skrypt do prawdziwego kodu."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-comments",
    "href": "statystyka/podstawy_R.html#sec-comments",
    "title": "Jakub Jędrusiak",
    "section": "4.5 Komentarze",
    "text": "4.5 Komentarze\nJeśli chcielibyśmy zrobić w skrypcie jakąś notatkę dla siebie, która nie jest kodem używamy znaczka #. Jest to tzw. komentarz. Możemy na przykład napisać:\n\nprint(\"Hello world!\") # czuję się programistą\n\nJeśli wykonamy taką linijkę, konsola zignoruje wszystko po znaku #. Pozwala to nam zostawiać sobie notatki w rodzaju # hipoteza 1 albo # nie wiem, czemu to działa, ale działa. Komentowanie kodu może nam (i naszym współpracownikom) ułatwić zrozumienie, o co nam chodziło, gdy to pisaliśmy. Jeśli chcemy zaopatrzyć nasz kod w nagłówki, konwencja mówi, żeby formatować je tak:\n\n# Przygotowanie ----\n\n## Ładowanie danych ----\n\n# kod ładujący dane\n\n## Ładowanie bibliotek ----\n\n# kod ładujący biblioteki\n\nKażdy znaczek # to niższy poziom nagłówka, czyli wytworzyłem sekcję Przygotowanie, a w niej dwie podsekcje Ładowanie danych i Ładowanie bibliotek. Takich poziomów nagłówków możemy mieć ile chcemy. Nagłówek tym się różni od zwykłego komentarza, że po nim występują cztery myślniki ---- lub inne znaki. Tak sformatowane nagłówki wyświetlają się w bocznym panelu RStudio i pozwalają się lepiej ogarnąć w długim kodzie. Panel outline możemy rozwinąć skrótem Ctrl+Shift+O albo klikając skrajną prawą ikonkę nad edytorem skryptu (poziome kreski na prawo od guzika Source)."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-projects",
    "href": "statystyka/podstawy_R.html#sec-projects",
    "title": "Jakub Jędrusiak",
    "section": "4.6 Projekty",
    "text": "4.6 Projekty\nZazwyczaj projekt badawczy składa się z wielu plików. Nie jest to tylko kod R, ale też chociażby pliki z danymi i inne. Zazwyczaj trzymamy to wszystko w jednym folderze, o ile utrzymujemy jakikolwiek porządek w plikach. Możemy też mieć całe studia luzem na pulpicie, nie oceniam. RStudio pomaga nam w zarządzaniu takimi grupami plików poprzez projekty. Projekty w RStudio robią kilka rzeczy, m.in. pozwalają ustawić niestandardowe opcje (np. zmienić język słownika na angielski tylko dla tego jednego projektu), zapamiętać otwarte okna i ich układ, ale przede wszystkim pomagają nam lokalizować pliki znajdujące się w tym samym folderze9. Zawsze, kiedy planujemy zachować jakiś zbiór powiązanych plików na dłużej, warto jest wytworzyć projekt.\nProjekty tworzymy i otwieramy przez guzik w prawym górnym rogu. Rozwijane menu pozwana nam wytworzyć nowy projekt, a wyskakujące okienko pyta, czy wytworzyć go w już istniejącym folderze, stworzyć nowy folder, czy może pobrać repozytorium Git. Jeśli wybraliśmy nowy folder, mamy kilka typów projektów do wyboru, ale w większości przypadków wybieramy po prostu New Project. Okienko pozwala nam nadać projektowi nazwę, wybrać jego lokalizację, a także wytworzyć puste repozytorium Git10. RStudio wytworzy nam w ten sposób plik .Rproj organizujący nasz projekt."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-libs",
    "href": "statystyka/podstawy_R.html#sec-libs",
    "title": "Jakub Jędrusiak",
    "section": "4.7 Pakiety",
    "text": "4.7 Pakiety\nPakiety (packages lub libraries) to pewnie dodatki do R, które rozszerzają jego możliwości. Dla przykładu – R w swojej podstawowej wersji nie ma funkcji liczącej skośność. Nie jest to jednak żaden problem, bo możemy R rozszerzyć np. o pakiet o nazwie e1071 albo moments. Oba te pakiety dodają nam do R możliwość szybkiego i prostego policzenia skośności. Pakiety – w olbrzymiej większości – są darmowe.\nAbsolutnie podstawowym pakietem, czy właściwie zbiorem pakietów, jest tidyverse. Tidyverse usprawnia R właściwie we wszystkim, co w podstawowej wersji jest niewygodne – readr (czyt. rider) pozwala łatwo ładować dane, dplyr (czyt. diplajer) niesamowicie usprawnia czyszczenie danych, lubridate i stringr (czyt. stringer) to podstawowe narzędzie do pracy z datami i z tekstem11, nie mówiąc już o ggplot2, czyli najpotężniejszym narzędziu do tworzenia wykresów. Na szczęście nie musimy wszystkich tych pakietów przywoływać z osobna, bo możemy załadować je wszystkie naraz, ładując jeden zbiorczy pakiet tidyverse. Współcześnie tidyverse to podstawowy sposób programowania w R. Pakiety ładujemy za pomocą funkcji library, do której wrzucamy nazwę pakietu w cudzysłowie. Nasz skrypt zaczniemy więc od takiej instrukcji:\n\nlibrary(\"tidyverse\")\n\nJeśli robimy to po raz pierwszy, to po wykonaniu skryptu konsola wyrzuci nam błąd Błąd w poleceniu 'library(\"tidyverse\")':nie ma pakietu o nazwie ‘tidyverse’. Wynika to z faktu, że instrukcja library tylko ładuje pakiet, ale przed załadowaniem trzeba go pobrać i zainstalować. Na szczęście robimy to tylko raz, zawsze później wystarczy samo library. Dlatego też nie będziemy wpisywać komendy instalującej pakiet do skryptu, tylko bezpośrednio do konsoli. Nie chcemy w końcu, żeby pakiet tidyverse instalował się za każdym razem, kiedy będziemy uruchamiać skrypt. Będzie to niemiłosiernie spowalniało skrypt i wymuszało dostęp do Internetu. Dlatego też do konsoli wpisujemy:\n\ninstall.packages(\"tidyverse\")\n\nInnym sposobem instalowania pakietów jest skierowanie się w prawe dolne okienko w RStudio, przejście do zakładki Packages, kliknięcie guzika Install, wpisanie nazwy pakietu w wyskakującym okienku (już bez cudzysłowu) i zatwierdzenie guzikiem Install.\nGdy zainstalujemy już pakiet tidyverse – dowolną z metod – ponownie próbujemy go załadować, tym razem, mam nadzieję, już bez błędu. Konsola poinformuje nas wtedy co dokładnie załadowała.\n\nlibrary(\"tidyverse\")\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.1     ✔ purrr   1.0.1\n✔ tibble  3.1.8     ✔ dplyr   1.1.0\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()"
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-choosing",
    "href": "statystyka/podstawy_R.html#sec-choosing",
    "title": "Jakub Jędrusiak",
    "section": "6.1 Wybieranie kolumn i wierszy",
    "text": "6.1 Wybieranie kolumn i wierszy\nBardzo często będziemy potrzebowali tylko określonych kolumn albo tylko określonych przypadków. Przeglądając nasze dane zauważamy, że składają się w dużej części z niepotrzebnych kolumn, jakie wygenerował dla nas program do ankiet. Kolumny takie jak godziny wypełniania są nam niepotrzebne do analizy. Nasze dane to fragment bazy danych z badania, w którym mówiliśmy mężczyznom, że są mało męscy i patrzyliśmy, jak to wpłynie na nich homofobię. Badaliśmy więc wyłącznie mężczyzn, a mimo to ankietę próbowało też wypełnić kilka kobiet i osób o innej płci. Ponieważ ankieta nie dopuściła ich nawet do metryczki, widzimy w ich przypadkach wartości NA, co w R oznacza „brak danych”.\n\n6.1.1 Filtrowanie wierszy z dplyr::filter\nZacznijmy od tego, że w naszej bazie zostawimy tylko mężczyzn. Wszystkie komendy poniżej wpisuję w konsoli, dla testów. Jeśli wpisujemy komendy modyfikujące dane w konsolę, to nie zapisujemy zmian, tylko sprawdzamy, co się stanie, jak tak zrobimy. Dopiero na koniec podam, jak nasze zmiany rzeczywiście zapisać. Jak więc odfiltrować nie-mężczyzn? Robimy to za pomocą komendy filter()12. Tidyverse opiera się o intuicyjnie brzmiące czasowniki takie jak filter, select, group_by, summarise itd. Komenda filter przyjmuje naszą bazę i jakieś warunki, np. płeć męska. W naszym wypadku będzie to wyglądać tak:\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\")\n\nDobra, co to jest %>%? Nie było o tym mowy. Owszem, nie było, ale to bardzo wygodna rzecz. Nazywa się pipe (czasem tłumaczone jako potok). Mówi mniej więcej „wrzuć to do tego”. W naszym przykładzie df %>% filter() oznacza „wrzuć bazę danych df do funkcji filter“, czyli dokładnie to samo, co filter(df)13. Po co więc w ogóle bawić się w potoki? Bo pozwalają nam wygodnie łączyć komendy w ciągi, jak zobaczymy za chwilę. Do wstawiania potoków służy nam wygodny skrót klawiszowy Ctrl+Shift+M, który jest chyba najczęściej stosowanym skrótem przy pisaniu dowolnego programu. Druga kwestia to podział na linijki. Rozbiłem tę komendę na dwie linijki dla czytelności, ale spokojnie mógłbym zapisać to w jednej linijce. Warto jednak pisać kod tak, żeby dało się go potem łatwo czytać. RStudio podpowiada nam też wcięcia, żebyśmy widzieli, że te linijki tworzą jedną całość. Potem opiszę, jak wygodnie formatować kod.\nTa komenda oznacza „weź zmienną df, wrzuć ją do komendy filter i zostaw tylko te przypadki, w których w kolumnie Płeć jest wartość \"Mężczyzna\".” Nazwy kolumn piszemy bez cudzysłowu, ale jeśli wartość komórki to tekst, to zawsze piszemy go w cudzysłowie. Inaczej R pomyśli, że podajemy mu jakąś zmienną, z której ma dopiero odczytać, co ma być w kolumnie Płeć. Nam chodzi o dosłowny tekst \"Mężczyzna\".\nOstatecznie zostaje operator logiczny. Dlaczego piszę == zamiast =? W programowaniu znak = służy do przypisywania wartości do zmiennych. Zapis a = 5 oznacza „niech a ma wartość 5”. Sprawdzenie czy a ma wartość 5 odbywa się poprzez komendę a == 5. Konsola wyrzuci nam wtedy TRUE, FALSE albo BŁĄD: nie znaleziono obiektu 'a'. Kilka innych operatorów logicznych prezentuje tabela.\n\nCzęść operatorów logicznych dostępnych w R.\n\n\n\n\n\n\n\nOperator\nZnaczenie\nPrzykład\n\n\n\n\n==\nrówna się\nPłeć == \"Mężczyzna\"\n\n\n!=\nnie równa się\nPłeć != \"Kobieta\"\n\n\n> (>=)\nwiększe niż(większe lub równe)\nWiek > 40\n\n\n< (<=)\nmniejsze niż(mniejsze lub równe)\nWiek < 40\n\n\n|\nlub\nWiek < 18 \\| Wiek > 60\n\n\n&\ni\nPłeć == \"Mężczyzna\" & Wiek > 40\n\n\n%in%\nzawiera się w zbiorze\nPłeć %in% c(\"Kobieta\", \"Inna\")\n\n\n!\nzaprzeczenie\n! Płeć %in% c(\"Kobieta\", \"Inna\")\n\n\n\n\n\n6.1.2 Wybieranie kolumn z dplyr::select\nOdfiltrowaliśmy więc nie-mężczyzn. Kolejny problem to cała seria niepotrzebnych kolumn. Godziny, adres, zgoda etyczna (która była obowiązkowa i wszyscy się zgodzili) i płeć (już jednakowa dla wszystkich) są nam do niczego niepotrzebne. Do wybierania, jakie kolumny zostawić, służy funkcja select(). Wrzucamy do niej nazwy albo numery kolumn, które chcemy zostawić w bazie. Rozszerzmy więc naszą poprzednią instrukcję o dodatkową komendę za pomocą potoku.\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18)\n\nPo pierwsze zauważmy, że wystarczyło dodać potok i kolejną komendę. Teraz cała nasza instrukcja oznacza „Weź df, odfiltruj mężczyzn i potem wybierz kolumny Id Wiek (ukończony w latach), Wykształcenie oraz kolumny od 9. do 18.”. Do tego więc służą potoki – pozwalają naraz wykonać całą serię modyfikacji tego samego obiektu. Wypada tu wyjaśnić dwie sprawy. Po pierwsze kolumna z wiekiem zawiera w nazwie spacje. Jeśli nazwa kolumny zawiera niestandardowe znaki, trzeba ją otoczyć znakami ` (pol. grawis, ang. backtick), który znajduje się na klawiaturze tuż pod Esc. Druga rzecz to 9:18, co znaczy „liczby od 9 do 18” i jest wygodnym, skrótowym zapisem seq(9, 18).\nEwentualnie możemy chcieć powiedzieć, żeby zostawić wszystkie kolumny poza jakąś kolumną. Jeśli chcemy wykluczyć 2 kolumny z 200, to lepiej wskazać te 2 do wywalenia niż pozostałe 198 do zachowania. Możemy to zrobić dodając przed kolumną -. Możemy ustawić minus zarówno przed nazwą kolumny lub zakresem, ale warto zauważyć, że zakres pozycji trzeba wziąć w nawias. Inaczej R pomyśli, że chodzi nam np. o kolumny od -2 do 5. Gdzie nie jest to głupie, kolumna -2 oznacza „druga od końca”.\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(-(2:5), -`Wyrażam świadomą i dobrowolną zgodę na udział w badaniu.`)"
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-rename",
    "href": "statystyka/podstawy_R.html#sec-rename",
    "title": "Jakub Jędrusiak",
    "section": "6.2 Zmiana nazw kolumn z dplyr::rename i purrr::set_names",
    "text": "6.2 Zmiana nazw kolumn z dplyr::rename i purrr::set_names\nCo zrobić, jeśli chcemy w naszej bazie coś pozmieniać? Zacznijmy może od zmiany nazw kolumn, żeby łatwiej nam się pisało dalsze komendy. Do tego służą komendy rename z pakietu dplyr i set_names z pakietu purrr14. rename służy do zmiany nazw pojedynczych kolumn i przyjmuje argumenty w postaci rename(\"nowa_nazwa\" = \"stara nazwa\"). Za jednym zamachem możemy zmienić ile nazw chcemy. Jeśli chcemy zmienić wszystkie nazwy, wygodniejsza jest funkcja set_names do której po kolei wrzucamy nowe nazwy, których potrzebujemy. Znowu – nazwy to dosłowne ciągi znaków, więc zawsze piszemy je w cudzysłowie.\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\"))\n\nCo znowu namieszałem? Czemu znowu coś utrudniam? Cóż, żeby ułatwić. o ile kolejne nazwy \"id\", \"wiek\" i \"wyksztalcenie\" rozumieją się same przez się, to do czego służy tutaj funkcja paste? Jeśli zerkniemy w bazę danych, zauważymy, że kolejne 10 kolumn to to samo pytanie „Takie widoki w przestrzeni publicznej są normalne”. Odpowiedź na to pytanie (znajdujące się pod obrazkiem neutralnym lub przedstawiającym parę jednopłciową) traktowaliśmy jako wskaźnik homofobii. Jest bardzo częste, że czyszcząc dane z badania mamy serię odpowiedzi z jednego kwestionariusza. Zazwyczaj wszystkie te pytania nazywamy według jednej konwencji np. wszystkie odpowiedzi z kwestionariusza TIPI nazywamy TIPI_1, TIPI_2, TIPI_3 itd. Ale po co pisać te etykiety ręcznie, skoro możemy je wygenerować? Do tego służy funkcja paste.\n\npaste(\"H\", 1:10, sep = \"_\")\n\n [1] \"H_1\"  \"H_2\"  \"H_3\"  \"H_4\"  \"H_5\"  \"H_6\"  \"H_7\"  \"H_8\"  \"H_9\"  \"H_10\"\n\n\nJak widzimy, paste wygenerowało nam 10 kolejnych etykiet łącząc \"H\" i liczby od 1 do 10. Argument sep = \"_\" mówi, żeby między kolejnymi kawałkami wstawiać podkreślnik. Do paste możemy wrzucić dowolną liczbę znaków do połączenia. Jeśli nie chcemy żadnego separatora, możemy ustawić sep = \"\", czyli pusty ciąg znaków w separatorze albo możemy użyć bliźniaczej funkcji paste0, która nie ma separatora."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-values",
    "href": "statystyka/podstawy_R.html#sec-values",
    "title": "Jakub Jędrusiak",
    "section": "6.3 Zmiana wartości komórek z dplyr::mutate i readr::parse_number",
    "text": "6.3 Zmiana wartości komórek z dplyr::mutate i readr::parse_number\nJak widzimy, odpowiedzi na pytania z homofobią zawierają nie tylko liczby, ale też tekst z legendą. My jednak chcemy zostawić same liczby. Jeśli spojrzymy na kolumnę z wiekiem, możemy zauważyć, że jest to również jest kolumna tekstowa. Dziwne, przecież wiek to (tylko) liczba. Przejrzenie danych pozwala stwierdzić, że respondent o id 50 w pytaniu o wiek wpisał „18 (2021)“. Nieważne, jak się będziemy przed tym bronić, co dopiszemy do pytania o wiek, zawsze znajdzie się ktoś, kto zrobi w nim elaborat. Ten jeden respondent sprawił, że cała kolumna nie jest traktowana jako kolumna liczbowa, ale jako tekst. Odpowiedź na oba te problemy jest taka sama – mutate i parse_number.\nFunkcja mutate to ogólna funkcja, za pomocą której modyfikujemy kolumny albo dodajemy nowe. Jej składnia wygląda następująco:\n\nzmienna_z_danymi %>%\n    mutate(\n        kolumna_do_modyfikacji = jakas_funkcja(kolumna_do_modyfikacji),\n        nowa_kolumna = inna_funkcja(jak_stworzyc_nowa_kolumne)\n    )\n\nmutate w pewnym sensie zawsze tworzy nową kolumnę. Jeśli nowa kolumna ma taką samą nazwę, jak stara, to zastępuje starą. W naszym przykładzie chcemy do całej kolumny wiek zastosować funkcję parse_number, która pozbywa się wszystkiego poza pierwszą napotkaną liczbą15. Taka instrukcja będzie wyglądała następująco:\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek)\n    )\n\n\n6.3.1 Przekształcanie wielu kolumn jednocześnie z dplyr::across\nPo wykonaniu tej funkcji widzimy, że cała kolumna jest już numeryczna. To samo możemy zrobić dla pytań z homofobią. Moglibyśmy, oczywiście, zapisać H_1 = parse_number(H_1), H_2 = parse_number(H_2) itd., ale po co się męczyć? Na początku roku 2020 dostaliśmy cudowną funkcję pomocniczą across, która przydaje nam się w takich dokładnie wypadkach, czyli gdy chcemy zmodyfikować serię kolumn w taki sam sposób. Jak jej używać?\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(starts_with(\"H_\"), parse_number)\n    )\n\nPierwszą rzeczą, którą ta funkcja przyjmuje, jest zestaw kolumn. Można je wskazać na różne sposoby, np. wypisać ich nazwy albo numery, ale możemy też użyć jednej z cudownych funkcji pomocniczych z zestawu tidy-select. Tutaj akurat użyłem starts_with(\"H_\"), żeby powiedzieć, że ma to zrobić ze wszystkimi kolumnami, których nazwy zaczynają się od H_. Te same funkcje możemy wykorzystywać w funkcji select. Kilka innych tego typu funkcji umieściłem w tabeli.\n\nFunkcje pomocnicze do select i across.\n\n\n\n\n\n\nFunkcja\nWybierz wszystkie kolumny…\n\n\n\n\nstarts_with()\nktórych nazwy zaczynają się od\n\n\nends_with()\nktórych nazwy kończą się na\n\n\ncontains()\nktórych nazwy zawierają w sobie\n\n\nmatches()\nktórych nazwy zawierają w sobiewyrażenie regularne16\n\n\n:\nzawierają się w zakresie(np. H_1:H_10)\n\n\nall_of()\nw których wszystkie wartościspełniają jakiś warunek\n\n\nany_of()\nw których jakakolwiekwartość spełnia warunek\n\n\neverything()\nw ogóle wszystkie kolumny\n\n\nwhere()\ngdzie spełniony jest inny warunek(np. where(is.numeric)))\n\n\n\nDrugim argumentem, jaki przyjmuje across jest nazwa funkcji, którą chcemy zastosować. Co ważne, musi to być jej nazwa bez nawiasów. Jest to częsty błąd i subtelna różnica, polegająca na tym, że jeśli nie używamy nawiasów, podajemy across samą funkcję, a jeśli damy nawiasy, to wrzucamy w ten sposób do across wynik działania tej funkcji. Spowodowałoby to, że w tym wypadku dostalibyśmy błąd, że funkcja parse_number() nie dostała wymaganych argumentów. Jeśli chcielibyśmy dorzucić do parse_number jakieś argumenty (jak locale17), możemy to zrobić po przecinku. Szczegóły, jak zwykle, znajdziemy w dokumentacji funkcji across.\n\n\n6.3.2 Odwracanie punktacji\nBardzo często zdarza nam się, że w kwestionariuszach niektóre pozycje mają odwróconą punktację. Na przykład w kwestionariuszu samooceny Rosenberga SES pojawia się pozycja „Czasem czuję się bezużyteczny(-a)“. Odpowiada się na skali 1 do 4. Wiadomo, że osoba, która zaznacza przy takiej pozycji 4, nie pokazuje swojej wysokiej samooceny. Jest to pozycja z odwróconą punktacją, czyli 4 należy liczyć jako 1, 3 jako 2 itd. Przekształcenie to można zrobić bardzo łatwo. Najpierw dodajemy skrajne wartości skali, np. dla SES \\(1 + 4 = 5\\). Teraz od 5 odejmujemy odpowiedź osoby badanej i dzięki temu rzeczywiście 4 zamienia się w 1, 3 w 2 itd. Jak odwrócić punktację w R?\nPonieważ jest to modyfikacja kolumny, użyjemy funkcji mutate. Załóżmy, że H_5 ma odwróconą punktację. Oceny były na skali od 1 do 6, więc wyniki osób badanych musimy odjąć od 7. W takiej sytuacji kod wyglądałby następująco:\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        H_5 = 7 - H_5\n    )\n\nNiektórzy lubią tworzyć nowe kolumny na odwróconą punktację, my jednak po prostu zastąpiliśmy oryginalną kolumnę H_5. Jeśli chcemy odwrócić wiele kolumn, możemy użyć across. Załóżmy, że H_7 też ma odwróconą punktację. W takim wypadku nasz kod mógłby wyglądać tak:\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        H_5 = 7 - H_5\n        H_7 = 7 - H_7\n    )\n\nAlbo z użyciem across:\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x)\n    )\n\nPierwszym argumentem jest zestaw kolumn, dlatego nazwy kolumn opakowałem w c(). Jest to podstawowa funkcja, która zbiera kilka rzeczy w jeden zestaw. select czy filter nie potrzebowały, żeby robić takie zestawy, ale wiele funkcji (zwłaszcza spoza tidyverse) tego wymaga. Co z drugim argumentem, czyli funkcją? Tutaj wchodzimy głębiej w programistyczne meandry i można, oczywiście, zostać przy wersji bez across. Odważnych zapraszam do świata funkcji anonimowych.\n\n\n6.3.3 Własne funkcje\nDrugi argument w across to funkcja, jakiej across ma użyć do przekształcenia kolumn. Niestety nie ma funkcji, która odejmowałaby od 7. Żeby sobie z tym poradzić, musimy albo taką funkcję wcześniej zadeklarować, albo użyć tzw. funkcji anonimowej (zwanej też lambda). Pierwsza opcja jest łatwa do zrozumienia, ale wymaga sporo pisania jak na coś, czego użyjemy tylko raz. Tworzenie własnych funkcji w R jest dość łatwe. Nasza funkcja mogłaby wyglądać tak:\n\nodejmij_od_7 <- function(wynik) {\n    7 - wynik\n}\n\nPierwsza rzecz to nazwa. Obrazowo nazwałem naszą funkcję odejmij_od_7. Dalej następuje słowo kluczowe function i w nawiasie argumenty naszej funkcji. Do naszej funkcji wrzucamy wynik osoby badanej, więc nasz argument nazwałem obrazowo wynik. Jeśli chcemy, dla czytelności, rozbić funkcję na kilka linijek otwieramy teraz nawiasy klamrowe i w nich opisujemy, co funkcja ma robić. Nic nie stoi na przeszkodzie, żeby opisać to wszystko w jednej linijce function(wynik) 7 - wynik. Po wykonaniu nasza funkcja rzeczywiście działa, co możemy sprawdzić używając jej w konsoli.\n\nodejmij_od_7(3)\n\n[1] 4\n\nodejmij_od_7(12)\n\n[1] -5\n\n\nJeśli mamy kilka kwestionariuszy z odwróconą punktacją, każdy z inną skalą, możemy od razu zrobić bardziej ogólną funkcję do odwracania.\n\nodejmij_od <- function(wynik, od_czego) {\n    od_czego - wynik\n}\n\nodejmij_od(3, 7)\n\n[1] 4\n\nodejmij_od(2, od_czego = 4)\n\n[1] 2\n\n\nBardziej ogólna funkcja wymaga podania drugiego argumentu, tzn. od czego trzeba odjąć wynik. Jak widać, ta funkcja też działa i przyjmuje argumenty pozycyjne lub nazwane. Jednej albo drugiej funkcji po zadeklarowaniu możemy użyć w across.\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), odejmij_od, od_czego = 7)\n    )\n\n\n6.3.3.1 Funkcje anonimowe\nJeśli funkcja jest prosta i używamy jej tylko raz, często nie chcemy zaśmiecać sobie kodu jej definicją. Wtedy z pomocą przychodzą nam funkcje anonimowe (zwane też funkcjami lambda). Anonimowe, bo nie mają swojej nazwy. Podstawowy sposób ich używania to zadeklarowanie ich od razu w miejscu użycia.\n\nacross(c(H_5, H_7), function(wynik) 7 - wynik)\n\nZamiast nazwy funkcji użyliśmy tutaj od razu jej definicji. Funkcja jest w pełni sprawna i różni się od odejmij_od_7 tylko tym, że nie ma nazwy.\nInny, jeszcze bardziej zwięzły, sposób używania funkcji anonimowych dodaje pakiet purrr wchodzący w skład tidyverse. Polega on na użyciu znaczka ~ (czyt. tylda). Nasza funkcja odejmij_od_7 ma w tej konwencji postać ~ 7 - .x. Poprzez .x oznacza się to, co do funkcji wrzucamy, czyli to, co w odejmij_od_7 oznaczyliśmy jako wynik. Takie coś możemy zapisać w miejscu funkcji w across, co zrobiłem na początku."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-newcols",
    "href": "statystyka/podstawy_R.html#sec-newcols",
    "title": "Jakub Jędrusiak",
    "section": "6.4 Nowe kolumny",
    "text": "6.4 Nowe kolumny\nJak wspomniałem, funkcja mutate nie tylko pozwala na modyfikowanie istniejących kolumn, ale też na tworzenie nowych. Zazwyczaj robimy to w dwóch przypadkach – gdy chcemy zagregować dane z wierszy, np. zsumować wyniki kwestionariusza albo gdy chcemy podzielić naszą bazę na kategorie, np. „młodzi”, „w średnim wieku”, „seniorzy”. Omówmy to po kolei.\n\n6.4.1 Agregowanie danych z wierszy z dplyr::pick\nZałóżmy, że homofobię będziemy liczyć poprzez dodanie H_1 + H_2 + H_3 itd. Czasami będziemy chcieli robić sumy, czasami policzyć średnią. W kwestionariuszach zazwyczaj liczymy sumy, ale dla czasów reakcji często będziemy chcieli policzyć średnią. Jak więc dodać taką sumującą kolumnę w R? Mamy dwa sposoby. Pierwszy to wprost opisanie, co dodajemy, wewnątrz mutate.\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = H_1 + H_2 + H_3 + H_4 + H_5 + H_6 + H_7 + H_8 + H_9 + H_10\n    )\n\nJak można się domyślić, istnieje sposób niewymagający tyle pisania, które w skomplikowanych bazach i długich kwestionariuszach naprawdę może być długotrwałe i uciążliwe. tidyverse ratuje nas tutaj funkcją pick18, a standardowy R dokłada funkcję rowSums (i rowMeans). Wystarczy, że do funkcji rowSums wrzucimy, które kolumny chcemy zsumować, wskazując je właśnie za pomocą pick i funkcji pomocniczych.\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10))\n    )\n\n\n\n6.4.2 Kategoryzowanie przypadków i przekodowywanie z dplyr::case_when\nCzasem zdarza się, że chcemy podzielić dane ilościowe (np. wiek, wzrost, szczęście mierzone kwestionariuszowo) na kategorie (młodzi vs nie-aż-tak-młodzi, wysocy vs niscy, szczęśliwi vs nieszczęśliwi). Zdarza się też, że osoba tworząca ankietę nie buła na tyle przewidująca, żeby w odpowiedziach na pozycję kwestionariuszową do „zdecydowanie się zgadzam” dodać 6, więc nie możemy po prostu użyć parse_number. I w jednym, i w drugim wypadku musimy stworzyć wartości na podstawie innych wartości, np. wzrost poniżej 160 cm zamienić na „niski” albo tekst „zdecydowanie się nie zgadzam” zamienić na 1. Do takich celów służy niezwykle przydatna funkcja case_when z pakietu dplyr. Załóżmy, że chcemy podzielić mężczyzn w naszej bazie na trzy kategorie wykształcenia – podstawowe, ponadpodstawowe i wyższe. Oznacza to, że osoby z wykształceniem średnim i zawodowym musimy wrzucić do jednego worka. W tym celu rozszerzymy naszą instrukcję o kolejną komendę. Funkcja case_when ma dość prostą składnię.\n\ncase_when(\n    warunek_1 ~ wartosc_jesli_prawda,\n    warunek_2 ~ wartosc_jesli_prawda,\n    warunek_3 ~ wartosc_jesli_prawda,\n    .default = wartosc_dla_calej_reszty\n)\n\nFunkcja po kolei sprawdza warunki. Jeśli natrafi na jakiś spełniony warunek zatrzyma się i da taką wartość, jaką temu warunkowi przypisaliśmy. Warunek jest logiczny, czyli może to być cokolwiek od wyksztalcenie == \"Średnie\" po wzrost <= 160. Należy pamiętać, że jeśli wartość wynikowa ma być tekstem, musimy napisać ją w cudzysłowie, jak każdy dosłowny tekst. .default = wartosc może nam służyć do ustalania, co ma być, jeśli żaden z powyższych warunków się nie sprawdzi19. Jeśli chodzi o przykład z wykształceniem, moglibyśmy rozwiązać go tak:\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        )\n    )\n\nW tym wypadku case_when, idąc wiersz po wierszu, sprawdza, czy w kolumnie wyksztalcenie nie znajduje się wartość \"Średnie\". Jeśli tak, to w tworzonej właśnie kolumnie wyksztalcenie_grupa wstawia wartość \"Ponadpodstawowe\" i przechodzi do kolejnego. Jeśli wykształcenie nie jest średnie, to sprawdza, czy jest zawodowe i w razie czego również wstawia \"Ponadpodstawowe\". Jeśli nie jest ani średnie, ani zawodowe, to wstawia to, co akurat jest w kolumnie wyksztalcenie, czyli dla osób z wykształceniem podstawowym wstawia \"Podstawowe\", a dla osób z wykształceniem wyższym \"Wyższe\"20. W ten sposób z 4 kategorii wykształcenia zrobiły nam się 3. W podobny sposób przekodowywalibyśmy klucz w ankiecie na liczby, np. pisząc H_1 == \"Zdecydowanie się zgadzam\" ~ 6."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sortowanie-i-kolejność-kolumn",
    "href": "statystyka/podstawy_R.html#sortowanie-i-kolejność-kolumn",
    "title": "Jakub Jędrusiak",
    "section": "6.5 Sortowanie i kolejność kolumn",
    "text": "6.5 Sortowanie i kolejność kolumn\nWychodzimy już z potężnej funkcji mutate i możemy czyścić dalej. Ostatnia rzecz, którą czasem chcemy zrobić (zazwyczaj ze względów estetycznych), to posortowanie wartości i ustawienie kolumn w określonej kolejności.\n\n6.5.1 Sortowanie z dplyr::arrange\nZa sortowanie w tidyverse odpowiada funkcja arrange. Domyślnie sortuje ona rosnąco, więc jeśli chcemy sortowanie malejące, użyjemy pomocniczej funkcji desc (od descending). Załóżmy, że chcemy posortować nasze dane najpierw według wykształcenia (od najwyższego, do najniższego), a w obrębie wykształcenia według wieku (od najmłodszych do najstarszych). W pierwszym odruchu chcielibyśmy wpisać arrange(desc(wyksztalcenie), wiek). Jest to dobry odruch, jednak jeśli to zrobimy zorientujemy się, że najwyższym z wykształceń jest wykształcenie zawodowe. Dzieje się tak dlatego, że w tej chwili wykształcenie to zwykły tekst, a więc jest sortowany alfabetycznie, nie według naszego klucza. Żeby to zmienić, musimy poznać nowy rodzaj danych.\n\n6.5.1.1 Factors\nCzynniki (factors) to rodzaj danych, za pomocą których przechowujemy tekst, który ma tylko kilka możliwych wartości albo te wartości mają jakąś kolejność, którą chcemy wziąć pod uwagę. Jeśli mamy etykiety takie jak wykształcenie, czy grupa kontrolna/eksperymentalna, to powinniśmy je przechowywać właśnie w tej postaci. Danymi tego typu w tidyverse zarządza pakiet forcats. Żeby zmienić wykształcenie z tekstu na factor, dopiszemy jedną linijkę do naszego mutate i od razu posortujemy.\n\ndf %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        ),\n        wyksztalcenie = factor(\n            wyksztalcenie,\n            levels = c(\"Podstawowe\", \"Zawodowe\", \"Średnie\", \"Wyższe\"),\n            ordered = TRUE\n            )\n    ) %>%\n    arrange(desc(wyksztalcenie), wiek)\n\nKomendę factor dla czytelności rozbiłem tutaj na trzy linijki, ale – oczywiście – można ją całą zapisać w jednej. Po pierwsze wskazałem, że na factor przerobiona ma być kolumna wyksztalcenie. Po drugie wskazałem, jakie wykształcenie może mieć wartości, zbierając je w jeden zestaw funkcją c i wrzucając do argumentu levels. Na koniec poinformowałem R, że w kolejność jest tutaj ważna, dopisując ordered = TRUE. Jeśli tak przerobione dane posortujemy, zobaczymy, że baza rzeczywiście zaczyna się od wykształcenia wyższego.\n\n6.5.1.1.1 Questionr\nPrzy okazji czynników chciałbym wspomnieć o pierwszym dodatku (addins) do RStudio, jaki może nam się przydać. Dodatki to przyjmują różną formę, ale tutaj omówię dwa, które są graficznymi narzędziami pomagającymi pisać nam kod. Można mieć do nich mieszany stosunek, ale póki umiemy też napisać kod ręcznie (lub chociaż wiemy, jak skorzystać z dokumentacji), to mogą być dużą pomocą. Zwłaszcza na początku przygody z R. Pierwszym takim dodatkiem jest Questionr, który pozwala nam stworzyć komendy związane z czynnikami (i kategoryzować dane ilościowe, co ręcznie zrobimy w podrozdziale 6.4.2).\nQuestionr instalujemy jak każdy inny pakiet (install.packages(\"questionr\")). Od tego momentu (lub po zresetowaniu RStudio) w menu addins na górnej belce znajdziemy trzy nowe opcje. Ta interesująca nas to Level ordering. Na początku zobaczymy okienko, w którym możemy wybrać kilka rzeczy. W jakiej zmiennej chcemy zmienić kolejność, w jakiej kolumnie i z jakiego pakietu wziąć funkcję do zmiany kolejności (domyślnie jest to fct_relevel z forcats). W drugiej zakładce możemy graficznie ustawić taką kolejność, jaką chcemy. W ostatniej zakładce otrzymujemy gotowy kod. Questionr nie robi nic samodzielnie, ten kod trzeba jeszcze wkleić w skrypt. Z jednym kruczkiem. Nam chodzi o samą komendę fct_relevel, bez pierwszej linijki, która służy do zapisania zmian. Ponieważ my tworzymy mutate, to wystarczy, że skopiujemy nasze gotowe fct_relevel() do funkcji mutate, dopisując nazwę kolumny i pierwszy argument. Może być to użyteczne, jeśli mamy dużo czynników lub dużo poziomów w czynniku. Ostatecznie nasza komenda mutate z użyciem questionr wyglądałaby tak:\n\nmutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        ),\n        wyksztalcenie = fct_relevel(\n            wyksztalcenie,\n            \"Podstawowe\", \"Zawodowe\", \"Średnie\", \"Wyższe\"\n        )\n)\n\n\n\n\n\n6.5.2 Kolejność kolumn z dplyr::relocate\nMożemy chcieć mieć nasze kolumny w określonej kolejności. Są zasadniczo dwa sposoby zmieniania kolejności kolumn. Jest funkcja relocate, która służy raczej przestawianiu pojedynczych kolumn lub ich niewielkiej liczby. Jeśli chcemy od nowa określić kolejność kolumn, możemy wykorzystać w tym celu znaną nam już funkcję select. Załóżmy, że chcielibyśmy przestawić kolumnę H_suma przed kolumny z cząstkowymi wynikami.\n\n# z użyciem relocate\ndf %>%\n    relocate(H_suma, .before = H_1)\n\n# z użyciem select\ndf %>%\n    select(id, wiek, wyksztalcenie, H_suma, everything())\n\nJeśli przestawiamy kolumny z użyciem relocate, powinniśmy ustawić argument .before albo .after. Oba wymagają nazwy kolumny przed którą lub po której chcemy przenieść naszą kolumnę. Jeśli nie ustawimy żadnego, nasze kolumny zostaną przeniesione na początek tabeli. Jeśli używamy select musimy wpisać kolejność naszych kolumn ręcznie. Ciekawe może być użycie przeze mnie everything(). W tym kontekście znaczy ono „i cała reszta”."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-save",
    "href": "statystyka/podstawy_R.html#sec-save",
    "title": "Jakub Jędrusiak",
    "section": "6.6 Zapisywanie zmian z <-",
    "text": "6.6 Zapisywanie zmian z <-\nW ten sposób uzyskaliśmy cały kod czyszczący. Mamy ów kod zapisany w naszym skrypcie, jeśli go uruchomimy, to widzimy, że działa. Jednak jeśli w konsolę wpiszemy df, naszym oczom ciągle ukazuje się stara, brzydka baza. Jak więc zmienić nasze df na wyczyszczoną wersję? Tak jak zawsze przypisujemy wartości w R – operatorem <-. Nasz kod na zmianę brudnej bazy w czystą ostatecznie przyjmie postać:\n\ndf <- df %>%\n    filter(Płeć == \"Mężczyzna\") %>%\n    select(ID, `Wiek (ukończony w latach)`, Wykształcenie, 9:18) %>%\n    set_names(\"id\", \"wiek\", \"wyksztalcenie\", paste(\"H\", 1:10, sep = \"_\")) %>%\n    mutate(\n        wiek = parse_number(wiek),\n        across(H_1:H_10, parse_number),\n        across(c(H_5, H_7), ~ 7 - .x),\n        H_suma = rowSums(pick(H_1:H_10)),\n        wyksztalcenie_grupa = case_when(\n            wyksztalcenie == \"Średnie\" ~ \"Ponadpodstawowe\",\n            wyksztalcenie == \"Zawodowe\" ~ \"Ponadpodstawowe\",\n            .default = wyksztalcenie\n        ),\n        wyksztalcenie = factor(\n            wyksztalcenie,\n            levels = c(\"Podstawowe\", \"Zawodowe\", \"Średnie\", \"Wyższe\"),\n            ordered = TRUE\n            )\n    ) %>%\n    arrange(desc(wyksztalcenie), wiek) %>%\n    relocate(H_suma, .before = H_1)\n\nKod ten możemy uruchomić dla dowolnej ilości danych, w dowolnym momencie. Jest wielokrotnego użytku i spokojnie możemy go wykorzystać, kiedy baza się rozrośnie. Nie musimy go wtedy pisać od nowa, wystarczy, że go uruchomimy. Co więcej, mogę wpaść jeszcze na jakiś pomysł i dopisać linijkę na samym początku, nie musząc całej reszty robić od nowa. Sprawmy sobie tę przyjemność i zerknijmy na naszą wyczyszczoną bazę.\n\ndf\n\n# A tibble: 45 × 15\n      id  wiek wyksztal…¹ H_suma   H_1   H_2   H_3   H_4   H_5   H_6   H_7   H_8\n   <dbl> <dbl> <ord>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1    54    21 Wyższe         44     5     5     5     5     2     5     2     5\n 2    33    22 Wyższe         43     5     5     4     5     2     5     2     5\n 3    30    23 Wyższe         45     6     5     5     5     2     5     3     5\n 4    49    23 Wyższe         45     6     6     6     6     1     6     1     6\n 5    31    24 Wyższe         39     3     4     4     4     3     4     4     4\n 6    38    25 Wyższe         44     5     5     5     5     2     5     2     5\n 7    46    25 Wyższe         33     5     5     1     5     2     1     3     5\n 8    43    26 Wyższe         45     6     6     6     6     1     6     1     6\n 9    35    29 Wyższe         50     6     6     6     6     1     6     1     6\n10     2    52 Wyższe         44     5     5     5     5     2     5     2     5\n# … with 35 more rows, 3 more variables: H_9 <dbl>, H_10 <dbl>,\n#   wyksztalcenie_grupa <chr>, and abbreviated variable name ¹​wyksztalcenie\n\n\nPo zapisaniu zmiennej df, tracimy naszą starą zmienną. Tym samym, jeśli uruchomilibyśmy nasz kod jeszcze raz, ale już na nowej zmiennej df, wyskoczy nam błąd. W końcu nowa zmienna nie ma tych samych kolumn, co stara zmienna. Co więcej, takiej operacji nie da się cofnąć. Jeśli chcemy dostać swoją starą, brudną bazę, musimy ponownie załadować ją z pliku. To prowadzi nas do ważnego wniosku co do pisania skryptów – powinniśmy pisać je tak, żeby dało się z nich odtworzyć wszystko, co robiliśmy od samego początku21. Dzięki temu, jeśli chcemy się wycofać, zaznaczamy i wykonujemy cały kod przed interesującym nas momentem. Brak skrótu Ctrl+Z jest jedną z ważniejszych różnic w analizie w programach typu SPSS czy Statistica a w językach programowania typu R czy Python. Wbrew pozorom, idzie się przyzwyczaić. Ta sama właściwość pozwala na zachowanie przejrzystości w nauce – pokaż mi swój kod, a będę wiedział bardzo dokładnie, jak prowadziłeś(-aś) swoją analizę."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-summarise",
    "href": "statystyka/podstawy_R.html#sec-summarise",
    "title": "Jakub Jędrusiak",
    "section": "6.7 Grupowanie (dplyr::group_by) i agregowanie (dplyr::summarise)",
    "text": "6.7 Grupowanie (dplyr::group_by) i agregowanie (dplyr::summarise)\nGdy mamy już bazę, zazwyczaj chcemy policzyć pewne statystyki dla grup badanych, np. dla osób różniących się wykształceniem, płcią czy jakąś manipulacją. Chcemy na przykład poznać średnią homofobię osób o różnym wykształceniu, sprawdzić liczność naszych podgrup czy policzyć inne zbiorcze statystyki. Możemy, oczywiście, odfiltrować najpierw osoby o wykształceniu podstawowym, policzyć dla nich, potem osoby o wykształceniu średnim itd. Są jednak prostsze sposoby, a obejmują one użycie group_by i summarise22. Te dwie funkcje zazwyczaj idą ze sobą w parze i zgrupowane dane od razu trafiają do summarise. Poniżej przykład.\n\ndf %>%\n    group_by(wyksztalcenie) %>%\n    summarise(\n        n = n(),\n        H_M = mean(H_suma),\n        H_SD = sd(H_suma),\n        H_Me = median(H_suma),\n        V = H_SD / H_M\n    )\n\n# A tibble: 4 × 6\n  wyksztalcenie     n   H_M  H_SD  H_Me      V\n  <ord>         <int> <dbl> <dbl> <dbl>  <dbl>\n1 Podstawowe        3  47    3.61    48 0.0767\n2 Zawodowe          2  46    2.83    46 0.0615\n3 Średnie          29  42.3  5.00    43 0.118 \n4 Wyższe           11  42.9  4.35    44 0.101 \n\n\nJak widzimy, dostaliśmy tabelkę z wykształceniem i wskazanymi statystykami. Funkcja n zliczyła nam przypadki osób z poszczególnym wykształceniem, mean policzyła średnią, sd odchylenie standardowe, a median medianę. V to tzw. współczynnik zmienności. Co to jest, nie jest teraz szczególnie ważne. Policzyłem to tutaj, żeby pokazać, że w obliczeniach możemy też wpisywać niestandardowe operacje (jak dzielenie) bez żadnych strasznych funkcji anonimowych, a także że możemy wziąć wartości z innych kolumn jako argumenty do naszych przekształceń. Tutaj V to odchylenie standardowe średniej homofobii (H_SD) podzielone przez samą średnią (H_M). Każdą kolumnę mogliśmy nazwać wedle życzenia. Grupować możemy na podstawie wielu zmiennych. Jak dowiemy się w podrozdziale o eksploracji danych, istnieją funkcje, które najpopularniejsze zastawy danych eksploracyjnych liczą za nas.\nTak robiliśmy to zawsze, jednak dplyr 1.1.0. wprowadził inny sposób grupowania. Jeśli nie chcemy zapisywać grup w naszej bazie danych na później (czyli w większości przypadków), nie musimy w ogóle używać funkcji group_by. Zamiast tego summarise dostał argument .by, za pomocą którego możemy wskazać grupy jednorazowo, tylko na potrzeby tego jednego podsumowania. Więcej na temat argumentu .by można znaleźć w dokumentacji. Poniżej przykład z innego zbioru danych, w którym pojawia się liczba krzyków w piosence w zależności od typu piosenki i jej autora (Field, Miles, & Field, 2012).\n\n# załadowanie danych z sieci\ndf_scream <- read_csv(\"https://github.com/profandyfield/discovr/blob/master/data-raw/csv_files/escape.csv?raw=true\")\n\nRows: 68 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): id, song_type, songwriter\ndbl (1): screams\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# podejrzenie, jak dane wyglądają\nglimpse(df_scream)\n\nRows: 68\nColumns: 4\n$ id         <chr> \"271\", \"q5b\", \"23x\", \"1ai\", \"7st\", \"fug\", \"v28\", \"64f\", \"c3…\n$ song_type  <chr> \"Fly song\", \"Fly song\", \"Fly song\", \"Fly song\", \"Fly song\",…\n$ songwriter <chr> \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"Andy\", \"An…\n$ screams    <dbl> 5, 7, 3, 5, 7, 7, 7, 11, 6, 8, 4, 10, 8, 5, 5, 6, 5, 6, 8, …\n\n# pogrupowanie i zliczenie średniej liczby krzyków\ndf_scream %>%\n    summarise(\n        M = mean(screams),\n        .by = c(song_type, songwriter)\n    )\n\n# A tibble: 4 × 3\n  song_type songwriter     M\n  <chr>     <chr>      <dbl>\n1 Fly song  Andy        6.41\n2 Fly song  Malcolm     6   \n3 Symphony  Andy        9.53\n4 Symphony  Malcolm     7.06\n\n\nKolumny do grupowania podałem jako zestaw, czyli wewnątrz c(). Zgrupowane w ten sposób dane pokazują nam, że Andy pisze bardziej krzykliwe piosenki od Malcolma, ale różnica powiększa się, gdy chodzi o piosenki symfoniczne."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-pivot",
    "href": "statystyka/podstawy_R.html#sec-pivot",
    "title": "Jakub Jędrusiak",
    "section": "6.8 Format długi i szeroki z tidyr::pivot_*",
    "text": "6.8 Format długi i szeroki z tidyr::pivot_*\nFormat długi i szeroki to coś, co rzadko pojawia się w tekstach wprowadzających i nie mam pojęcia czemu. To jest naprawdę ważne. Przełożenie danych z jednego formatu na drugi to często podstawowa operacja, jaką musimy wykonać, kiedy chcemy coś policzyć. Nie mam chyba ani jednego projektu, w którym bym tego nie robił. Do tego współczesne komendy, które to robią, są naprawdę proste. Tym bardziej zaskakujące jest, że np. w Excelu wykonać taką operację jest trudno, jeśli nie umie się korzystać z Power Query. Zacznijmy jednak od tego, co to jest format długi i szeroki.\nTerminy te odnoszą się do sposobu, w jaki składujemy dane. Format szeroki jest tym, co odruchowo tworzymy, kiedy robimy czyste tabelki. Jeden wiersz to jedna obserwacja. Wszystkie dane o konkretnej osobie badanej znajdują się w tym jednym wierszu. Każda kolumna to jedna zebrana dana, np. odpowiedź na konkretne pytanie. W takim formacie znajduje się teraz nasza baza. Weźmy z niej kilka kolumn, po czym użyjmy head, żeby zobaczyć pierwszych pięć wierszy.\n\ndf_wide <- df %>% # zapiszę to jako df_wide, na później\n    select(id, H_1:H_5) %>%\n    mutate(id = 1:nrow(df)) %>% # poprawiam id, żeby były kolejne liczby, zmiana kosmetyczna\n    arrange(id) # sortuję wg id\n\ndf_wide %>% # zapisane dane trzeba jeszcze wyświetlić\n    head(n = 5) # tylko 5 pierwszych wierszy\n\n# A tibble: 5 × 6\n     id   H_1   H_2   H_3   H_4   H_5\n  <int> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     1     5     5     5     5     2\n2     2     5     5     4     5     2\n3     3     6     5     5     5     2\n4     4     6     6     6     6     1\n5     5     3     4     4     4     3\n\n\nSą to typowe dane w formacie szerokim. Żeby jednak zrozumieć różnicę, między formatem długim, a szerokim, trzeba jeszcze zobaczyć dane długie. Stwórzmy więc takie.\n\ndf_long <- df_wide %>%\n    pivot_longer(H_1:H_5, names_to = \"pytanie\", values_to = \"ocena\")\n\ndf_long %>%\n    head(n = 10)\n\n# A tibble: 10 × 3\n      id pytanie ocena\n   <int> <chr>   <dbl>\n 1     1 H_1         5\n 2     1 H_2         5\n 3     1 H_3         5\n 4     1 H_4         5\n 5     1 H_5         2\n 6     2 H_1         5\n 7     2 H_2         5\n 8     2 H_3         4\n 9     2 H_4         5\n10     2 H_5         2\n\n\nZacznę od skomentowania samych danych, a potem wyjaśnię funkcję. Dane w formacie długim mają oddzielną kolumnę na numer pytania i odpowiedź. Pięć kolumn z odpowiedziami na pytania zmieniliśmy w dwie. Powoduje to jednak, że każda osoba badana ma pięć wierszy – w każdym odpowiedź na tylko jedno pytanie. Najpierw następuje 5 wierszy osoby z id 1, potem 5 wierszy osoby z id 2 itd. Widać więc dlaczego formaty te nazywają się szeroki i długi. Szeroki ma wiele kolumn, mało wierszy (1 na osobę), długi mało kolumn, wiele wierszy (1 na każde pytanie).\nPo co nam taki format? Zawiera te same informacje, co format długi, a trudniej się to czyta. Po pierwsze umożliwia nam to policzenie niektórych rzeczy, których nie policzylibyśmy z formatu szerokiego. Dla przykładu, teraz mogę grupować dane według pytań, żeby sprawdzić, czy na każde pytanie badani odpowiadają podobnie. Jeśli moja skala jest dobra i każde pytanie rzeczywiście mierzy to samo, to odpowiedzi na wszystkie pytania powinny być podobne. Być może zrobiłem jakieś kontrowersyjne pytanie, na które wszyscy odpowiadają nisko, mimo że nie różnią się, w tym przykładzie, poziomem homofobii. Mogę więc, na oko, sprawdzić rzetelność pozycji testowych23. Formatu długiego wymagają też niektóre testy statystyczne.\n\ndf_long %>%\n    summarise(\n        M = mean(ocena),\n        SD = sd(ocena),\n        .by = c(pytanie)\n    )\n\n# A tibble: 5 × 3\n  pytanie     M    SD\n  <chr>   <dbl> <dbl>\n1 H_1      5.16 0.767\n2 H_2      5    0.929\n3 H_3      4.47 1.46 \n4 H_4      5.49 0.661\n5 H_5      1.53 0.694\n\n\nNawet częściej niż do grupowania po pytaniach, wykorzystujemy format długi do wykresów. Jak się przekonamy dalej, w gramatyce graficznej (the grammar of graphics) do jednego obiektu na wykresie możemy przypisać tylko jedną kolumnę. Jeśli więc chcemy zrobić wykres słupkowy np. wyników przed i po, to do osi X przypiszemy kolumnę z etykietami, a do osi Y kolumnę z wynikami. Nie da się więc sensownie zrobić wykresu, jeśli wyniki przed i po mamy w osobnych kolumnach.\nOmówmy więc funkcję, której użyłem do zmiany formatu. Kiedyś robiło się to skomplikowanymi funkcjami melt i cast, które często można znaleźć w innych językach programowania. Dziś w R mamy, na szczęście, intuicyjne funkcje pivot_wider i pivot_longer. Tej pierwszej używamy zmieniając format na szeroki, tą drugą zmieniamy format na długi. Na przykładzie powyżej można stwierdzić, że pivot_longer przyjmuje trzy argumenty. Pierwszy to zbiór kolumn, do jakich chcemy tę funkcję zastosować. Można tu skorzystać z funkcji pomocniczych typu starts_with() albo everything(). Kolejne dwa argumenty funkcji pivot_longer to names_to i values_to. Są to nazwy kolumn, do których mają trafić, jak nazwa wskazuje, nazwy i wartości kolumn. W naszym przykładzie etykiety H_1, H_2 itd. trafiły do kolumny pytanie, zaś same odpowiedzi na te pytania do kolumny ocena.\n\ndf_long %>%\n    pivot_wider(names_from = \"pytanie\", values_from = \"ocena\")\n\npivot_wider ma prostszą składnię, ponieważ nie trzeba w niej wskazywać zakresu kolumn do rozwinięcia, a jedynie w jakiej kolumnie znajdują się nazwy kolumn, a w jakiej jej wartości. Robimy to odpowiednio argumentami names_from i values_from. Jeśli jakiejś wartości nie ma w formacie długim (np. gdy osoba z numerem 4 nie odpowiedziała na pytanie 2, to w formacie długim może nie być wiersza 4 H_2), to pivot_wider automatycznie wstawi w tę komórkę NA24. Zdarza się, że funkcji tej musimy użyć dlatego, że niektóre programy generują dane w formacie długim."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-join",
    "href": "statystyka/podstawy_R.html#sec-join",
    "title": "Jakub Jędrusiak",
    "section": "6.9 Retesty czyli złączenia (joins)",
    "text": "6.9 Retesty czyli złączenia (joins)\nZłączenia (joins) to, jak nazwa wskazuje, metoda łączenia dwóch baz danych. Jest to jedna z podstawowych operacji na bazach danych, znana co najmniej od lat 70. i instrukcji JOIN w SQL. Jest to także jedna z operacji niedostępnych w Excelu bez Power Query. W praktyce badawczej może być ona konieczna, gdy mamy badanie wieloczęściowe, w którym musimy stosować wiele baz danych (np. jedną tworzą pomiary z eyetrackera, drugą wyniki w ankiecie, a trzecią test szybkości reakcji). Zdarza się to również często w prostych badaniach ankietowych, kiedy po jakimś czasie musimy wykonać retest. W obu tych przypadkach lądujemy z dwiema (lub więcej) bazami, które – miejmy nadzieję – mają jaką wspólną kolumnę, identyfikator osoby badanej, taki sam w każdej z trzech baz25.\nJak więc takie bazy połączyć? Wykorzystajmy tutaj dwie bazy zawierające test i retest, zrobione podczas walidacji kwestionariusza o nazwie KTR. Składał się on z dwóch skal oznaczonych tutaj literkami O i W. Standardową procedurą przy projektowaniu kwestionariusza jest powtórzenie pomiaru po jakimś czasie, żeby sprawdzić, na ile wyniki są stabilne. My taką procedurę wykonaliśmy, przez co dysponujemy dwoma oddzielnymi bazami. Zerknijmy na nie.\n\ndb_test <- read_csv(\"./dane/podstawy-R/join-test.csv\", show_col_types = FALSE)\ndb_retest <- read_csv(\"./dane/podstawy-R/join-retest.csv\", show_col_types = FALSE)\n\ndb_test\n\n# A tibble: 76 × 3\n   ID    KTR_O KTR_W\n   <chr> <dbl> <dbl>\n 1 B3RP     26    30\n 2 v4Eb     31    36\n 3 j3vB     20    31\n 4 wced     27    37\n 5 RhPy     15    31\n 6 aoEF     32    31\n 7 CjRB     23    28\n 8 bYhC     28    41\n 9 zCdZ     17    27\n10 wspA     21    34\n# … with 66 more rows\n\ndb_retest\n\n# A tibble: 66 × 3\n   Subject KTR_O KTR_W\n   <chr>   <dbl> <dbl>\n 1 j3vB       23    31\n 2 wced       25    38\n 3 aoEF       33    31\n 4 CjRB       24    34\n 5 bYhC       29    41\n 6 zCdZ       17    28\n 7 wspA       24    29\n 8 GGwI       18    35\n 9 L9ZW       23    35\n10 1stk       24    33\n# … with 56 more rows\n\n\nPierwsza rzecz, która może zwrócić naszą uwagę, to znacznie mniejsza liczba osób badanych przy reteście. Jest to naturalne, jako że wiele osób, mimo wcześniejszych deklaracji nie wypełnia naszego testu po raz drugi. Widzimy też, że każdy wiersz posiada jakiegoś rodzaju kolumnę z unikatowym identyfikatorem osoby badanej. W języku relacyjnych baz danych takie unikatowe kolumny określa się jako PRIMARY KEY. W bazie danych z pierwszego testu kolumna ta nosi nazwę ID, a w bazie danych z retestu nazywa się ona Subject. Od razu wychodzi na jaw, że identyfikatory są spreparowane, bo nikt się nie pomylił, nie robił dopisków ani nie zdecydował się z jakiegoś powodu NaGlE pIsAć TaK.\nŻeby połączyć te bazy, musimy najpierw zdecydować, jak chcemy to zrobić. Możemy albo przyłączyć wyniki z retestu do bazy z testem, albo przyłączyć wyniki z testu do bazy z retestem. Jest to o tyle istotne, że jeśli przyłączymy retest do testu, to będziemy mieli puste wartości u tych osób, które nie wypełniły retestu. Jeśli zrobimy odwrotnie, to z założenia każda osoba, która wypełniła retest, wcześniej wypełniła test, a więc figuruje w pierwotnej bazie. W praktyce bywa różnie. Na przykład ludzie kłamią, że wypełnili test, a jak dostaną link do retestu, to myślą, że w takim razie chociaż to wypełnią. Tak czy inaczej, ta decyzja determinuje typ złączenia, jaki wybierzemy. Najbardziej powszechnym typem jest LEFT JOIN, który do każdego wiersza jednej bazy (pisanej jako pierwszej, czyli po lewej) przypisuje pasujący wiersz drugiej bazy (pisanej jako drugiej, czyli po prawej). Jeśli jakiś wiersz w lewej bazie nie ma odpowiednika w prawej bazie, otrzymujemy puste wartości. Jeśli jakiś wiersz w bazie po prawej nie został przypisany do żadnego wiersza po lewej, nie jesteśmy o tym informowani. Więcej o różnych typach złączeń (np. pozwalających uzyskać wszystkie możliwe kombinacje wierszy) można przeczytać i zobaczyć na obrazkach na przykład tutaj.\nJa przyłączę retest do bazy z wynikami pierwszego testu. Widzę jednak dwa problemy, które będę musiał rozwiązać. Po pierwsze, kolumna z identyfikatorem osoby badanej nazywa się inaczej w obu bazach. Po drugie, kolumny KTR_O i KTR_W nazywają się tak samo w obu bazach. Będę więc musiał wskazać R, na podstawie jakich kolumn ma dokonać złączenia, a także jak ma nazwać kolumny w gotowej bazie, żebym wiedział, które wyniki dotyczą pierwszego testu, a które retestu.\n\ndb_joined <- db_test %>%\n    left_join(\n        db_retest,\n        by = join_by(ID == Subject),\n        suffix = c(\"\", \"_retest\")\n    )\n\ndb_joined\n\n# A tibble: 76 × 5\n   ID    KTR_O KTR_W KTR_O_retest KTR_W_retest\n   <chr> <dbl> <dbl>        <dbl>        <dbl>\n 1 B3RP     26    30           NA           NA\n 2 v4Eb     31    36           NA           NA\n 3 j3vB     20    31           23           31\n 4 wced     27    37           25           38\n 5 RhPy     15    31           NA           NA\n 6 aoEF     32    31           33           31\n 7 CjRB     23    28           24           34\n 8 bYhC     28    41           29           41\n 9 zCdZ     17    27           17           28\n10 wspA     21    34           24           29\n# … with 66 more rows\n\n\nPierwszy problem rozwiązałem za pomocą argumentu by. Od wersji dplyr 1.1.0 przyjmuje on inną funkcję o nazwie join_by. W jej nawiasach precyzujemy, na podstawie jakich kolumn należy dokonać złączenia. Identyczne kolumny łączymy znakiem ==. Drugi problem rozwiązałem dodając w argumencie suffix przyrostki do nazw kolumn. Zawsze zapisuje się je jako zestaw, czyli wewnątrz c() i zawsze najpierw jest w cudzysłowie przyrostek lewej bazy (u nas db_test), a potem przyrostek prawej bazy (u nas db_retest). Ja chciałem, by kolumny pierwotnej bazy nie miały przyrostka, więc za przyrostek dałem pusty ciąg znaków (czyli po prostu nic w cudzysłowie), zaś do kolumn bazy z retestem dodałem przyrostek \"_retest\". Efekt widać na obrazku – 5 kolumn i puste wartości u osób, które nie wypełniły retestu.\nJak widać złączenia to zaskakująco szeroki temat, który daje duże możliwości. Omówiona tu funkcja left_join jest najczęściej stosowana, ale warto zerknąć do dokumentacji i w tutoriale, żeby chociaż dowiedzieć się, co możemy za pomocą złączeń zrobić."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-describe",
    "href": "statystyka/podstawy_R.html#sec-describe",
    "title": "Jakub Jędrusiak",
    "section": "7.1 Statystyki opisowe z psych::describe",
    "text": "7.1 Statystyki opisowe z psych::describe\nFunkcja describe pozwala szybko uzyskać zestaw wielu podstawowych statystyk opisowych dla wszystkich kolumn naraz. Powstaje z tego duża tabela, która zawiera znacznie więcej danych niż nam potrzeba, ale ma tę zaletę, że można ją zrobić szybko. Składnia jest tutaj tak banalna, że ciężko mówić o składni, bo polega na tym, że piszemy describe(df). I to zazwyczaj zupełnie wystarczy. Że to za proste, to pokażę trochę więcej możliwości.\n\nlibrary(\"psych\") # normalnie wszystkie pakiety ładowałbym w jednym miejscu, gdzieś na początku kodu\n\ndescr <- df %>%\n    describe(quant = c(0.25, 0.75), IQR = TRUE) %>%\n    as_tibble(rownames = \"var\")\n\nŹródłem problemów czasem bywa to, że mamy dwa główne sposoby przechowywania tabelek w R. Pierwszy to data.frame czyli natywny typ danych w R. Drugi to tibble, czyli data.frame na prochach. Jak nietrudno się domyślić, tibble pochodzi z ze świata tidyverse i właściwie wszystko robi lepiej. Przede wszystkim lepsze jest to, jak wyświetla się w konsoli, ale też kilka rzeczy pod maską. Wszystkie dotychczasowe „ramki danych” (jak to się czasem tłumaczy), z jakimi mieliśmy do czynienia, to właśnie tibble. Żeby na własnej skórze przekonać się o wyższości tibble nad data.frame można wpisać w konsolę df (która jest w formacie tibble), a potem as.data.frame(df). Dlatego też w ostatniej linijce kodu przerabiam wynik działania describe na format tibble.\nJedną rzecz muszę jednak dodać. Z wielu praktycznych przyczyn tibble nie używa nazwanych wierszy. Tylko kolumny mogą mieć nazwy własne, jeśli chcemy mieć identyfikatory wierszy, powinniśmy zrobić z nich oddzielną kolumnę. describe zaś w czystej postaci wyrzuca data.frame, w której nazwy opisanych zmiennych to właśnie nazwy wierszy, dlatego muszę powiedzieć funkcji as_tibble, do jakiej kolumny ma mi wrzucić nazwy tych zmiennych. Swoją nazwałem var.\nPowstała tabela jest na tyle rozległa, że nie chciałem jej tu umieszczać, ale można ją sobie samodzielnie wyświetlić. Mam nadzieję, że wiesz już, jak to zrobić. Żeby podejrzeć zawartość naszej tabeli skorzystam z raz już użytej funkcji glimpse.\n\nglimpse(descr)\n\nRows: 15\nColumns: 17\n$ var      <chr> \"id\", \"wiek\", \"wyksztalcenie*\", \"H_suma\", \"H_1\", \"H_2\", \"H_3\"…\n$ vars     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\n$ n        <dbl> 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45\n$ mean     <dbl> 30.133333, 23.222222, 3.066667, 42.933333, 5.155556, 5.000000…\n$ sd       <dbl> 15.8065004, 8.0450123, 0.7507572, 4.7739825, 0.7673910, 0.929…\n$ median   <dbl> 31, 21, 3, 43, 5, 5, 5, 6, 1, 5, 2, 5, 5, 5, 2\n$ trimmed  <dbl> 30.540541, 21.675676, 3.162162, 43.243243, 5.216216, 5.108108…\n$ mad      <dbl> 19.2738, 1.4826, 0.0000, 4.4478, 1.4826, 0.0000, 1.4826, 0.00…\n$ min      <dbl> 1, 17, 1, 30, 3, 1, 1, 4, 1, 1, 1, 2, 1, 1, 1\n$ max      <dbl> 55, 64, 4, 50, 6, 6, 6, 6, 4, 6, 6, 6, 6, 6, 3\n$ range    <dbl> 54, 47, 3, 20, 3, 5, 5, 2, 3, 5, 5, 4, 5, 5, 2\n$ skew     <dbl> -0.1867356, 3.8733561, -1.0489131, -0.6250676, -0.5537624, -1…\n$ kurtosis <dbl> -1.12792891, 15.34224689, 1.59321540, 0.13697596, -0.29463444…\n$ se       <dbl> 2.35629396, 1.19927962, 0.11191627, 0.71166330, 0.11439589, 0…\n$ IQR      <dbl> 25, 3, 0, 6, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 0\n$ Q0.25    <dbl> 18, 20, 3, 40, 5, 5, 4, 5, 1, 4, 1, 5, 4, 5, 2\n$ Q0.75    <dbl> 43, 23, 3, 46, 6, 6, 6, 6, 2, 6, 2, 6, 6, 6, 2\n\n\nNazwy kolumn mówią nam już, co udało nam się wygenerować. Jest nasza kolumna var z nazwami zmiennych z bazy. Jest cała seria statystyk opisowych. Warto zauważyć, że swoje statystyki typu średnia dostała też zmienna tekstowa wyksztalcenie. Żeby ostrzec, że liczby te nie mają za wiele sensu, nazwa tej kolumny automatycznie dostała gwiazdkę wyksztalcenie*. Opis wszystkich policzonych statystyk znajduje się, a jakże, w dokumentacji. Powiem tylko o dwóch rzeczach, które wprost musiałem wywołać argumentami. Pierwsza to kwantyle, czyli wartości dzielące zbiór danych na części. 25. kwantyl to obserwacja, poniżej której znajduje się 25% danych. Jeśli mielibyśmy dokładnie 100 obserwacji, to 25. kwantylem byłaby 25. obserwacja. Mediana to nic innego jak 50. kwantyl. Możemy poprosić describe o podanie nam dowolnych kwantyli. Wystarczy, że zbierzemy je wszystkie w c jako ułamki dziesiętne i wrzucimy do argumentu quant. Druga rzecz, o którą wprost poprosiłem, to IQR, czyli rozstęp międzykwartylowy. Bywa wykorzystywany jako wskaźnik rozproszenia danych. Ponieważ jest to argument typu „licz albo nie licz”, to wpisałem w niego TRUE, czyli kazałem describe ten rozstęp policzyć."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-describeBy",
    "href": "statystyka/podstawy_R.html#sec-describeBy",
    "title": "Jakub Jędrusiak",
    "section": "7.2 Statystyki opisowe dla grup z psych::describeBy",
    "text": "7.2 Statystyki opisowe dla grup z psych::describeBy\nCo jeśli chcielibyśmy policzyć statystyki opisowe w grupach? Niestety psych nie współpracuje z group_by (patrz 6.7), ale ma własną metodę grupowania danych. Służy temu funkcja describeBy. Przypomnijmy sobie tutaj bazę df_scream z podrozdziału 6.7.\n\nhead(df_scream)\n\n# A tibble: 6 × 4\n  id    song_type songwriter screams\n  <chr> <chr>     <chr>        <dbl>\n1 271   Fly song  Andy             5\n2 q5b   Fly song  Andy             7\n3 23x   Fly song  Andy             3\n4 1ai   Fly song  Andy             5\n5 7st   Fly song  Andy             7\n6 fug   Fly song  Andy             7\n\n\nMamy dwa sposoby, żeby naszą bazę wrzucić do describeBy, z czego ja pokażę ten bardziej intuicyjny. Ma od postać tzw. formuły (formula), z którymi spotkamy się jeszcze przy okazji testów statystycznych (9), zwłaszcza w rstatix (9.2). Przy tej okazji omówimy sobie też głębiej działanie potoków. Najpierw jednak pokażę przykład formuły. Jeśli chcielibyśmy dostać statystyki opisowe dla liczby krzyków w zależności od tego, kto tę piosenkę napisał, skorzystalibyśmy z takiej formuły screams ~ songwriter. Ten znaczek pośrodku to tylda, używana też w funkcjach anonimowych z purrr (6.3.3.1). Jeśli chcielibyśmy dołożyć też do tego podziału typ piosenki, użylibyśmy znaku + pisząc screams ~ songwriter + song_type. Jak mogłaby wyglądać taka funkcja?\n\ndf_scream %>%\n    as.data.frame() %>%\n    describeBy(screams ~ songwriter + song_type, data = .)\n\n\n Descriptive statistics by group \nsongwriter: Andy\nsong_type: Fly song\n        vars  n mean   sd median trimmed  mad min max range skew kurtosis  se\nscreams    1 17 6.41 2.06      6    6.33 1.48   3  11     8 0.56    -0.37 0.5\n------------------------------------------------------------ \nsongwriter: Malcolm\nsong_type: Fly song\n        vars  n mean   sd median trimmed  mad min max range skew kurtosis   se\nscreams    1 17    6 1.97      6    5.87 1.48   3  11     8 0.42     0.49 0.48\n------------------------------------------------------------ \nsongwriter: Andy\nsong_type: Symphony\n        vars  n mean   sd median trimmed  mad min max range skew kurtosis   se\nscreams    1 17 9.53 1.74     10    9.47 2.97   7  13     6 0.43    -1.07 0.42\n------------------------------------------------------------ \nsongwriter: Malcolm\nsong_type: Symphony\n        vars  n mean   sd median trimmed  mad min max range skew kurtosis   se\nscreams    1 17 7.06 1.75      7       7 1.48   4  11     7 0.25    -0.39 0.42\n\n\nWynikiem działania takiej funkcji jest lista (też nieużywany wcześniej typ danych). W tytule możemy zobaczyć autora i typ piosenki. Jeśli to jest jasne, wyjaśnijmy pozostałą część instrukcji. Po pierwsze znów spotykamy problem, że describeBy nie chce współpracować z tibble, więc musimy przekształcić naszą bazę na stary dobry format data.frame. Robimy to funkcją as.data.frame. Druga sprawa to data = .. Argument data to obowiązkowy argument, do którego wrzucamy naszą bazę danych. Dzięki temu R wie, gdzie ma szukać kolumn screams, songwriter i song_type z naszej formuły. Problem polega jednak na użyciu potoków. Standardowo potok %>% wrzuca rzeczy do pierwszego argumentu. Wiele funkcji, tak jak describeBy, nie przyjmuje bazy danych w pierwszym argumencie. Jak na złość data w describeBy to ostatni argument, więc najlepiej wprost go nazwać. Omawiany w podrozdziale 9.2 pakiet rstatix za pierwszy cel postawił sobie nawet przerobienie testów statystycznych tak, żeby lepiej współpracowały z potokami. Jak więc poradzić sobie w takiej sytuacji? Jedna opcja, to całkiem zrezygnować z potoków.\n\ndescribeBy(screams ~ songwriter + song_type, data = as.data.frame(df_scream))\n\nW krótkich instrukcjach takie coś jest nawet czytelne. Warto pamiętać, że potoki nie są obowiązkowe, można równie dobrze zagnieżdżać funkcje. Cierpi na tym tylko czytelność kodu. Druga opcja, to wprost powiedzenie potokowi, gdzie ma wrzucić naszą bazę danych. Do tego właśnie służy .. Zapis data = . mówi potokowi „halo halo, proszę nie wrzucać bazy do pierwszego argumentu, tylko o tu, gdzie pokazałem(-am) kropką”. Ostatecznie oba przykłady z tego podrozdziału są równoważne."
  },
  {
    "objectID": "statystyka/podstawy_R.html#statystyki-opisowe-z-rstatixget_summary_stats",
    "href": "statystyka/podstawy_R.html#statystyki-opisowe-z-rstatixget_summary_stats",
    "title": "Jakub Jędrusiak",
    "section": "7.3 Statystyki opisowe z rstatix::get_summary_stats",
    "text": "7.3 Statystyki opisowe z rstatix::get_summary_stats\nMuszę się do czegoś przyznać. Przez lata wykorzystywałem describe i describeBy do statystyk opisowych, ale… są lepsze alternatywy. Chciałem jednak wykorzystać te funkcje jako praktyczny pretekst to pokazania różnic między data.frame i tibble, używania kropki z potokami i ogólnie jak sobie radzić, jak nowszych alternatyw nie ma. Jednak do liczenia statystyk opisowych takie alternatywy są. I przychodzą do nas z rstatix. Jest to świetny, nowoczesny pakiet, który szerzej omówimy w podrozdziale 9.2.\nDo liczenia statystyk opisowych w rstatix wykorzystujemy funkcję get_summary_stats. Dobrze współpracuje ona z tidyverse, potokami i funkcją group_by. Ma też bardzo prostą składnię. Szczegóły i wyjaśnienia poszczególnych statystyk znajdują się w dokumentacji.\n\nlibrary(\"rstatix\")\n\n# bez grupowania\nget_summary_stats(df)\n\n# A tibble: 13 × 13\n   variable     n   min   max median    q1    q3   iqr   mad  mean     sd    se\n   <fct>    <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>\n 1 id          45     1    55     31    18    43    25 19.3  30.1  15.8   2.36 \n 2 wiek        45    17    64     21    20    23     3  1.48 23.2   8.04  1.20 \n 3 H_suma      45    30    50     43    40    46     6  4.45 42.9   4.77  0.712\n 4 H_1         45     3     6      5     5     6     1  1.48  5.16  0.767 0.114\n 5 H_2         45     1     6      5     5     6     1  0     5     0.929 0.139\n 6 H_3         45     1     6      5     4     6     2  1.48  4.47  1.46  0.217\n 7 H_4         45     4     6      6     5     6     1  0     5.49  0.661 0.099\n 8 H_5         45     1     4      1     1     2     1  0     1.53  0.694 0.103\n 9 H_6         45     1     6      5     4     6     2  1.48  4.73  1.34  0.199\n10 H_7         45     1     6      2     1     2     1  1.48  1.87  1.08  0.161\n11 H_8         45     2     6      5     5     6     1  1.48  5.13  0.991 0.148\n12 H_9         45     1     6      5     4     6     2  1.48  4.49  1.47  0.219\n13 H_10        45     1     6      5     5     6     1  1.48  5.07  1.29  0.192\n# … with 1 more variable: ci <dbl>\n\n# z grupowaniem\ndf_scream %>%\n    group_by(songwriter, song_type) %>%\n    get_summary_stats()\n\n# A tibble: 4 × 15\n  song_…¹ songw…² varia…³     n   min   max median    q1    q3   iqr   mad  mean\n  <chr>   <chr>   <fct>   <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 Fly so… Andy    screams    17     3    11      6     5     7     2  1.48  6.41\n2 Sympho… Andy    screams    17     7    13     10     8    10     2  2.96  9.53\n3 Fly so… Malcolm screams    17     3    11      6     5     7     2  1.48  6   \n4 Sympho… Malcolm screams    17     4    11      7     6     8     2  1.48  7.06\n# … with 3 more variables: sd <dbl>, se <dbl>, ci <dbl>, and abbreviated\n#   variable names ¹​song_type, ²​songwriter, ³​variable"
  },
  {
    "objectID": "statystyka/podstawy_R.html#tabele-liczności-z-rstatixfreq_table",
    "href": "statystyka/podstawy_R.html#tabele-liczności-z-rstatixfreq_table",
    "title": "Jakub Jędrusiak",
    "section": "7.4 Tabele liczności z rstatix::freq_table",
    "text": "7.4 Tabele liczności z rstatix::freq_table\nDla danych kategorialnych (np. płeć, wykształcenie, klasa) nie liczymy statystyk opisowych, a tabele liczności. Chcemy na przykład wiedzieć, ile w naszej bazie mamy osób z wykształceniem wyższym, ile ze średnim itd. W podrozdziale 6.7 pokazałem, jak to zrobić ręcznie funkcjami summarise i n. Pakiet rstatix ma dla nas gotową funkcję freq_table właśnie do tego celu. Wymaga ona jedynie podania, które kolumny z naszej tabeli ująć. Robimy to tak samo, jak zrobilibyśmy w funkcji select (patrz 6.1.2) czy across (patrz 6.3.1).\n\nfreq_table(df, wyksztalcenie)\n\n# A tibble: 4 × 3\n  wyksztalcenie     n  prop\n  <ord>         <int> <dbl>\n1 Podstawowe        3   6.7\n2 Zawodowe          2   4.4\n3 Średnie          29  64.4\n4 Wyższe           11  24.4"
  },
  {
    "objectID": "statystyka/podstawy_R.html#macierze-korelacji-i-ich-istotność-z-rstatixcor_mat",
    "href": "statystyka/podstawy_R.html#macierze-korelacji-i-ich-istotność-z-rstatixcor_mat",
    "title": "Jakub Jędrusiak",
    "section": "7.5 Macierze korelacji i ich istotność z rstatix::cor_mat",
    "text": "7.5 Macierze korelacji i ich istotność z rstatix::cor_mat\nW ramach eksploracji chcemy czasami zrobić macierz korelacji całego naszego zestawu danych. Może nam do tego posłużyć funkcja cor_mat z pakietu rstatix. Zobaczmy to na przykładzie bazy db_joined z podrozdziału 6.9.\n\n(korelacje <- cor_mat(db_joined, -ID))\n\n# A tibble: 4 × 5\n  rowname      KTR_O KTR_W KTR_O_retest KTR_W_retest\n* <chr>        <dbl> <dbl>        <dbl>        <dbl>\n1 KTR_O         1     0.43         0.79         0.45\n2 KTR_W         0.43  1            0.35         0.71\n3 KTR_O_retest  0.79  0.35         1            0.39\n4 KTR_W_retest  0.45  0.71         0.39         1   \n\n\nUżycie, jak widać, jest bardzo proste. Jedyna dodatkowa informacja, jaką sprecyzowałem, to żeby nie brać pod uwagę kolumny ID. W tym wypadku mógłbym też użyć starts_with(\"KTR\"). Efektem działania funkcji jest macierz korelacji. Możemy z niej wyczytać m.in., że korelacja KTR_O i KTR_W z ich retestami wyniosła odpowiednio KTR_O_retest i 0,45. Nie są to jakoś oszałamiające wyniki jak na testy, które mają mierzyć względnie stałe cechy.\nMożna zwrócić uwagę na to, że powyższa komenda wyświetliła nam macierz korelacji, pomimo że normalnie musiałbym jeszcze wywołać samą zmienną korelacje, do której ją zapisałem. Tak to robiliśmy wcześniej. Wykorzystałem tutaj wygodą sztuczkę – jeśli weźmie się całe przypisanie w nawiasy, R potraktuje to jako „przypisz i wyświetl”.\nKorelacje mają jednak swoją istotność, którą możemy tu oznaczyć. Jeśli policzyliśmy już macierz korelacji, możemy ją wrzucić do funkcji cor_get_pval. Ewentualnie możemy samą bazę wrzucić do funkcji cor_pmat. Efekt jest ostatecznie ten sam.\n\nkorelacje_p_1 <- cor_get_pval(korelacje)\n\nkorelacje_p_2 <- cor_pmat(db_joined, -ID)\n\nidentical(korelacje_p_1, korelacje_p_2)\n\nkorelacje_p_1\n\nFunkcja identical informuje nas, że obiekty stworzone obiema funkcjami rzeczywiście są identyczne. Powstała nam macierz istotności korelacji. Może być mylące, że korelacje wyświetlają się w notacji naukowej, co jest wygodnym sposobem oznaczania bardzo małych lub bardzo dużych liczb. Zasada jest tu prosta: \\(1,22e-4 = 1,22 \\times 10^{-4} = 0,000122\\). Jeśli jednak chcemy dostać tę macierz w przyjaźniejszej formie, możemy użyć funkcji cor_mark_significant26, do której wrzucamy macierz korelacji (nie macierz istotności).\n\ncor_mark_significant(korelacje)\n\n       rowname    KTR_O    KTR_W KTR_O_retest KTR_W_retest\n1        KTR_O                                            \n2        KTR_W  0.43***                                   \n3 KTR_O_retest 0.79****   0.35**                          \n4 KTR_W_retest  0.45*** 0.71****       0.39**             \n\n\nOtrzymujemy naszą macierz korelacji wzbogaconą o gwiazdki. Domyślnie mamy standardowy układ27 z dodatkiem **** oznaczającym mniej niż 0,0001. Gwiazdki możemy dostosowywać, a szczegóły znajdują się w dokumentacji.\n\nPozostała część już wkrótce."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-aes",
    "href": "statystyka/podstawy_R.html#sec-aes",
    "title": "Jakub Jędrusiak",
    "section": "8.1 Mapowanie estetyk (aes)",
    "text": "8.1 Mapowanie estetyk (aes)\nPrzejdźmy więc do praktyki. Pakietu ggplot2 nie musimy ładować osobno, bo wchodzi w skład, a jakże, tidyverse. Żeby zacząć tworzyć wykres, musimy zacząć od wywołania funkcji ggplot() (ważne – nie ggplot2, ggplot2 to nazwa pakietu, funkcja to ggplot). W jej obrębie wskazujemy na zbiór danych, na którym chcemy pracować i dokonujemy mapowania estetyk, czyli mówimy naszej funkcji jakie kolumny mają przełożyć się na jakie elementy wizualne. Wykorzystajmy sobie tutaj jeden z klasycznych zbiorów danych o nazwie diamonds, który powinien stać się dostępny po załadowaniu ggplot2. Zerknijmy na niego.\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\ndiamonds to zbiór różnych danych na temat 53940 diamentów. Szczegółowe dane na jego temat można uzyskać wpisując w konsoli ?diamonds. Ponieważ jest to gigantyczny zbiór, wybierzmy sobie losowo 100 diamentów za pomocą funkcji slice_sample. Użyję jeszcze funkcji set.seed, żeby wyniki losowania były za każdym razem takie same. Nie jest ona obowiązkowa, ale użycie jej sprawi że wykresy wyjdą identyczne jak moje.\n\nset.seed(123)\n\ndf_diamonds <- slice_sample(diamonds, n = 100)\n\nZnacznie lepiej. Zrobimy sobie prosty wykres ceny diamentu od jego masy w karatach. Pierwsza rzecz, którą musimy wykonać, to mapowanie kolumn carat i price do osi X i y. Estetyki mapujemy wrzucając je do funkcji aes – najpierw x, potem y.\n\nggplot(df_diamonds, aes(carat, price))\n\n\n\n\nJak widzimy, powstał nam pusty wykres. To jest właśnie układ współrzędnych, o którym mówiłem wcześniej.\nPoza estetykami X i Y mamy do dyspozycji mnóstwo innych estetyk, m.in. colour, fill, alpha (przeźroczystość), size, linetype, shape. Estetyki mają jedną wspólną cechę – są powiązane z jakimiś danymi. Jeśli stwierdzę, że wszystkie moje punkty mają być czerwone, to nie będzie to estetyka, tylko atrybut. O estetyce będę mógł mówić wtedy, gdy kolor będzie zależał np. od przejrzystości diamentu. To rozróżnienie, że atrybuty to stałe właściwości wyglądu, a estetyki to związek wyglądu z danymi, jest o tyle ważne, że nieco inaczej się je definiuje, jak zobaczymy za chwilę."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-geom",
    "href": "statystyka/podstawy_R.html#sec-geom",
    "title": "Jakub Jędrusiak",
    "section": "8.2 Obiekty geom_*",
    "text": "8.2 Obiekty geom_*\nPusty układ współrzędnych to jeszcze nie wykres. Musimy jeszcze dodać jakiegoś rodzaju geom. W naszym przypadku będą to punkty, gdzie każdy punkt będzie reprezentował inny brylant. Ściąga do ggplot2 zawiera świetną rozpiskę, jakie można stworzyć wykresy, w zależności od typu zmiennych, jakimi dysponujemy.\nDo tworzenia wykresów punktowych mamy dwa rodzaje obiektów geom – geom_point i geom_jitter. geom_point to typowy wykres punktowy. geom_jitter przydaje się wtedy, kiedy mamy wiele danych o tych samych współrzędnych, np. wiele brylantów o masie dokładnie 0,2 karata i cenie dokładnie $ 300. W takim wypadku wszystkie te punkty nałożyłyby się na siebie, ukryły jeden pod drugim i wydawałoby się, że mamy mniej danych, niż w rzeczywistości mamy. geom_jitter rozwiązuje ten problem odrobinkę przesuwając każdy punkt w losowym kierunku. Odrobinę tracimy wtedy na dokładności, ale widzimy wszystkie nasze dane. Żeby do naszego wykresu dołożyć kolejne elementy, używamy znaku +.\n\nggplot(df_diamonds, aes(carat, price)) +\n    geom_point()\n\n\n\n\nWygląda na to, że im większy diament, tym droższy. Bez zaskoczenia. Możemy do naszego wykresy dołożyć linię trendu jako kolejny geom – geom_smooth. Jeśli chcemy mieć prostą linię, musimy ustawić argument method = \"lm\", co jest skrótem od linear model.\n\nggplot(df_diamonds, aes(carat, price)) +\n    geom_point() +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nSzare pole wokół niebieskiej linii to przedział ufności. Możemy go wyłączyć ustawiając se = FALSE. Przy tej okazji powiedzmy sobie jeszcze raz o estetykach i atrybutach. Mogę chcieć, żeby kolor mojego punktu zależał od jakości wyszlifowania brylantu z kolumny cut. Ponieważ jest to związek wyglądu z danymi, to jest to estetyka i ustawiam ją wewnątrz aes. mogę to aes wrzucić albo do funkcji ggplot, jak robiłem wcześniej, albo też do funkcji geom_point. Zwyczajowo argumenty X i Y w aes są nienazwane, ale wszystkie inne już tak. Mogę też zmienić kolor linii trendu z niebieskiego na czarny i zrobić ją trochę cieńszą. Jest to zmiana wyglądu, ale arbitralna, bez związku z danymi. Jest to więc atrybut i ustawiam go poza aes, wewnątrz funkcji, której ten atrybut dotyczy. Zobaczmy to.\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-labs",
    "href": "statystyka/podstawy_R.html#sec-labs",
    "title": "Jakub Jędrusiak",
    "section": "8.3 Tytuły osi i wykresu (labs)",
    "text": "8.3 Tytuły osi i wykresu (labs)\nKolejną rzeczą, którą moglibyśmy chcieć zmienić, są tytuły osi. Możemy też dodać tytuł do samego wykresu. Najwygodniej jest to zrobić dodając do wykresy kolejny element, labs, w którym dopiszemy nasze tytuły.\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu z zależności od masy\"\n    )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nJeśli chcemy zmienić etykiety wartości z legendy, mamy dwie opcje – albo przekodujemy te etykiety bezpośrednio w bazie danych, chociażby zaprzęgając do pracy questionr (zob. 6.5.1.1.1), albo używając jednej z funkcji scale_*_discrete i jej argumentu labels, gdzie zamiast gwiazdki piszemy nazwę naszej estetyki. Szlif naszych diamentów jest zmapowany do estetyki colour, więc użyjemy funkcji scale_colour_discrete.\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu z zależności od masy\"\n    ) +\n    scale_colour_discrete(labels = c(\"Zadowalający\", \"Dobry\", \"Bardzo dobry\", \"Premium\", \"Idealny\"))\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-appearance",
    "href": "statystyka/podstawy_R.html#sec-appearance",
    "title": "Jakub Jędrusiak",
    "section": "8.4 Wygląd wykresów",
    "text": "8.4 Wygląd wykresów\nWykres ma wszystkie elementy na miejscu, ale nie oszukujmy się, nie jest to dzieło sztuki. Żeby poprawić wygląd naszego wykresu, sięgniemy po dwa narzędzia. Po pierwsze ustalimy jego ogólny styl za pomocą jednego z motywów (theme), a potem poprawimy szczegóły z użyciem dodatku do RStudio esquisse.\n\n8.4.1 Gotowe szablony (theme_*)\nOgólny styl wykresu ustala się za pomocą elementów zaczynających się słówkiem theme_. ggplot2 ma wbudowane osiem takich motywów, które można przejrzeć tutaj. W Internecie roi się jednak od niestandardowych motywów, które mogą zaczarować nasze wykresy. Kluczowym ich źródłem może być pakiet ggthemes, z motywy z którego można zobaczyć tutaj. Ja jednak chciałem pokazać dwa inne – theme_apa z pakietu papaja i theme_Publication z repozytorium na GitHubie koundy/ggplot_theme_Publication.\npapaja to skrótowiec od Preparing APA Journal Articles i jest to rozległy pakiet pomagający pisać artykuły zgodne ze standardami Amerykańskiego Towarzystwa Psychologicznego (APA). Z tych standardów korzystają nie tylko psychologowie, ale też wiele czasopism z zakresu nauk przyrodniczych. Daje nam on dostęp m.in. do motywu theme_apa dostosowującego wykres do standardów APA.\n\nlibrary(\"papaja\")\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu z zależności od masy\"\n    ) +\n    scale_colour_discrete(\n        labels = c(\"Zadowalający\", \"Dobry\", \"Bardzo dobry\", \"Premium\", \"Idealny\")\n    ) +\n    theme_apa()\n\n\n\n\nDrugi motyw nie jest szałowo popularny w społeczności, ale bardzo go lubię. Żeby zadziałał, musimy wcześniej zainstalować u siebie pakiety grid, scales i ggthemes. Spojrzenie w repozytorium pozwoli nam stwierdzić, że nie jest to pakiet, a po prostu zbiór plików. Wchodzimy więc w plik ggplot_theme_Publication-2.R, kopiujemy link i dodajemy na końcu ?raw=true, co pozwoli R go odczytać. Następnie użyjemy komendy source, która pozwala nam uruchamiać z innych plików .R, jak podamy ich ścieżkę lub link. Cały kod mółgby więc wyglądać tak:\n\nsource(\"https://github.com/koundy/ggplot_theme_Publication/blob/master/ggplot_theme_Publication-2.R?raw=true\")\n\nggplot(df_diamonds, aes(carat, price, colour = cut)) +\n    geom_point() +\n    geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.5, colour = \"black\") +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        colour = \"Szlif\",\n        title = \"Cena brylantu z zależności od masy\"\n    ) +\n    scale_colour_discrete(\n        labels = c(\"Zadowalający\", \"Dobry\", \"Bardzo dobry\", \"Premium\", \"Idealny\"),\n    ) +\n    theme_Publication()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-esquisse",
    "href": "statystyka/podstawy_R.html#sec-esquisse",
    "title": "Jakub Jędrusiak",
    "section": "8.5 esquisse",
    "text": "8.5 esquisse\nWykres w takiej formie można już uznać za zadowalający. Ale co jeśli chcemy poprawić jakieś szczegóły? Na przykład dostosować kolory? Są na to odpowiednie funkcje, ale jeśli mam być szczery, jest ich na tyle dużo, a w tutorialach tak bardzo przeplatają się stare i nowe metody, że bez gruntownego wyszkolenia (którego na przykład ja nie posiadam) bardzo łatwo jest się zgubić i bez zrozumienia kopiować kod znaleziony w Internecie. Na szczęście istnieje pewne narzędzie, które może nam w tej sytuacji pomóc. Nazywa się esquisse i jest dodatkiem do RStudio.\nInstalujemy esquisse jak każdy inny pakiet (install.packages(\"esquisse\")). Po zainstalowaniu, jeśli jest taka potrzeba, można spróbować zmienić język na polski komendą set_i18n(\"pl\"). Będzie to jednak możliwe dopiero, gdy autorzy zaakceptują polskie tłumaczenie do swojego repozytorium. Po zainstalowaniu pakietu, w menu Addins na górnej belce powinniśmy uzyskać dostęp do opcji ggplot2 builder28, które jest narzędziem do interaktywnego konstruowania wykresów. Pozwala na stworzenie kodu w wygodnym, graficznym interfejsie. Po wygenerowaniu możemy skopiować gotowy kod do skryptu.\n\nggplot(df_diamonds) +\n    aes(x = carat, y = price, colour = clarity) +\n    geom_point(shape = \"diamond\", size = 2L) +\n    scale_color_brewer(palette = \"YlOrRd\", direction = 1) +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        title = \"Cena diamentów od ich masy według szlifu\",\n        caption = \"Źródło danych: ggplot2\",\n        color = \"Przejrzystość\"\n    ) +\n    theme_bw() +\n    theme(\n        legend.position = \"bottom\",\n        plot.title = element_text(face = \"bold\")\n    ) +\n    facet_wrap(vars(cut))\n\n\n\n\nPowyżej wykres, który stworzyłem w equisse. Warto jednak zwrócić uwagę na jego niedoskonałości, takie jak brak polskich tłumaczeń typów szlifu. Pewnym wyjaśnieniem może być dla nas nazwa francuskiego słowa equisse – szkic. Takie narzędzia jak equisse nie zwalniają nas całkowicie z umiejętności kodowania wykresów, ale pozwalają wygodnie tworzyć szkice naszego kodu. Ten szkic mogę pozmieniać, np. zamieniając theme_bw na theme_Publication i dodając polskie tłumaczenia.\n\nggplot(df_diamonds) +\n    aes(x = carat, y = price, colour = clarity) +\n    geom_point(shape = \"diamond\", size = 2L) +\n    scale_color_brewer(palette = \"YlOrRd\", direction = 1) +\n    labs(\n        x = \"Masa [karaty]\",\n        y = \"Cena [$]\",\n        title = \"Cena diamentów od ich masy według szlifu\",\n        caption = \"Źródło danych: ggplot2\",\n        color = \"Przejrzystość\"\n    ) +\n    theme_Publication() +\n    theme(\n        legend.position = \"bottom\",\n        plot.title = element_text(face = \"bold\")\n    ) +\n    facet_wrap(\n        vars(cut),\n        labeller = labeller(cut = c(\n            \"Fair\" = \"Zadowalający\",\n            \"Good\" = \"Dobry\",\n            \"Very Good\" = \"Bardzo dobry\",\n            \"Premium\" = \"Premium\",\n            \"Ideal\" = \"Idealny\"\n            )\n        )\n    )\n\n\n\n\nWykresy to olbrzymi temat, który tutaj tylko liznęliśmy z wierzchu. Sądzę jednak, że ta wiedza wystarczy, żeby – z pomocą dokumentacji i Google – być w stanie powoli rozbudowywać swoje umiejętności z zakresu ggplot2."
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-broom",
    "href": "statystyka/podstawy_R.html#sec-broom",
    "title": "Jakub Jędrusiak",
    "section": "9.1 Czyste wyniki, czyli pakiet broom",
    "text": "9.1 Czyste wyniki, czyli pakiet broom"
  },
  {
    "objectID": "statystyka/podstawy_R.html#sec-rstatix",
    "href": "statystyka/podstawy_R.html#sec-rstatix",
    "title": "Jakub Jędrusiak",
    "section": "9.2 Swiss Army Knife w R, czyli rstatix",
    "text": "9.2 Swiss Army Knife w R, czyli rstatix"
  },
  {
    "objectID": "statystyka/podstawy_R.html#lintr",
    "href": "statystyka/podstawy_R.html#lintr",
    "title": "Jakub Jędrusiak",
    "section": "10.1 lintr",
    "text": "10.1 lintr"
  },
  {
    "objectID": "statystyka/podstawy_R.html#styler",
    "href": "statystyka/podstawy_R.html#styler",
    "title": "Jakub Jędrusiak",
    "section": "10.2 styler",
    "text": "10.2 styler"
  },
  {
    "objectID": "matematyka/kombinatoryka.html",
    "href": "matematyka/kombinatoryka.html",
    "title": "Kombinatoryka",
    "section": "",
    "text": "Pokaż kod\nimport itertools\nimport pandas as pd\n\ndef variations(iterable, subset_length):\n    '''Kombinacje w wierszach, permutacje w kolumnach, wszystko razem to wariacje'''\n    df = pd.DataFrame([list(itertools.permutations(x)) for x in itertools.combinations(iterable, subset_length)])\n    df.index = range(1, len(df.index) + 1)\n    df.columns = range(1, len(df.columns) + 1)\n    return df\n\n\nKombinatoryka to część matematyki zajmująca się modyfikacjami zbiorów. Weźmy sobie zbiór 5 pierwszych liter alfabetu i nazwijmy go Z jak zbiór. Zacznę od skomplikowanie brzmiącego wstępu, a potem wyjaśnię to na przykładach.\n\nZ = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n\nZ tym zbiorem mogę zrobić kilka rzeczy. Mogę go zacząć rozbijać na mniejsze zbiory. Mogę zacząć przestawiać w nim elementy. Mogę najpierw rozbić go na mniejsze zbiory, a potem przestawiać elementy w tych małych zbiorach. Każda z tych akcji ma swoją własną nazwę. Jeżeli mówię, że:\n\npermutuję – zmieniam kolejność elementów;\nkombinuję – rozbijam swój zbiór na mniejsze zbiory (combine – łączyć; łączę stare elementy na nowo).\n\nKiedy robię permutacje, z góry zakładam, że kolejność ma znaczenie. Jest wiele sytuacji, w których kolejność ma znaczenie, ale są też sytuacje, w których liczy się tylko to, jakie mam elementy, a nie w jakiej są kolejności. Dla przykładu nieważne, czy w losowaniu Lotto wyciągnięto 2, 5, 7 czy 7, 5, 2 – jeśli mamy te liczby na swoim kuponie, możemy dostać nagrodę. Jeśli kolejność ma znaczenie, mówimy o wariacjach, a jeśli znaczenia nie ma, mówimy o kombinacjach.\nPrzed chwilą mówiłem, że zmiana kolejności to permutacja, a potem nagle używam słowa wariacja. Istnieje pomiędzy nimi pewna różnica, polegająca na tym, czy zmieniam kolejność w całym naszym zbiorze, czy wcześniej rozbijam go na mniejsze zbiory. Słowem permutacja określamy zmiany kolejności w całym zbiorze, zaś o wariacjach mówimy wtedy, gdy przed zmianą kolejności rozbijamy nasz zbiór na mniejsze zbiory.\n\nPermutacje\nOmówmy to na przykładzie naszego zbioru liter od A do E. Permutacja tego zbioru będzie wyglądała tak:\n\npd.DataFrame(itertools.permutations(Z))\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      0\n      A\n      B\n      C\n      D\n      E\n    \n    \n      1\n      A\n      B\n      C\n      E\n      D\n    \n    \n      2\n      A\n      B\n      D\n      C\n      E\n    \n    \n      3\n      A\n      B\n      D\n      E\n      C\n    \n    \n      4\n      A\n      B\n      E\n      C\n      D\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      115\n      E\n      D\n      A\n      C\n      B\n    \n    \n      116\n      E\n      D\n      B\n      A\n      C\n    \n    \n      117\n      E\n      D\n      B\n      C\n      A\n    \n    \n      118\n      E\n      D\n      C\n      A\n      B\n    \n    \n      119\n      E\n      D\n      C\n      B\n      A\n    \n  \n\n120 rows × 5 columns\n\n\n\nZe zbioru 5 liter możemy zatem wytworzyć 120 zbiorów, każdy z inną kolejnością liter. Policzyć jest to dość łatwo. Mamy 5 miejsc i 5 liter, które możemy tam umieścić: \\(P_5 = \\_ \\times \\_ \\times \\_ \\times \\_ \\times \\_\\). Na pierwszym miejscu możemy umieścić 5 liter: \\(P_5 = 5 \\times \\_ \\times \\_ \\times \\_ \\times \\_\\). Ponieważ jedną literę już zużyliśmy, do drugiego miejsca możemy wsadzić tylko jedną z 4 pozostałych liter: \\(P_5 = 5 \\times 4 \\times \\_ \\times \\_ \\times \\_\\). Uzupełniając nasz schemacik dalej otrzymujemy równanie \\(P_5 = 5\\times 4 \\times 3 \\times 2 \\times 1 = 5! = 120\\). 5! (czyt. pięć silnia) to skrótowy zapis mnożenia liczb od 1 do 5. Powstaje nam z tego wzór na liczbę możliwych permutacji n elementów:\n\\[\nP_n = n!\n\\]\n\n\nWariacje\nCiekawie zaczyna się robić, gdy przed zmianą kolejności chcemy jeszcze rozbić nasz zbiór na mniejsze zbiory. Dla przykładu możemy sobie wyobrazić, że chcemy z naszego zbioru 5 liter wybrać wszystkie możliwe zbiory po 2 litery, np. AB, AC, AD itd. Mamy do dyspozycji mniej miejsca, niż liter w zbiorze. Liczenie czegoś takiego jest analogiczne. Na pierwszym miejscu może pojawić się 1 z 5 liter, na drugim tylko 1 z 4: \\(V^2_5 = 5 \\times 4 = 20\\). Powinno więc istnieć 20 takich zbiorów. Wypiszmy je wszystkie.\n\nvariations(Z, 2)\n\n\n\n\n\n  \n    \n      \n      1\n      2\n    \n  \n  \n    \n      1\n      (A, B)\n      (B, A)\n    \n    \n      2\n      (A, C)\n      (C, A)\n    \n    \n      3\n      (A, D)\n      (D, A)\n    \n    \n      4\n      (A, E)\n      (E, A)\n    \n    \n      5\n      (B, C)\n      (C, B)\n    \n    \n      6\n      (B, D)\n      (D, B)\n    \n    \n      7\n      (B, E)\n      (E, B)\n    \n    \n      8\n      (C, D)\n      (D, C)\n    \n    \n      9\n      (C, E)\n      (E, C)\n    \n    \n      10\n      (D, E)\n      (E, D)\n    \n  \n\n\n\n\nŻeby wyprowadzić wzór na takie wariacje, musimy zwrócić uwagę na fakt, że nasze obliczenie \\(5 \\times 4\\) wygląda jak kawałek silni. Brakuje tylko \\(3 \\times 2 \\times 1\\). Moglibyśmy więc zapisać to w taki sposób:\n\\[\nV^2_5 = 5 \\times 4 = \\frac{5 \\times 4 \\times 3 \\times 2 \\times 1}{3 \\times 2 \\times 1} = \\frac{5!}{3!}\n\\]\nW taki sposób \\(3 \\times 2 \\times 1\\) skróci się i zostanie tylko \\(5 \\times 4\\). Jeśli mielibyśmy 3 miejsca, chcielibyśmy uzyskać \\(5 \\times 4 \\times 3\\), a więc w mianowniku zapisalibyśmy tylko \\(2 \\times 1\\), czyli ostatecznie \\(\\frac{5!}{2!}\\). Powstaje nam z tego następujący wzór na liczbę wariacji n elementów po k elementów (czyli rozbicie w podzbiory po k elementów):\n\\[\nV^k_n = \\frac{n!}{(n-k)!}\n\\]\nSpróbujmy wypisać wariacje naszego zbioru po 3 elementy.\n\nvariations(Z, 3)\n\n\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n      6\n    \n  \n  \n    \n      1\n      (A, B, C)\n      (A, C, B)\n      (B, A, C)\n      (B, C, A)\n      (C, A, B)\n      (C, B, A)\n    \n    \n      2\n      (A, B, D)\n      (A, D, B)\n      (B, A, D)\n      (B, D, A)\n      (D, A, B)\n      (D, B, A)\n    \n    \n      3\n      (A, B, E)\n      (A, E, B)\n      (B, A, E)\n      (B, E, A)\n      (E, A, B)\n      (E, B, A)\n    \n    \n      4\n      (A, C, D)\n      (A, D, C)\n      (C, A, D)\n      (C, D, A)\n      (D, A, C)\n      (D, C, A)\n    \n    \n      5\n      (A, C, E)\n      (A, E, C)\n      (C, A, E)\n      (C, E, A)\n      (E, A, C)\n      (E, C, A)\n    \n    \n      6\n      (A, D, E)\n      (A, E, D)\n      (D, A, E)\n      (D, E, A)\n      (E, A, D)\n      (E, D, A)\n    \n    \n      7\n      (B, C, D)\n      (B, D, C)\n      (C, B, D)\n      (C, D, B)\n      (D, B, C)\n      (D, C, B)\n    \n    \n      8\n      (B, C, E)\n      (B, E, C)\n      (C, B, E)\n      (C, E, B)\n      (E, B, C)\n      (E, C, B)\n    \n    \n      9\n      (B, D, E)\n      (B, E, D)\n      (D, B, E)\n      (D, E, B)\n      (E, B, D)\n      (E, D, B)\n    \n    \n      10\n      (C, D, E)\n      (C, E, D)\n      (D, C, E)\n      (D, E, C)\n      (E, C, D)\n      (E, D, C)\n    \n  \n\n\n\n\nTabela jest bardziej rozbudowana, ale wszystko zgadza się z naszymi poprzednimi wnioskami:\n\\[\n\\displaylines{\nV^3_5 = \\frac{5!}{(5-3)!} = \\frac{5!}{2!} = \\\\\n= \\frac{5 \\times 4 \\times 3 \\times 2 \\times 1}{2 \\times 1} = \\\\\n= 5 \\times 4 \\times 3 = 20 \\times 3 = 60\n}\n\\]\nCzyli wariacji po 3 elementy jest w naszym przykładzie 3 razy więcej, niż wariacji po 2 elementy.\n\n\nKombinacje\nMożemy zwrócić uwagę, że tabela powyżej ma ściśle określoną strukturę. W pierwszym wierszu wszystkie podzbiory składają się z literek A, B i C ułożonych na różne sposoby. Można więc powiedzieć, że podzbiory w każdym wierszu są dla siebie permutacjami, bo składają się z tych samych elementów, różnią się tylko kolejnością. Każda kolumna zawiera unikalne zestawy literek. Widzimy więc, że ze zbioru 5 literek możemy wybrać 10 różnych zestawów literek, a w każdym z tych zestawów można ułożyć literki na 6 sposobów, co daje łącznie 60 wariacji. Wariacje możemy więc uzyskać tak, że weźmiemy wszyskie unikalne mniejsze zestawy literek, a potem rozpiszemy permutacje każdego z tych zestawów. Takie unikalne zestawy literek, bez zwracania uwagi na ich kolejność, to kombinacje. W tabeli każdy wiersz to pełny zestaw kombinacji. Wynika nam z tego inny wzór na liczbę wariacji:\n\\[\nV^k_n = C^k_n \\times P_k\n\\]\nSą to w rzeczywistości wymiary naszej tabeli. Liczba kombinacji (tj. unikalnych zestawów) to liczba wierszy, a liczba permutacji to liczba kolumn. Tabela powyżej ma wymiary \\(10 \\times 6\\), bo mamy 10 unikalnych zestawów po 3 elementy i każdy taki zestaw da się ułożyć na 6 różnych sposobów, co ostatecznie daje 60 komórek.\nŻeby wyprowadzić wzór na liczbę kombinacji, możemy wykorzystać fakt, że wiemy, jak się liczy liczbę wariacji i permutacji. W powyższej tabeli mamy 60 wariacji, a każda kombinacja ma 6 możliwych permutacji. Żeby więc pozbyć się informacji o permutacjach, musimy podzielić 60 wariacji na 6. Podstawiając do wzoru:\n\\[\n\\displaylines{\nV^3_5 = C^3_5 \\times P_3 \\\\\n60 = C^3_5 \\times 6\\ |\\div 6 \\\\\nC^3_5 = \\frac{60}{6} = 10\n}\n\\]\nCzyli jeśli mamy tabelę z 6 kolumnami i ilomaś wierszami, która ma 60 komórek, to wierszy musi być 10.\nMożemy do naszego nowego wzoru podstawić wzory na liczbę wariacji i permutacji i w ten sposób uzyskać ogólny wzór na liczbę kombinacji:\n\\[\n\\displaylines{\nV^k_n = C^k_n \\times P_k \\\\\n\\frac{n!}{(n-k)!} = C^k_n \\times k! \\ |\\div k! \\\\\nC^k_n = \\frac{\\frac{n!}{(n-k)!}}{k!} = \\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n}\n\\]\nWzór ten doczekał się nawet własnego symbolu zwanego dwumianem Newtona \\(\\binom{n}{k}\\) (czyt. en nad ka). Dla przykładu liczba kombinacji 5 elementów po 3 elementy oznacza się jako 5 nad 3 i liczy tak:\n\\[\n\\displaylines{\n\\binom{5}{3} = \\frac{5!}{3!(5-3)!} = \\\\\n= \\frac{5!}{3!2!} = \\frac{5 \\times 4 \\times 3 \\times 2 \\times 1}{(3 \\times 2 \\times 1) \\times (2 \\times 1)} = \\\\\n= 10\n}\n\\]\n\n\nPowtórzenia\nDo tej pory omówiliśmy wariacje i kombinacje bez powtórzeń. Innymi słowy litera raz użyta nie mogła zostać użyta ponownie. Spotykaliśmy zbiory ABC, ale nie spotkaliśmy zbioru AAA. Wariacje i kombinacje mogą pozwalać na takie powtórzenia. Wariacje możemy policzyć jak zawsze kreskami. W zbiorze Z mamy 5 liter i chcemy zrobić z niego podzbiory po 2 elementy ze zwracaniem (czyli po wylosowaniu wraca do puli, czyli z powtórzeniami). Na pierwszym miejscu może być 5 liter, ale na drugim miejscu także może być 5 liter, bo litery się nie zużywają. Wychodzi nam więc takie działanie:\n\\[\n\\bar{V}^2_5 = 5 \\times 5 = 5^2 = 25\n\\]\nWychodzi nam z tego prosty wzór na liczbę wariacji n elementów po k elementów z powtórzeniami:\n\\[\n\\bar{V}^k_n = n^k\n\\]\nWzór na kombinacje z porwtórzeniami podaję raczej pro forma, bo rzadko jest używany.\n\\[\n\\bar{C}^k_n = \\binom{k+n-1}{k} = \\frac{(k+n-1)!}{k!(n-1)!}\n\\]\n\n\nPodsumowanie\nPermutacje to zmiany kolejności, kombinacje to unikalne podzbiory. Jeśli zaczniemy robić permutacje unikalnych podzbiorów, wyjdą nam wariacje. Albo patrząc inaczej – permutacje to wariacje \\(V^n_n\\). Permutacje i wariacje możemy liczyć kreskami i silnią. Liczbę kombinacji uzyskamy dzieląc liczbę wariacji po k elementów przez liczbę permutacji k. Powstały wzór oznacza się symbolem Newtona \\(\\binom{n}{k}\\). Pomocny może okazać się poniższy schemat.\n\n\nflowchart TD\n    START(START) --> zbior\n    zbior[/Mam zbiór, z którym chcę coś zrobić/] -->\n    kolejnosc{Czy kolejność ma znaczenie?}\n    kolejnosc -->|Nie| C[kombinacja]\n        C --> C_powtorzenia{Czy elementy mogą się powtarzać?}\n        C_powtorzenia -->|Tak| C_powtorzenia_koniec(kombinacja z powtórzeniami)\n        C_powtorzenia -->|Nie| C_koniec(kombinacja bez powrótrzeń)\n    kolejnosc -->|Tak| V[wariacja]\n        V --> V_calosc{Czy wykorzystuję cały zbiór?}\n        V_calosc -->|Tak| P(permutacja)\n        V_calosc -->|Nie| V_powtorzenia{Czy elementy mogą się powtarzać?}\n        V_powtorzenia -->|Tak| V_powtorzenia_koniec(wariacja z powtórzeniami)\n        V_powtorzenia -->|Nie| V_koniec(wariacja bez powrótrzeń)\n\n\n\nflowchart TD\n    START(START) --> zbior\n    zbior[/Mam zbiór, z którym chcę coś zrobić/] -->\n    kolejnosc{Czy kolejność ma znaczenie?}\n    kolejnosc -->|Nie| C[kombinacja]\n        C --> C_powtorzenia{Czy elementy mogą się powtarzać?}\n        C_powtorzenia -->|Tak| C_powtorzenia_koniec(kombinacja z powtórzeniami)\n        C_powtorzenia -->|Nie| C_koniec(kombinacja bez powrótrzeń)\n    kolejnosc -->|Tak| V[wariacja]\n        V --> V_calosc{Czy wykorzystuję cały zbiór?}\n        V_calosc -->|Tak| P(permutacja)\n        V_calosc -->|Nie| V_powtorzenia{Czy elementy mogą się powtarzać?}\n        V_powtorzenia -->|Tak| V_powtorzenia_koniec(wariacja z powtórzeniami)\n        V_powtorzenia -->|Nie| V_koniec(wariacja bez powrótrzeń)"
  }
]